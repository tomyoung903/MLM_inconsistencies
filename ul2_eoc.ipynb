{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses UL2 to \n",
    "\n",
    "(1) measure inconsistencies in its bidirectional conditionals; \n",
    "\n",
    "(2) improve llm inference with Emsemble of Conditionals.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nus-ytj/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import general_utils\n",
    "# clear GPU memory\n",
    "if False:   \n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import lambada_utils\n",
    "from lambada_utils import LambadaProcessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "'''Load tokenizer'''\n",
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.50s/it]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = 1 # Load model 0\n",
    "# device_map = general_utils.get_ul2_device_map('4')\n",
    "device_map = \"balanced\"\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", \n",
    "                                                    cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2', \n",
    "                                                    low_cpu_mem_usage=True, \n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    device_map=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Specify set partition and UL2 mode; and instantiate the lambada processor'''\n",
    "IS_DEVELOPMENT = False # Set to False to run on the test set\n",
    "set_partition = 'validation_' if IS_DEVELOPMENT else 'test_' # filename part for saving results \n",
    "LAMBADA_TEST_DATA_PATH = \"data/jsonls/validation.jsonl\" if IS_DEVELOPMENT else \"data/jsonls/test.jsonl\"\n",
    "UL2_MODE = \"[NLG]\"\n",
    "\n",
    "processor = LambadaProcessor(tokenizer, \n",
    "                             ul2_mode=UL2_MODE, \n",
    "                             lambada_dataset_path=LAMBADA_TEST_DATA_PATH, \n",
    "                             rm_punc_space=True)\n",
    "lambada = processor.dataset\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy for different punctuations\n",
    "<details>\n",
    "<summary>click to expand</summary>\n",
    "\n",
    "In the LAMBADA last word prediction task, natural language models (LLMs) may append various punctuations to the same last word, leading to different completions. For example, to complete the sentence \"My color of my pet dog is\":\n",
    "\n",
    "Possible Completions:\n",
    "\n",
    "1. _white._ with probability `p_1`\n",
    "2. _white!_ with probability `p_2` (assuming `p_1 > p_2`)\n",
    "3. _black,_ with probability `p_3`\n",
    "4. _black?_ with probability `p_4` (assuming `p_3 > p_4`)\n",
    "\n",
    "Strategies to Rank _white_ and _black_:\n",
    "\n",
    "1. Maximum Probability Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1`\n",
    "- Probability of _black_: `p(black) = p_3`\n",
    "\n",
    "2. Sum of Probabilities Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1 + p_2`\n",
    "- Probability of _black_: `p(black) = p_3 + p_4`\n",
    "\n",
    "Afterwards `p(_white_)` and `p(_black_)` may need normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # '''Generate the top completions (through beam search) for each example, and get the word from each completion.'''\n",
    "if RUN_CELL:\n",
    "    # generate for all examples, and then get the words from the completions, and compare the first one with the target\n",
    "    count_correct = 0 # No. correct last word predictions if only the top completion is considered\n",
    "    count_correct_top_num_beams = 0 # ... if the top num_beams completions are considered\n",
    "    count_no_words_found = 0  # No. examples where no valid last word is found\n",
    "\n",
    "    # punctuated_word: the last word and the punctuation that follows it\n",
    "    id_to_punctuated_words = {} # maps example index to a list of word and punc pairs; every punc is kept for each word\n",
    "    id_to_punctuated_words_unique = {} # ...; every punc is kept for each word  \n",
    "    id_to_completions_ids = {}\n",
    "\n",
    "    MAX_COMPLETION_LENGTH = 8 # for last word prediction, 8 is sufficient\n",
    "    NUM_BEAMS = 20 # 20 is sufficient; more doesn't help\n",
    "\n",
    "    # for example_index in tqdm(range(10)): # len(lambada)\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        input_string = lambada[example_index]['inputs_pretokenized']\n",
    "        inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs,\n",
    "                                max_length=MAX_COMPLETION_LENGTH, \n",
    "                                num_beams=NUM_BEAMS, \n",
    "                                num_return_sequences=NUM_BEAMS, \n",
    "                                output_scores=True,\n",
    "                                eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                                return_dict_in_generate=True)\n",
    "        \n",
    "        completions = [tokenizer.decode(outputs['sequences'][i]) for i in range(NUM_BEAMS)]\n",
    "        completions_ids = [\n",
    "            outputs['sequences'][i].cpu()\n",
    "            for i in range(NUM_BEAMS)\n",
    "            if processor.get_word_from_completion(completions[i]) is not None # if the completion has a valid last word\n",
    "        ]\n",
    "\n",
    "        words = processor.get_words_from_completions(completions)\n",
    "\n",
    "        # TODO: combine them and move to utils.py\n",
    "        completions_without_pad = processor.remove_pad_id(completions_ids)\n",
    "        completions_without_pad_before_punctution = processor.before_first_punc_including(completions_without_pad) # including the first punc\n",
    "        \n",
    "        if words:\n",
    "            if words[0] == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct += 1\n",
    "        else:\n",
    "            count_no_words_found += 1\n",
    "            # print(\"no words found\")\n",
    "        punctuated_words = processor.get_punctuated_words(completions)\n",
    "        id_to_punctuated_words[example_index] = punctuated_words\n",
    "        words_unique = list(set(words))\n",
    "        \n",
    "        # make completions_ids unique\n",
    "        completions_ids_unique = list(set([tuple(x.numpy()) for x in completions_without_pad_before_punctution]))\n",
    "        id_to_completions_ids[example_index] = [torch.tensor(x) for x in completions_ids_unique]\n",
    "\n",
    "        # find the best punctuatuation for each unique word (Maximum Probability Strategy, \n",
    "        # completions are naturally ordered by probs by generate()) TODO: move this for loop to utils.py\n",
    "        id_to_punctuated_words_unique[example_index] = []\n",
    "        for word in words_unique:\n",
    "            found = 0\n",
    "            # iterate through the word and punc pairs, and find the one that matches the word\n",
    "            for punctuated_word in punctuated_words:\n",
    "                # it is a match if pair = word + punc\n",
    "                ENDING_PUNCTUATIONS = ',!.:;?'\n",
    "                for punc in ENDING_PUNCTUATIONS:\n",
    "                    if punctuated_word == word + punc:\n",
    "                        id_to_punctuated_words_unique[example_index].append(punctuated_word)\n",
    "                        found = 1\n",
    "                        break\n",
    "                if found == 1:\n",
    "                    break\n",
    "        \n",
    "        # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "        for word in words_unique:\n",
    "            if word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_top_num_beams += 1\n",
    "                break\n",
    "    print(\"count_correct\", count_correct)\n",
    "    # count_correct, NLU: 0.7595\n",
    "    # count_correct, NLG: 0.7680 (test)\n",
    "    # count_correct, NLG: 0.7689 (test, spaced punc fixed)\n",
    "    # count_correct, S2S: 0.3743 (could be because how the mode handles extra_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # '''Save the beam search results by generate()'''\n",
    "if RUN_CELL:\n",
    "    # note = '_fixed_spaced_punc'\n",
    "    note = ''\n",
    "    timed_pickle_filename = 'data/pkls/' + set_partition + UL2_MODE + '_ul2_lambada_vanilla_beam_search_results_' + general_utils.get_time() + note + '.pickle'\n",
    "    print(timed_pickle_filename)\n",
    "\n",
    "    data_keys = ['count_correct', 'count_correct_top_num_beams', 'count_no_words_found',\n",
    "                'id_to_punctuated_words', 'id_to_punctuated_words_unique', 'id_to_completions_ids']\n",
    "    data = {}\n",
    "    for key in data_keys:\n",
    "        data[key] = locals()[key]\n",
    "\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibly a lot of duplicates; remove them\n"
     ]
    }
   ],
   "source": [
    "'''Load the beam search results'''\n",
    "# timed_pickle_filename = 'data/pkls/ul2_lambada_vanilla_beam_search_results_2023-11-11 20:08:17.pickle'\n",
    "if IS_DEVELOPMENT:\n",
    "    timed_pickle_filename = 'data/pkls/validation_[NLG]_ul2_lambada_vanilla_beam_search_results_2023-11-29-22:45:19.pickle'\n",
    "else:\n",
    "    timed_pickle_filename = 'data/pkls/[NLG]_ul2_lambada_vanilla_beam_search_results_2023-12-07-11:25:07.pickle'    \n",
    "    # timed_pickle_filename = 'data/pkls/ul2_lambada_vanilla_beam_search_results_2023-11-11 20:08:17.pickle'\n",
    "print('possibly a lot of duplicates; remove them')\n",
    "with open(timed_pickle_filename, 'rb') as fp:\n",
    "    ul2_lambada_vanilla_beam_search_results = pickle.load(fp)\n",
    "id_to_completions_ids = ul2_lambada_vanilla_beam_search_results['id_to_completions_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # TODO: a lot of duplicates in id_to_completions_ids, remove them\n",
    "if RUN_CELL:\n",
    "    id_to_completions_ids_unique = {}\n",
    "    for key in id_to_completions_ids:\n",
    "        completions_ids_unique = list(set([tuple(x.numpy()) for x in id_to_completions_ids[key]]))\n",
    "        id_to_completions_ids_unique[key] = [torch.tensor(x) for x in completions_ids_unique]\n",
    "    ul2_lambada_vanilla_beam_search_results['id_to_completions_ids'] = id_to_completions_ids_unique\n",
    "    # save it back to the pickle file\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(ul2_lambada_vanilla_beam_search_results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-offset Ensemble\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "__K-offset Ensemble__ is a particular type of __Ensemble of Conditionals__ for last word prediction tasks like lambada.\n",
    "\n",
    "It aims to augment the only conditional distribution obtained by masking the last word with more distributions. The new distributions are obtained by masking the last __offset__ + 1 words.\n",
    "\n",
    "An example with the _lambada[0]_\n",
    "\n",
    "_lambada[0]['input_pretokenized']_: `... his mouth curved in a confident grin , i do n't care about <last_word>`\n",
    "\n",
    "We consider candidates `['angels.', 'signs.', 'that.']`.\n",
    "\n",
    "The baseline approach is to input `... his mouth curved in a confident grin , i do n't care about <extra_id_0>` to UL2 and obtain the distribution containing the 3 candidates.\n",
    "\n",
    "For the offset=1 case in K-offset Ensemble, we mask an extra token `about` in the end and input instead\n",
    "\n",
    "`... his mouth curved in a confident grin , i do n't care <extra_id_1>`\n",
    "\n",
    "This gives us a different distribution regarding `['about angels.', 'about signs.', 'about that.']`. They are given in an autoregressive manner\n",
    "e.g., `p(about angels) = p(about) * p(angels|about)`. Therefore we will use conditionals in the style of `p(angels|about)` to augment the baseline conditionals.\n",
    "\n",
    "Cases where __K__ is larger can be similarly derived.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OFFSET = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # '''Generate and save the offset samples'''\n",
    "if RUN_CELL:\n",
    "    id_and_offset_to_inputs_and_completions = \\\n",
    "        processor.get_offset_samples(\n",
    "            ul2_lambada_vanilla_beam_search_results['id_to_completions_ids'], \n",
    "            max_offset=MAX_OFFSET,\n",
    "            to_gpu=True\n",
    "        )\n",
    "    note = '_fixed_spaced_punc'\n",
    "    timed_pickle_filename = 'data/pkls/offset_samples_' + set_partition + 'max_offset_' + str(MAX_OFFSET) + note + '_' + general_utils.get_time() + '.pickle'\n",
    "    print(timed_pickle_filename)\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(id_and_offset_to_inputs_and_completions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Load the offset samples\n",
    "if RUN_CELL:\n",
    "    if IS_DEVELOPMENT:\n",
    "        timed_pickle_filename = 'data/pkls/offset_samples_validation_max_offset_15_2023-11-30-00:27:00.pickle'\n",
    "    else:\n",
    "        # data/pkls/avg_log_p_map_max_offset_15_fixed_spaced_punc_2023-12-07-13:44:15.pickle\n",
    "        timed_pickle_filename = 'data/pkls/offset_samples_max_offset_15_fixed_spaced_punc_2023-12-07-11:47:28.pickle'\n",
    "        # timed_pickle_filename = 'data/pkls/offset_samples_parallel_max_offset_5_2023-11-21-20:01:12.pickle'\n",
    "    with open(timed_pickle_filename, 'rb') as fp:\n",
    "        id_and_offset_to_inputs_and_completions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Obtain and save the avg_log_p_map_offset\n",
    "if RUN_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> input_ids, [completion_ids_0, completion_ids_1, completion_ids_2,...]\n",
    "    avg_log_p_map_offset = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    \n",
    "    for example_index in tqdm(range(len(lambada))): \n",
    "    # for example_index in tqdm(range(1)): \n",
    "        if len(id_to_completions_ids[example_index]) == 0:\n",
    "            continue\n",
    "        for offset in range(MAX_OFFSET):\n",
    "            completions_batch = id_and_offset_to_inputs_and_completions[(example_index, offset)]['labels']\n",
    "            input_ids = id_and_offset_to_inputs_and_completions[(example_index, offset)]['inputs'].unsqueeze(0)\n",
    "            outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "            for completion_index in range(len(id_to_completions_ids[example_index])):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1+offset:], \n",
    "                    completions_batch[completion_index][1+offset:]\n",
    "                )\n",
    "                avg_log_p_map_offset[(example_index, offset, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()\n",
    "    \n",
    "    # Save the avg_log_p_map_offset\n",
    "    note = '_fixed_spaced_punc'\n",
    "    pickle_filename = 'data/pkls/avg_log_p_map_' + set_partition + 'max_offset_' + str(MAX_OFFSET) + note + '_' + general_utils.get_time() + '.pickle'\n",
    "    print(pickle_filename)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(avg_log_p_map_offset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 1 # Load the avg_log_p_map for the offset samples\n",
    "if RUN_CELL:\n",
    "    # pickle_filename = 'data/pkls/avg_log_p_map_validation_max_offset_15_2023-11-30-02:43:57.pickle'\n",
    "    pickle_filename = 'data/pkls/avg_log_p_map_max_offset_15_2023-12-07-13:44:15.pickle'\n",
    "    # pickle_filename = 'data/pkls/avg_log_p_map_max_offset_5_2023-11-15-04:12:17.pickle'\n",
    "    # avg_log_p_map_offset (Dict): (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        avg_log_p_map_offset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Quantify disagreement on last word predictions among K-offset conditionals\n",
    "if RUN_CELL: \n",
    "    for NUM_CONDITIONALS in range(2, 6): # 2, 3, 4, 5; how many sets of conditionals to consider; offset = 0 and offset = 1 are 2 different sets of conditionals\n",
    "        id_offset_to_lastword = dict()\n",
    "        id_to_lastwords_by_offsets = dict()\n",
    "        for offset in range(NUM_CONDITIONALS): # if NUM_CONDITIONALS = 2, then offset = 0, 1\n",
    "            for example_index in range(len(lambada)): # len(lambada)\n",
    "                # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "                avg_log_p_and_completion = [\n",
    "                    (avg_log_p_map_offset[(example_index, offset, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                    for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "                ]\n",
    "                if len(avg_log_p_and_completion) == 0:\n",
    "                    continue\n",
    "                # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "                best_avg_log_p, best_completion = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "                lastword = processor.get_word_from_completion(tokenizer.decode(best_completion))\n",
    "                id_offset_to_lastword[(example_index, offset)] = lastword\n",
    "                if example_index not in id_to_lastwords_by_offsets:\n",
    "                    id_to_lastwords_by_offsets[example_index] = []\n",
    "                id_to_lastwords_by_offsets[example_index].append(lastword)\n",
    "        no_disagreement_count = 0\n",
    "        for example_index in id_to_lastwords_by_offsets:\n",
    "            if len(set(id_to_lastwords_by_offsets[example_index])) > 1:\n",
    "                no_disagreement_count += 1\n",
    "        ratio_disagreement = no_disagreement_count / (len(lambada) - ul2_lambada_vanilla_beam_search_results['count_no_words_found'])\n",
    "        print(\"NUM_CONDITIONALS\", NUM_CONDITIONALS, \"ratio_disagreement\", ratio_disagreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle-off ensemble (incomplete)\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "__Middle-off Ensemble__ is a particular type of __Ensemble of Conditionals__ for last word prediction tasks like lambada.\n",
    "\n",
    "\n",
    "It aims to augment the only conditional distribution obtained by masking some additional words in the middle of the input for additional distributions. The new distributions are obtained by masking the last __offset__ + 1 words.\n",
    "\n",
    "The key sample generation function is create_middle_off_sample() in lambada_utils, which is controlled by\n",
    "`middle_span_length`: the length of the masked span in the middle\n",
    "and \n",
    "`middle_to_end_gap`： the gap between the middle_span and the last word\n",
    "\n",
    "on average, the len of a lambada completion is 4.8 (including punc and extra_token_id)\n",
    "\n",
    "An example with the _lambada[0]_\n",
    "\n",
    "_lambada[0]['input_pretokenized']_: `... his mouth curved in a confident grin , i do n't care about <last_word>`\n",
    "\n",
    "We consider candidates `['angels.', 'signs.', 'that.']`.\n",
    "\n",
    "The baseline approach is to input `... his mouth curved in a confident grin , i do n't care about <extra_id_0>` to UL2 and obtain the distribution containing the 3 candidates.\n",
    "\n",
    "\n",
    "\n",
    "completion_lengths = [\n",
    "    id_and_offset_to_inputs_and_completions[example_index,0][completion_index][1].shape[0] - 1\n",
    "    for example_index in range(len(lambada)) \n",
    "    for completion_index in range(len(id_and_offset_to_inputs_and_completions[example_index,0]))\n",
    "] \n",
    "np.mean(completion_lengths) == 3.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define range_middle_span_length and range_middle_to_end_gap'''\n",
    "# RANGE_MIDDLE_SPAN_LENGTH = [3]\n",
    "# RANGE_MIDDLE_TO_END_GAP = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# LENGTH_GAP_TUPLES = [\n",
    "#     (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), \n",
    "#     (3, 6), (3, 7), (3, 8), (3, 9), (3, 10)\n",
    "# ]\n",
    "LENGTH_GAP_TUPLES = [(3, 5)]\n",
    "middle_off_filename_part = 'length_3_gap_5_num_1' # num 1 means only one span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Generate the middle-off samples\n",
    "if RUN_CELL:\n",
    "    # id_middlespan_gap_to_input_and_completions: maps (id, middle_span_length, middle_to_end_gap) to a input_ids(Tensor) and completion_ids(List[Tensor])\n",
    "    id_middlespan_gap_to_input_and_completions = \\\n",
    "        processor.get_middle_off_samples(\n",
    "            id_to_completions_ids, \n",
    "            length_gap_tuples=LENGTH_GAP_TUPLES,\n",
    "            to_gpu=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Save the middle-off samples\n",
    "if RUN_CELL:    \n",
    "    timed_pickle_filename = 'data/pkls/middle_off_samples_' + set_partition + middle_off_filename_part + '_' + general_utils.get_time() + '.pickle'\n",
    "    print('\\''+timed_pickle_filename+'\\'')\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(id_middlespan_gap_to_input_and_completions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Obtain and save the avg_log_p_map for middle-off samples\n",
    "if RUN_CELL:\n",
    "    # id_middlespan_gap_to_input_and_completions: maps (id, middle_span_length, middle_to_end_gap) to a input_ids(Tensor) and completion_ids(List[Tensor])\n",
    "    # avg_log_p_map_middle_off: maps (id, middle_span_length, middle_to_end_gap, completion_index) to avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    avg_log_p_map_middle_off = dict()\n",
    "    for id_middlespan_gap in tqdm(id_middlespan_gap_to_input_and_completions):\n",
    "        input_ids = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['inputs'].unsqueeze(0)\n",
    "        completions_batch = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['labels']\n",
    "        outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "        middlespan_length = id_middlespan_gap[1]\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            avg_log_p = -ce_loss(\n",
    "                # Only care about the tokens corresponding to the last word; omit <extra_id_0> and <extra_id_1>, see assert below\n",
    "                outputs.logits[completion_index][2+middlespan_length:], \n",
    "                completions_batch[completion_index][2+middlespan_length:]\n",
    "            )\n",
    "            \n",
    "            assert id_to_completions_ids[id_middlespan_gap[0]][completion_index].shape[0] - 1 == \\\n",
    "                completions_batch[completion_index][2+middlespan_length:].nonzero().shape[0]\n",
    "\n",
    "            avg_log_p_map_middle_off[(*id_middlespan_gap, completion_index)] = \\\n",
    "                avg_log_p.detach().cpu().tolist()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Save the avg_log_p_map_middle_off\n",
    "if RUN_CELL:\n",
    "    pickle_filename = 'data/pkls/avg_log_p_map_middle_off_' + set_partition + middle_off_filename_part + '_' + general_utils.get_time() + '.pickle'\n",
    "    print('\\'' + pickle_filename + '\\'')\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(avg_log_p_map_middle_off, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 1 # Load the avg_log_p_map_middle_off\n",
    "if RUN_CELL:\n",
    "    # pickle_filename = 'data/pkls/avg_log_p_map_middle_off_rmsl_3_rmteg_1_10_2023-11-25-22:45:36.pickle'\n",
    "    if IS_DEVELOPMENT:\n",
    "        pickle_filename = 'data/pkls/avg_log_p_map_middle_off_validation_length_3_gap_5_2023-12-01-04:22:26.pickle'\n",
    "    else:\n",
    "        pickle_filename = 'data/pkls/avg_log_p_map_middle_off_length_3_gap_5_num_1_2023-12-12-09:03:19.pickle'\n",
    "\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        avg_log_p_map_middle_off = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # Max reduction to emsemble conditionals for the same last word\n",
    "'''Max reduction to emsemble conditionals for the same last word, \n",
    "i.e., only the maximum avg_log_p is kept for each last word across different range_middle_span_length's and range_middle_to_end_gap's.\n",
    "Emsemble the baseline conditionals with the K-offset conditionals and middle-off conditionals.'''\n",
    "\n",
    "if RUN_CELL:\n",
    "    ADD_MIDDLE_OFF = False # ADD the middle-off ensemble to the list\n",
    "    ADD_BASELINE = True # ADD the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "    ADD_K_OFFSET = False # ADD the whole K-offset ensemble to the list\n",
    "    MAX_OFFSET = 11\n",
    "    LENGTH_GAP_TUPLES =  [(3,5)]\n",
    "    ADD_MULTISPAN = False\n",
    "\n",
    "    count_correct = 0\n",
    "    correct_ids = []\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "        avg_log_p_and_completion = []\n",
    "        # add middle-off to the list\n",
    "        if ADD_MIDDLE_OFF:\n",
    "            avg_log_p_and_completion += [\n",
    "                (avg_log_p_map_middle_off[(example_index, middle_span_length, middle_to_end_gap, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for middle_span_length, middle_to_end_gap in LENGTH_GAP_TUPLES\n",
    "                for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "            ]\n",
    "        if ADD_MULTISPAN:\n",
    "            avg_log_p_and_completion += [\n",
    "                (avg_log_p_map_multiple_spans[(example_index, span_length, gap_length, num_spans, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for (_, span_length, gap_length, num_spans, completion_index) in avg_log_p_map_multiple_spans.keys()\n",
    "            ]\n",
    "        # add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "        if ADD_BASELINE:\n",
    "            avg_log_p_and_completion += [\n",
    "                (avg_log_p_map_offset[(example_index, 0, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "            ]\n",
    "            \n",
    "        # add the whole K-offset ensemble to the list\n",
    "        if ADD_K_OFFSET:\n",
    "            avg_log_p_and_completion += [\n",
    "                (avg_log_p_map_offset[(example_index, offset, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for offset in range(1, MAX_OFFSET + 1)\n",
    "                for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "            ]\n",
    "\n",
    "        if len(avg_log_p_and_completion) == 0: # if no completions are found\n",
    "            continue\n",
    "        # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "        best_avg_log_p, best_completion = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "        if processor.is_correct_completion(example_index, best_completion):\n",
    "            count_correct += 1\n",
    "            correct_ids.append(example_index)\n",
    "    print(\"count_correct:\", count_correct)\n",
    "    print(\"accuracy:\", count_correct / len(lambada))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle-off ensemble with multiple spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_GAP_NUM_TUPLE = (3, 5, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: avg_log_p should sum up for different puncs\n",
    "# completion: <extra_id_0> change and <extra_id_1> at the dock<extra_id_2> she'd<extra_id_3> teeth. <extra_id_4>. we won<extra_id_5>. '<extra_id_6> gripped her<extra_id_7> to the dock<extra_id_8> can make it<extra_id_9> shane.\n",
    "\n",
    "# completion: <extra_id_0> change and <extra_id_1> at the dock<extra_id_2> she'd<extra_id_3> teeth. <extra_id_4>. we won<extra_id_5>. '<extra_id_6> gripped her<extra_id_7> to the dock<extra_id_8> can make it<extra_id_9> shane,\n",
    "\n",
    "# completion: <extra_id_0> change and <extra_id_1> at the dock<extra_id_2> she'd<extra_id_3> teeth. <extra_id_4>. we won<extra_id_5>. '<extra_id_6> gripped her<extra_id_7> to the dock<extra_id_8> can make it<extra_id_9> shane!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [00:17<00:00, 288.22it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = 1 # Generate the multispan samples\n",
    "if RUN_CELL:\n",
    "    # id_span_gap_num_to_input_and_completions: maps (id, span_length, gap_between_spans, num_spans) to a input_ids(Tensor) and completion_ids(List[Tensor])\n",
    "    id_span_gap_num_to_input_and_completions = \\\n",
    "        processor.get_multiple_span_samples(\n",
    "            id_to_completions_ids, \n",
    "            length_gap_num_tuple=LENGTH_GAP_NUM_TUPLE,\n",
    "            to_gpu=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_span_gap_num_to_input_and_completions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/4889 [00:08<26:53,  3.01it/s] \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 79.21 GiB total capacity; 52.16 GiB already allocated; 12.62 MiB free; 52.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m id_span_gap_num_to_input_and_completions[id_span_gap_num][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m completions_batch \u001b[38;5;241m=\u001b[39m id_span_gap_num_to_input_and_completions[id_span_gap_num][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m lambada_utils\u001b[38;5;241m.\u001b[39mmulti_labels_forward(model, input_ids, completions_batch)\n\u001b[1;32m     11\u001b[0m span_length \u001b[38;5;241m=\u001b[39m id_span_gap_num[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m num_spans \u001b[38;5;241m=\u001b[39m id_span_gap_num[\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m/data/personal/nus-ytj/MLM_inconsistencies/lambada_utils.py:567\u001b[0m, in \u001b[0;36mmulti_labels_forward\u001b[0;34m(model, input_ids, labels, use_cache, return_dict)\u001b[0m\n\u001b[1;32m    564\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m decoder_input_ids\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# hidden_states.shape: batch_size * max_len * hidden_states_dim\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    568\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m    569\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    570\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    572\u001b[0m )\n\u001b[1;32m    574\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1114\u001b[0m         hidden_states,\n\u001b[1;32m   1115\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1116\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1117\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1118\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1119\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1120\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1121\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1122\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m   1123\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1124\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1125\u001b[0m     )\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:754\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:343\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:313\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    311\u001b[0m hidden_gelu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi_0(hidden_states))\n\u001b[1;32m    312\u001b[0m hidden_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi_1(hidden_states)\n\u001b[0;32m--> 313\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_gelu \u001b[38;5;241m*\u001b[39m hidden_linear\n\u001b[1;32m    314\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/issues/20287\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 79.21 GiB total capacity; 52.16 GiB already allocated; 12.62 MiB free; 52.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "RUN_CELL = 1 # Obtain and save the avg_log_p_map for multispan samples\n",
    "if RUN_CELL:\n",
    "    # id_span_gap_num_to_input_and_completions: maps (id, span_length, gap_length, num_spans) to a input_ids(Tensor) and completion_ids(List[Tensor])\n",
    "    # avg_log_p_map_middle_off: maps (id, middle_span_length, middle_to_end_gap, completion_index) to avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    avg_log_p_map_multiple_spans = dict()\n",
    "    for id_span_gap_num in tqdm(id_span_gap_num_to_input_and_completions):\n",
    "        input_ids = id_span_gap_num_to_input_and_completions[id_span_gap_num]['inputs'].unsqueeze(0)\n",
    "        completions_batch = id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels']\n",
    "        outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "        span_length = id_span_gap_num[1]\n",
    "        num_spans = id_span_gap_num[3]\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            avg_log_p = -ce_loss(\n",
    "                # Only care about the tokens corresponding to the last word (see assert below)); \n",
    "                # so the first <extra_id_0> is omitted, and for each span, the span + <extra_id_k> is omitted;\n",
    "                # totally 1 + num_spans * (span_length + 1) tokens are omitted;\n",
    "                # contains paddings.\n",
    "                outputs.logits[completion_index][1 + num_spans * (span_length + 1) :], \n",
    "                completions_batch[completion_index][1 + num_spans * (span_length + 1) :]\n",
    "            )\n",
    "            \n",
    "            assert id_to_completions_ids[id_span_gap_num[0]][completion_index].shape[0] - 1 == \\\n",
    "                completions_batch[completion_index][1 + num_spans * (span_length + 1) :].nonzero().shape[0]\n",
    "\n",
    "            avg_log_p_map_multiple_spans[(*id_span_gap_num, completion_index)] = \\\n",
    "                avg_log_p.detach().cpu().tolist()    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 1 # Save the avg_log_p_map_multiple_spans\n",
    "if RUN_CELL:\n",
    "    pickle_filename = 'data/pkls/avg_log_p_map_multiple_spans_' + set_partition + 'length_3_gap_5_multispan_' + general_utils.get_time() + '.pickle'\n",
    "    print('\\'' + pickle_filename + '\\'')\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(avg_log_p_map_multiple_spans, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_log_p_map_multiple_spans.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    " Hypothesis: conditionals based on the mask patterns used during pretraining are more powerful; ->  change your story: smartly pick mask patterns to only use one conditional\n",
    "\n",
    "\n",
    " just ensemble with one LENGTH_GAP_TUPLE == (3,5) leads to accuracy: 0.7814865127110421\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls\n",
    "\n",
    "#### tokenizer.decode() can ignore spaces\n",
    "tokenizer.get_vocab()['▁'] == 3\n",
    "tokenizer.decode([32099,  4340,     3,     5]) = 0 '<extra_id_0> cake.' # the lowline token is  \n",
    "\n",
    "This happens a lot: tokenizer.decode(tokenizer.encode(text)) != text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Obtain the avg_log_p_map for middle-off samples via data parallelism'''\n",
    "RUN_CELL = 0\n",
    "if RUN_CELL:\n",
    "    from multiprocessing import Process\n",
    "    import multiprocessing\n",
    "    avg_log_p_map_middle_off = dict()\n",
    "    # define the processing for each id_middlespan_gap example as a function and use threading to use 3 models in parallel\n",
    "    def process(list_id_middlespan_gap, model_, device='cuda:0'):\n",
    "        for id_middlespan_gap in tqdm(list_id_middlespan_gap):\n",
    "            input_ids = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['inputs'].unsqueeze(0).to(device)\n",
    "            completions_batch = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['labels'].to(device)\n",
    "            outputs = lambada_utils.multi_labels_forward(model_, input_ids, completions_batch)\n",
    "\n",
    "            middlespan_length = id_middlespan_gap[1]\n",
    "\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][2+middlespan_length:], \n",
    "                    completions_batch[completion_index][2+middlespan_length:]\n",
    "                )\n",
    "                avg_log_p_map_middle_off[(*id_middlespan_gap, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()\n",
    "            \n",
    "    # run the above function in parallel\n",
    "    import threading\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "\n",
    "    all_id_middlespan_gaps = list(id_middlespan_gap_to_input_and_completions.keys())\n",
    "    all_id_middlespan_gaps_0 = all_id_middlespan_gaps[:len(all_id_middlespan_gaps)//2]\n",
    "    all_id_middlespan_gaps_1 = all_id_middlespan_gaps[len(all_id_middlespan_gaps)//2:]\n",
    "    # all_id_middlespan_gaps_2 = all_id_middlespan_gaps[2*len(all_id_middlespan_gaps)//3:]\n",
    "\n",
    "    t0 = Process(target=process, args=(all_id_middlespan_gaps_0, model, 'cuda:0'))\n",
    "    t1 = Process(target=process, args=(all_id_middlespan_gaps_1, model1, 'cuda:2'))\n",
    "    # t2 = threading.Thread(target=process, args=(all_id_middlespan_gaps_2, model2, 'cuda:4'))\n",
    "\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    # t2.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot ensembled conditionals vs accuracy'''\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load a nice font\n",
    "font_path = '/usr/share/fonts/urw-base35/NimbusMonoPS-Italic.otf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# offset = 0 corresponds to the baseline, which is no. ensembled conditionals = 1; adjust the offset by 1\n",
    "no_ensembled_conditionals_to_accuracy = dict()\n",
    "for offset in range(1, MAX_OFFSET_TEST+1):\n",
    "    no_ensembled_conditionals_to_accuracy[offset] = offset_to_accuracy[offset-1]\n",
    "\n",
    "\n",
    "max_line = plt.plot(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()), label='max')\n",
    "plt.xlabel('No. ensembled conditionals', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "# the interval on x should be 10\n",
    "plt.xticks(np.arange(10, max(list(no_ensembled_conditionals_to_accuracy.keys()))+1, 10))\\\n",
    "# add a tick at 1 on the x axis\n",
    "plt.xticks(list(plt.xticks()[0]) + [1])\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# add a dot at each point\n",
    "plt.scatter(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()))\n",
    "\n",
    "\n",
    "# add a yellow horizontal line at y=offset_to_accuracy[0]\n",
    "plt.axhline(y=no_ensembled_conditionals_to_accuracy[1], color='y', linestyle='--')\n",
    "# add the word \"baseline\" at the end of the yellow line in the font of calibri\n",
    "plt.text(48, no_ensembled_conditionals_to_accuracy[1] + 0.0002, 'baseline', fontproperties=font_prop, fontsize=13)\n",
    "\n",
    "# # plot the accuracy with avg reduction\n",
    "# avg_line = plt.plot([item+1 for item in list(offset_to_accuracy_avg_reduction.keys())], list(offset_to_accuracy_avg_reduction.values()), color='r', label='avg')\n",
    "# # add a dot at each point\n",
    "# plt.scatter([item+1 for item in list(offset_to_accuracy_avg_reduction.keys())], list(offset_to_accuracy_avg_reduction.values()), color='r')\n",
    "\n",
    "plt.scatter(1, no_ensembled_conditionals_to_accuracy[1], color='y')\n",
    "\n",
    "plt.legend(handles=[max_line[0], avg_line[0]], loc='upper center', bbox_to_anchor=(0.9, 0.45), ncol=1, fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot at a high resolution\n",
    "# plt.savefig('no_ensembled_conditionals_to_accuracy_combined.png', dpi=1200)\n",
    "\n",
    "# plt.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# A simple function that prints and sleeps\n",
    "def print_numbers(name):\n",
    "    for i in range(1, 6):\n",
    "        time.sleep(2)\n",
    "        print(f\"{name} prints: {i}\")\n",
    "\n",
    "# Creating threads\n",
    "thread1 = threading.Thread(target=print_numbers, args=(\"Thread 1\",))\n",
    "thread2 = threading.Thread(target=print_numbers, args=(\"Thread 2\",))\n",
    "\n",
    "# Starting threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# Waiting for threads to complete\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "print(\"Threads finished execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import lambada_utils  # Import the module, not just the class\n",
    "importlib.reload(lambada_utils)\n",
    "from lambada_utils import LambadaProcessor  # Re-import the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import general_utils\n",
    "importlib.reload(general_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hf_device_map.keys().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.get_ul2_device_map('6,7').__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
