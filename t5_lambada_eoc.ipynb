{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and commit files and push them to github using git python API and access token\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"font-size:25px;\">Table of Contents</span>\n",
    "\n",
    "* [Initialization with token, remote repo_url, and repo_path](#0)\n",
    "* [Choose the files with EXCLUDE_FOLDERS and MAX_FILE_SIZE](#1)\n",
    "* [Add files](#2)\n",
    "* [Delete files](#3)\n",
    "* [Commit and push](#4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/09127/tomyoung/.local/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-11b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.device_map\n",
      "{0: [0, 1, 2, 3, 4, 5, 6, 7], 1: [8, 9, 10, 11, 12, 13, 14, 15], 2: [16, 17, 18, 19, 20, 21, 22, 23]}\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-11b\", cache_dir='/work/09127/tomyoung/ls6/inconsistencies_project/t5-11b-cache', low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)\n",
    "model.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_print = False\n",
    "\n",
    "if activate_print:\n",
    "    # define show as a function that takes any kind of input and does nothing\n",
    "    def show(*args, **kwargs):\n",
    "        pass\n",
    "else:\n",
    "    show = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/glm/GLM-130B/evaluation_data/evaluation/lambada/lambada/please_next_word/gen/test.jsonl\n",
    "import json\n",
    "import os\n",
    "with open(\"/work/09127/tomyoung/ls6/glm/GLM-130B/evaluation_data/evaluation/lambada/lambada/please_next_word/gen/test.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]\n",
    "# append [NLG] to the beginning of each input, and <extra_id_0> to the end\n",
    "data_appended = [{\"inputs_pretokenized\": x['inputs_pretokenized'] + \" <extra_id_0>\", \"targets_pretokenized\": x['targets_pretokenized']} for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as PUNCTUATIONS\n",
    "PUNCTUATIONS_LIST = list(PUNCTUATIONS)\n",
    "PUNCTUATIONS_LIST.remove(\"<\")\n",
    "PUNCTUATIONS_LIST.remove(\">\")\n",
    "PUNCTUATIONS_LIST.remove(\"_\")\n",
    "PUNCTUATION_IDS_LIST = [tokenizer.get_vocab()[p] for p in PUNCTUATIONS_LIST if p in tokenizer.get_vocab()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: a bit hacky, add explanations\n",
    "def get_words_from_options(options):\n",
    "    '''Get the first word from each of the given options. Return the words.'''\n",
    "    # if a punctuation can be found in the option, get the word before the punctuation\n",
    "    words = []\n",
    "    for option in options:\n",
    "        # find the punctuation\n",
    "        for i in range(len(option)):\n",
    "            if option[i] in PUNCTUATIONS_LIST:\n",
    "                word = option[:i]\n",
    "                words.append(word)\n",
    "                # print(words)\n",
    "                break\n",
    "\n",
    "    # if the word starts with <pad>, remove it\n",
    "    words = [word[5:] if word.startswith(\"<pad>\") else word for word in words]\n",
    "\n",
    "    # check it it the case that, assert that if the word starts with <extra_id_0>, ' ' follows. print the word if it is not the case\n",
    "    for word in words:\n",
    "        if word.startswith(\"<extra_id_0>\") and len(word) > 13:\n",
    "            if word[12] != \" \":\n",
    "                print('word[12] != \\\" \\\"')\n",
    "                print(word)\n",
    "\n",
    "    # if the word starts with <extra_id_0>, remove it\n",
    "    words = [word[12:] if word.startswith(\"<extra_id_0>\") else word for word in words]\n",
    "    # if the word starts with ' ', remove it\n",
    "    words = [word[1:] if word.startswith(\" \") else word for word in words]\n",
    "    # if the word ends with ' ', remove it\n",
    "    words = [word[:-1] if word.endswith(\" \") else word for word in words]\n",
    "    # if the word is empty, remove it\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    # if there are multiple words in word, remove it\n",
    "    words = [word for word in words if len(word.split(\" \")) == 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_option(option):\n",
    "    '''Get the first word from the given option. Return the word.'''\n",
    "    found = False\n",
    "    # if a punctuation can be found in the option, get the word before the punctuation\n",
    "    for i in range(len(option)):\n",
    "        if option[i] in PUNCTUATIONS_LIST:\n",
    "            word = option[:i]\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        return None\n",
    "\n",
    "    # if the word starts with <pad>, remove it\n",
    "    word = word[5:] if word.startswith(\"<pad>\") else word\n",
    "\n",
    "    # check it it the case that, assert that if the word starts with <extra_id_0>, ' ' follows. print the word if it is not the case\n",
    "    if word.startswith(\"<extra_id_0>\") and len(word) > 13:\n",
    "        if word[12] != \" \":\n",
    "            show('word[12] != \\\" \\\"')\n",
    "            show(word)\n",
    "\n",
    "    # if the word starts with <extra_id_0>, remove it\n",
    "    word = word[12:] if word.startswith(\"<extra_id_0>\") else word\n",
    "    # if the word starts with ' ', remove it\n",
    "    word = word[1:] if word.startswith(\" \") else word\n",
    "    # if the word ends with ' ', remove it\n",
    "    word = word[:-1] if word.endswith(\" \") else word\n",
    "    # if the word is empty, remove it\n",
    "    word = word if word != \"\" else None\n",
    "    # if there are multiple words in word, remove it\n",
    "    if word:\n",
    "        word = word if len(word.split(\" \")) == 1 else None\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_punc_pairs(options):\n",
    "    '''given a list of options (completions by the LLM), return a list of word-punc pairs'''\n",
    "    # show(options)\n",
    "    # if a punctuation can be found in the option, get the word before the punctuation\n",
    "    words = []\n",
    "    for option in options:\n",
    "        # find the punctuation\n",
    "        for i in range(len(option)):\n",
    "            if option[i] in PUNCTUATIONS_LIST:\n",
    "                word = option[:i+1]\n",
    "                words.append(word)\n",
    "                # show(words)\n",
    "                break\n",
    "    \n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    words = [word[5:] if word.startswith(\"<pad>\") else word for word in words]\n",
    "    # if the word starts with <extra_id_0>, remove the <extra_id_0>\n",
    "    words = [word[12:] if word.startswith(\"<extra_id_0>\") else word for word in words]\n",
    "    # if the word starts with ' ', remove it\n",
    "    words = [word[1:] if word.startswith(\" \") else word for word in words]\n",
    "    # if the word ends with ' ', remove it\n",
    "    words = [word[:-1] if word.endswith(\" \") else word for word in words]\n",
    "    # if the word is empty, remove it\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    # if there are multiple words in word, remove it\n",
    "    words = [word for word in words if len(word.split(\" \")) == 1]\n",
    "    # if the length is 1, remove it (to prevent the case where it is just a punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    # if the word contains <unk>, remove it\n",
    "    words = [word for word in words if \"<unk>\" not in word]\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pad(options):\n",
    "    '''given a list of options (completions by the LLM), remove the <pad>'''\n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    options = [option[5:] if option.startswith(\"<pad>\") else option for option in options]\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pad_id(options):\n",
    "    '''given a list of options of ids (completions by the LLM), remove the <pad>'''\n",
    "    pad_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    options_return = []\n",
    "    for option in options:\n",
    "        if option[0] == pad_id:\n",
    "            options_return.append(option[1:])\n",
    "        else:\n",
    "            options_return.append(option)\n",
    "    return options_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_first_punc(options):\n",
    "    options_return = []\n",
    "    for option in options:\n",
    "        for i in range(len(option)):\n",
    "            if option[i] in PUNCTUATION_IDS_LIST:\n",
    "                options_return.append(option[:i+1])\n",
    "                break\n",
    "    return options_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# cross entroy loss with logits and labels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='sum'\n",
    "# loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "loss_fn_sum = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for all examples, and then get the words from the options, and compare the first one with the target\n",
    "count_correct = 0\n",
    "count_correct_top_num_beams = 0\n",
    "count_no_words_found = 0\n",
    "id_to_word_and_punc_pairs = {}\n",
    "id_to_word_and_punc_pairs_processed = {}\n",
    "id_to_options = {}\n",
    "for example_index in tqdm(range(len(data_appended))): # len(data_appended)\n",
    "    input_string = data_appended[example_index]['inputs_pretokenized']\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    num_beams = 10\n",
    "    outputs = model.generate(inputs, \n",
    "                             max_length=8,\n",
    "                             num_beams=num_beams, \n",
    "                             num_return_sequences=num_beams, \n",
    "                             output_scores=True,\n",
    "                             eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                             return_dict_in_generate=True)\n",
    "    \n",
    "    options = [tokenizer.decode(outputs['sequences'][i]) for i in range(num_beams)]\n",
    "    # for option in options:\n",
    "    #     print(option)\n",
    "    # print([tokenizer.batch_decode(outputs['sequences'][i]) for i in range(num_beams)])\n",
    "    options_ids = [outputs['sequences'][i] for i in range(num_beams) if get_word_from_option(options[i]) is not None]\n",
    "    words = get_words_from_options(options)\n",
    "    options_without_pad = remove_pad_id(options_ids)\n",
    "    options_without_pad_before_punctution = before_first_punc(options_without_pad)\n",
    "    # print(words)\n",
    "    if words:\n",
    "        # print(options)\n",
    "        # print(words[0], data_appended[example_index]['targets_pretokenized'])\n",
    "        if words[0] == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "            count_correct += 1\n",
    "    else:\n",
    "        count_no_words_found += 1\n",
    "        print(\"no words found\")\n",
    "    word_and_punc_pairs = get_word_punc_pairs(options)\n",
    "    id_to_word_and_punc_pairs[example_index] = word_and_punc_pairs\n",
    "    words_unique = list(set(words))\n",
    "    id_to_word_and_punc_pairs_processed[example_index] = []\n",
    "    id_to_options[example_index] = options_without_pad_before_punctution\n",
    "    for word in words_unique:\n",
    "        found = 0\n",
    "        # iterate through the word and punc pairs, and find the one that matches the word\n",
    "        for word_and_punc_pair in word_and_punc_pairs:\n",
    "            # it is a match if pair = word + punc\n",
    "            for punc in PUNCTUATIONS_LIST:\n",
    "                if word_and_punc_pair == word + punc:\n",
    "                    id_to_word_and_punc_pairs_processed[example_index].append(word_and_punc_pair)\n",
    "                    found = 1\n",
    "                    break\n",
    "            if found == 1:\n",
    "                break\n",
    "    # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "    for word in words_unique:\n",
    "        if word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "            count_correct_top_num_beams += 1\n",
    "            break\n",
    "id_to_options_numpy = {}\n",
    "for key in id_to_options:\n",
    "    options_numpy = []\n",
    "    for option in id_to_options[key]:\n",
    "        options_numpy.append(np.array(option.cpu()))\n",
    "    id_to_options_numpy[key] = options_numpy\n",
    "\n",
    "timed_pickle_file_name = '/work/09127/tomyoung/ls6/data/pkls/t5_lambada_vanilla_beam_search_results_' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + '.pickle'\n",
    "# Save your data to a pickle file\n",
    "\n",
    "with open(timed_pickle_file_name, 'wb') as fp:\n",
    "    pickle.dump({'count_correct': count_correct,\n",
    "                 'count_correct_top_num_beams': count_correct_top_num_beams,\n",
    "                 'count_no_words_found': count_no_words_found,\n",
    "                 'id_to_word_and_punc_pairs': id_to_word_and_punc_pairs,\n",
    "                 'id_to_word_and_punc_pairs_processed': id_to_word_and_punc_pairs_processed,\n",
    "                 'id_to_options_numpy': id_to_options_numpy}, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_options_numpy = {}\n",
    "for key in id_to_options:\n",
    "    options_numpy = []\n",
    "    for option in id_to_options[key]:\n",
    "        options_numpy.append(np.array(option.cpu()))\n",
    "    id_to_options_numpy[key] = options_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed_pickle_file_name = '/work/09127/tomyoung/ls6/data/pkls/t5_lambada_vanilla_beam_search_results_' + str(time.time()) + '.pickle'\n",
    "# Save your data to a pickle file\n",
    "with open(timed_pickle_file_name, 'wb') as fp:\n",
    "    pickle.dump({'count_correct': count_correct,\n",
    "                 'count_correct_top_num_beams': count_correct_top_num_beams,\n",
    "                 'count_no_words_found': count_no_words_found,\n",
    "                 'id_to_word_and_punc_pairs': id_to_word_and_punc_pairs,\n",
    "                 'id_to_word_and_punc_pairs_processed': id_to_word_and_punc_pairs_processed,\n",
    "                 'id_to_options_numpy': id_to_options_numpy}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "# /work/09127/tomyoung/ls6/data/pkls/t5_lambada_vanilla_beam_search_results_2023-06-18 09:50:49.pickle\n",
    "timed_pickle_file_name = '/work/09127/tomyoung/ls6/data/pkls/t5_lambada_vanilla_beam_search_results_2023-06-18 09:50:49.pickle'\n",
    "with open(timed_pickle_file_name, 'rb') as fp:\n",
    "    t5_lambada_vanilla_beam_search_results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_options = t5_lambada_vanilla_beam_search_results['id_to_options_numpy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_options = {}\n",
    "for key in t5_lambada_vanilla_beam_search_results['id_to_options_numpy']:\n",
    "    options = []\n",
    "    for option in t5_lambada_vanilla_beam_search_results['id_to_options_numpy'][key]:\n",
    "        options.append(torch.from_numpy(option).to(device))\n",
    "    id_to_options[key] = options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_log_p_of_option_without_pad(inputs_pretokenized, option, offset=0):\n",
    "    # input_ids: 1*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # labels: 1*len = 32099 + words\n",
    "    labels = option.unsqueeze(0).to(device)\n",
    "    # print('input_ids', input_ids)\n",
    "    # print('labels', labels)\n",
    "    # when offset is used, we move the last offset from input_ids to the front of labels.\n",
    "    if offset != 0:\n",
    "        to_move = input_ids[0][-offset-2:-2]\n",
    "        labels = torch.cat((labels[0][0].unsqueeze(0), to_move, labels[0][1:]), dim=0).unsqueeze(0)\n",
    "        input_ids = torch.cat((input_ids[0][:-offset-2], input_ids[0][-2:]), dim=0).unsqueeze(0)\n",
    "    # print('input_ids offset', input_ids)\n",
    "    # print('labels offset', labels)\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    return -outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsetted(inputs_pretokenized, option, offset=0):\n",
    "    # input_ids: 1*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # labels: 1*len = 32099 + words\n",
    "    labels = option.unsqueeze(0).to(device)\n",
    "    # print('input_ids', input_ids)\n",
    "    # print('labels', labels)\n",
    "    # when offset is used, we move the last offset from input_ids to the front of labels.\n",
    "    if offset != 0:\n",
    "        to_move = input_ids[0][-offset-2:-2]\n",
    "        labels = torch.cat((labels[0][0].unsqueeze(0), to_move, labels[0][1:]), dim=0)\n",
    "        input_ids = torch.cat((input_ids[0][:-offset-2], input_ids[0][-2:]), dim=0)\n",
    "    else:\n",
    "        # squeeze the batch dimension\n",
    "        labels = labels[0]\n",
    "        input_ids = input_ids[0]\n",
    "    # print('input_ids offset', input_ids)\n",
    "    # print('labels offset', labels)\n",
    "    return (input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [07:02<00:00, 12.20it/s]\n"
     ]
    }
   ],
   "source": [
    "''''obtain the offsetted input_ids and labels for each option for each id'''\n",
    "id_and_offset_to_input_and_options = {}\n",
    "max_offset = 30\n",
    "for id in tqdm(range(len(id_to_options))): #len(id_to_options)\n",
    "    # # offset = 0\n",
    "    # id_to_offset_to_input_and_options[(id, 0)] = []\n",
    "    # for option in id_to_options[id]:\n",
    "    #     id_to_offset_to_input_and_options[(id, 0)].append(get_offsetted(data_appended[id]['inputs_pretokenized'], option, offset=0))\n",
    "    # print(id_to_offset_to_input_and_options[(id, 0)])\n",
    "    # print('---------------')\n",
    "    # print('id:', id)\n",
    "    for offset in range(max_offset):\n",
    "        # print('offset:', offset)\n",
    "        id_and_offset_to_input_and_options[(id, offset)] = []\n",
    "        for option in id_to_options[id]:\n",
    "            id_and_offset_to_input_and_options[(id, offset)].append(get_offsetted(data_appended[id]['inputs_pretokenized'], option, offset=offset))\n",
    "            # print(get_offsetted(data_appended[id]['inputs_pretokenized'], option, offset=offset))\n",
    "            # print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save id_and_offset_to_input_and_options to a pickle file\n",
    "pickle_file_name = '/work/09127/tomyoung/ls6/data/pkls/id_and_offset_to_input_and_options_t5.pickle'\n",
    "with open(pickle_file_name, 'wb') as fp:\n",
    "    pickle.dump(id_and_offset_to_input_and_options, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_log_p_of_option_without_pad_batch(inputs_pretokenized_batch, options_batch):\n",
    "    # input_ids: batch_size*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    labels = options_batch.to(device)\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    return -outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' count the number of correct predictions again using get_avg_log_p_of_options_without_pad and ids_to_options_without_pad'''\n",
    "count_correct_avg_log_p_reranking_without_pad = 0\n",
    "for example_index in range(100): # len(data_appended)\n",
    "    # print(example_index)\n",
    "    input_string = data_appended[example_index]['inputs_pretokenized']\n",
    "    option_avg_log_p_max = -10000000\n",
    "    best_option =  \"\"\n",
    "    print('-------------')\n",
    "    for option in id_to_options[example_index]:\n",
    "        avg_log_p, logits = get_avg_log_p_of_option_without_pad(input_string, option, offset=0)\n",
    "        # print(avg_log_p)\n",
    "        # print(logits)\n",
    "        # probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # argmax for each index\n",
    "        # for i in range(probs.shape[1]):\n",
    "            # print(torch.argmax(probs[0][i]))\n",
    "            # print(probs[0][i][torch.argmax(probs[0][i])])\n",
    "        print('avg_log_p', avg_log_p)\n",
    "        if avg_log_p > option_avg_log_p_max:\n",
    "            option_avg_log_p_max = avg_log_p\n",
    "            best_option = option\n",
    "    if best_option != \"\":\n",
    "        best_option_string = tokenizer.decode(best_option)\n",
    "        if get_words_from_options([best_option_string]) != []:\n",
    "            best_word = get_words_from_options([best_option_string])[0]\n",
    "            # print(best_word)\n",
    "            # print(best_option)\n",
    "            if best_word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_avg_log_p_reranking_without_pad += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "''' count the number of correct predictions again using get_avg_log_p_of_options_without_pad and ids_to_options_without_pad using batch processing'''\n",
    "count_correct_avg_log_p_reranking_without_pad_batch = 0\n",
    "id_to_offset_to_option_probs = dict()\n",
    "for example_index in tqdm(range(100)): # len(data_appended)\n",
    "    # print(example_index)\n",
    "    input_string = data_appended[example_index]['inputs_pretokenized']\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    if len(id_to_options[example_index]) == 0:\n",
    "        continue\n",
    "    option_avg_log_p_max = -10000000\n",
    "    best_option =  \"\"\n",
    "    # for option in id_to_options[example_index]:\n",
    "    #     avg_log_p, logits = get_avg_log_p_of_option_without_pad(input_string, option, offset=1)\n",
    "    #     if avg_log_p > option_avg_log_p_max:\n",
    "    #         option_avg_log_p_max = avg_log_p\n",
    "    #         best_option = option\n",
    "    options_batch = torch.nn.utils.rnn.pad_sequence(id_to_options[example_index], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # options_batch\n",
    "    # create a number of input_ids same to the number of elements in ids_to_options[0]\n",
    "    input_ids_batch = torch.cat([input_ids for i in range(len(id_to_options[example_index]))], dim=0)\n",
    "    # print('input_ids_batch', input_ids_batch)\n",
    "    # print('options_batch', options_batch)\n",
    "    outputs = model(input_ids_batch, labels=options_batch)\n",
    "    # print('-------------')\n",
    "    for option_index in range(len(id_to_options[example_index])):\n",
    "        avg_log_p = -loss_fn(outputs.logits[option_index][1:], options_batch[option_index][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "        # print('avg_log_p', avg_log_p)\n",
    "        if avg_log_p > option_avg_log_p_max:\n",
    "            option_avg_log_p_max = avg_log_p\n",
    "            best_option = options_batch[option_index]\n",
    "\n",
    "    if best_option != \"\":\n",
    "        best_option_string = tokenizer.decode(best_option)\n",
    "        if get_words_from_options([best_option_string]) != []:\n",
    "            best_word = get_words_from_options([best_option_string])[0]\n",
    "            if best_word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_avg_log_p_reranking_without_pad_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5153 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [5:38:46<00:00,  3.94s/it]  \n"
     ]
    }
   ],
   "source": [
    "''' obtain the avg_log_ps '''\n",
    "import traceback\n",
    "import datetime\n",
    "max_offset = 30\n",
    "\n",
    "id_and_offset_to_option_probs = dict()\n",
    "failed_example_indices = []\n",
    "for example_index in tqdm(range(len(data_appended))): # len(data_appended)\n",
    "    try:\n",
    "        if len(id_to_options[example_index]) == 0:\n",
    "            continue\n",
    "        for offset in range(max_offset):\n",
    "            options_batch = torch.nn.utils.rnn.pad_sequence([id_and_offset_to_input_and_options[(example_index, offset)][i][1].to(device) for i in range(len(id_to_options[example_index]))], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "            input_ids_batch = torch.cat([id_and_offset_to_input_and_options[(example_index, offset)][i][0].to(device).unsqueeze(0) for i in range(len(id_to_options[example_index]))], dim=0)\n",
    "            outputs = model(input_ids_batch, labels=options_batch)\n",
    "            for option_index in range(len(id_to_options[example_index])):\n",
    "                avg_log_p = -loss_fn(outputs.logits[option_index][1+offset:], options_batch[option_index][1+offset:]) # [1:] to remove the first token <extra_id_0>\n",
    "                id_and_offset_to_option_probs[(example_index, offset, option_index)] = avg_log_p.detach().cpu().tolist()\n",
    "            \n",
    "            # allocated_memory_bytes = torch.cuda.memory_allocated()\n",
    "            # # Convert the allocated memory to gigabytes\n",
    "            # allocated_memory_gb = allocated_memory_bytes / (1024 ** 3)\n",
    "            # print(f\"Current GPU memory allocation: {allocated_memory_gb} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print('example_index:', example_index, ' failed')\n",
    "        failed_example_indices.append(example_index)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# save avg_log_ps into a pickle file with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f'/work/09127/tomyoung/ls6/data/pkls/id_and_offset_to_option_probs_{timestamp}.pickle', 'wb') as handle:\n",
    "    pickle.dump(id_and_offset_to_option_probs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# save failed_example_indices into a pickle file with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f'/work/09127/tomyoung/ls6/data/pkls/failed_example_indices_{timestamp}.pickle', 'wb') as handle:\n",
    "    pickle.dump(failed_example_indices, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_example_indices\n",
    "# save failed_example_indices into a pickle file with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f'failed_example_indices_{timestamp}.pickle', 'wb') as handle:\n",
    "    pickle.dump(failed_example_indices, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_and_offset_to_option_probs_20230509-155934.pickle\n",
    "import pickle\n",
    "with open('id_and_offset_to_option_probs_20230509-155934.pickle', 'rb') as handle:\n",
    "    id_and_offset_to_option_probs = pickle.load(handle)\n",
    "# failed_example_indices_20230509-155934.pickle\n",
    "with open('failed_example_indices_20230509-155934.pickle', 'rb') as handle:\n",
    "    failed_example_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_example_indices_20230524-190511\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/id_and_offset_to_option_probs_20230618-165808.pickle', 'rb') as handle:\n",
    "    id_and_offset_to_option_probs = pickle.load(handle)\n",
    "\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/failed_example_indices_20230618-165808.pickle', 'rb') as handle:\n",
    "    failed_example_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154590/154590 [02:19<00:00, 1111.55it/s]\n"
     ]
    }
   ],
   "source": [
    "id_and_offset_to_input_and_options[(0,0)][0]\n",
    "id_and_offset_to_input_and_options_numpy = dict()\n",
    "for key in tqdm(id_and_offset_to_input_and_options):\n",
    "    id_and_offset_to_input_and_options_numpy[key] = \\\n",
    "        [(id_and_offset_to_input_and_options[key][i][0].detach().cpu().numpy(), \\\n",
    "          id_and_offset_to_input_and_options[key][i][1].detach().cpu().numpy()) \n",
    "          for i in range(len(id_and_offset_to_input_and_options[key]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (0, 0, 2),\n",
       " (0, 0, 3),\n",
       " (0, 1, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 2),\n",
       " (0, 1, 3),\n",
       " (0, 2, 0),\n",
       " (0, 2, 1),\n",
       " (0, 2, 2),\n",
       " (0, 2, 3),\n",
       " (0, 3, 0),\n",
       " (0, 3, 1),\n",
       " (0, 3, 2),\n",
       " (0, 3, 3),\n",
       " (0, 4, 0),\n",
       " (0, 4, 1),\n",
       " (0, 4, 2),\n",
       " (0, 4, 3),\n",
       " (0, 5, 0),\n",
       " (0, 5, 1),\n",
       " (0, 5, 2),\n",
       " (0, 5, 3),\n",
       " (0, 6, 0),\n",
       " (0, 6, 1),\n",
       " (0, 6, 2),\n",
       " (0, 6, 3),\n",
       " (0, 7, 0),\n",
       " (0, 7, 1),\n",
       " (0, 7, 2),\n",
       " (0, 7, 3),\n",
       " (0, 8, 0),\n",
       " (0, 8, 1),\n",
       " (0, 8, 2),\n",
       " (0, 8, 3),\n",
       " (0, 9, 0),\n",
       " (0, 9, 1),\n",
       " (0, 9, 2),\n",
       " (0, 9, 3),\n",
       " (0, 10, 0),\n",
       " (0, 10, 1),\n",
       " (0, 10, 2),\n",
       " (0, 10, 3),\n",
       " (0, 11, 0),\n",
       " (0, 11, 1),\n",
       " (0, 11, 2),\n",
       " (0, 11, 3),\n",
       " (0, 12, 0),\n",
       " (0, 12, 1),\n",
       " (0, 12, 2),\n",
       " (0, 12, 3),\n",
       " (0, 13, 0),\n",
       " (0, 13, 1),\n",
       " (0, 13, 2),\n",
       " (0, 13, 3),\n",
       " (0, 14, 0),\n",
       " (0, 14, 1),\n",
       " (0, 14, 2),\n",
       " (0, 14, 3),\n",
       " (0, 15, 0),\n",
       " (0, 15, 1),\n",
       " (0, 15, 2),\n",
       " (0, 15, 3),\n",
       " (0, 16, 0),\n",
       " (0, 16, 1),\n",
       " (0, 16, 2),\n",
       " (0, 16, 3),\n",
       " (0, 17, 0),\n",
       " (0, 17, 1),\n",
       " (0, 17, 2),\n",
       " (0, 17, 3),\n",
       " (0, 18, 0),\n",
       " (0, 18, 1),\n",
       " (0, 18, 2),\n",
       " (0, 18, 3),\n",
       " (0, 19, 0),\n",
       " (0, 19, 1),\n",
       " (0, 19, 2),\n",
       " (0, 19, 3),\n",
       " (0, 20, 0),\n",
       " (0, 20, 1),\n",
       " (0, 20, 2),\n",
       " (0, 20, 3),\n",
       " (0, 21, 0),\n",
       " (0, 21, 1),\n",
       " (0, 21, 2),\n",
       " (0, 21, 3),\n",
       " (0, 22, 0),\n",
       " (0, 22, 1),\n",
       " (0, 22, 2),\n",
       " (0, 22, 3),\n",
       " (0, 23, 0),\n",
       " (0, 23, 1),\n",
       " (0, 23, 2),\n",
       " (0, 23, 3),\n",
       " (0, 24, 0),\n",
       " (0, 24, 1),\n",
       " (0, 24, 2),\n",
       " (0, 24, 3)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(id_and_offset_to_option_probs.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EOC with max pooling'''\n",
    "max_offset_test = 30\n",
    "offset_to_accuracy = dict()\n",
    "\n",
    "for offset_test in range(max_offset_test):\n",
    "    count_eoc = 0\n",
    "    # postprocess the id_and_offset_to_option_probs to get the best option\n",
    "    for example_index in tqdm(range(len(data_appended))): # len(data_appended)\n",
    "        if len(id_to_options[example_index]) == 0 or example_index in failed_example_indices:\n",
    "            continue\n",
    "        option_avg_log_p_max = -10000000\n",
    "        best_option =  \"\"\n",
    "        for offset in range(offset_test+1):\n",
    "            for option_index in range(100):\n",
    "                if (example_index, offset, option_index) not in id_and_offset_to_option_probs:\n",
    "                    continue\n",
    "                avg_log_p = id_and_offset_to_option_probs[(example_index, offset, option_index)]\n",
    "                if avg_log_p > option_avg_log_p_max:\n",
    "                    option_avg_log_p_max = avg_log_p\n",
    "                    best_option = id_to_options[example_index][option_index]\n",
    "\n",
    "                        \n",
    "        best_option_string = tokenizer.decode(best_option)\n",
    "        # print('best_option_string', best_option_string)\n",
    "        if get_words_from_options([best_option_string]) != []:\n",
    "            best_word = get_words_from_options([best_option_string])[0]\n",
    "            if best_word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "                count_eoc += 1\n",
    "    offset_to_accuracy[offset_test] = count_eoc / (len(data_appended) - len(failed_example_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EOC with avg pooling'''\n",
    "max_offset_test = 30\n",
    "offset_to_accuracy_avg_pooling = dict()\n",
    "for offset_test in range(max_offset_test):\n",
    "    count_eoc = 0\n",
    "    # postprocess the id_and_offset_to_option_probs to get the best option\n",
    "    for example_index in tqdm(range(len(data_appended))): # len(data_appended)\n",
    "        if len(id_to_options[example_index]) == 0 or example_index in failed_example_indices:\n",
    "            continue\n",
    "        option_avg_log_p_avg_over_offset_max = -10000000\n",
    "        best_option =  \"\"\n",
    "        for option_index in range(len(id_to_options[example_index])):\n",
    "            option_avg_log_p_avg_over_offset = 0\n",
    "            for offset in range(offset_test+1):\n",
    "                avg_log_p = id_and_offset_to_option_probs[(example_index, offset, option_index)]\n",
    "                option_avg_log_p_avg_over_offset += avg_log_p\n",
    "            option_avg_log_p_avg_over_offset /= (offset_test+1)\n",
    "            if option_avg_log_p_avg_over_offset > option_avg_log_p_avg_over_offset_max:\n",
    "                option_avg_log_p_avg_over_offset_max = option_avg_log_p_avg_over_offset\n",
    "                best_option = id_to_options[example_index][option_index]\n",
    "        best_option_string = tokenizer.decode(best_option)\n",
    "        # print('best_option_string', best_option_string)\n",
    "        if get_words_from_options([best_option_string]) != []:\n",
    "            best_word = get_words_from_options([best_option_string])[0]\n",
    "            if best_word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "                count_eoc += 1\n",
    "    offset_to_accuracy_avg_pooling[offset_test] = count_eoc / (len(data_appended) - len(failed_example_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the 0: points of offset_to_accuracy\n",
    "offset_to_accuracy_20 = dict()\n",
    "for i in range(20):\n",
    "    offset_to_accuracy_20[i] = offset_to_accuracy[i]\n",
    "offset_to_accuracy_avg_pooling_20 = dict()\n",
    "for i in range(20):\n",
    "    offset_to_accuracy_avg_pooling_20[i] = offset_to_accuracy_avg_pooling[i]\n",
    "\n",
    "# add 1 to all the keys\n",
    "offset_to_accuracy_20_plus_1 = dict()\n",
    "for key in offset_to_accuracy_20:\n",
    "    offset_to_accuracy_20_plus_1[key+1] = offset_to_accuracy_20[key]\n",
    "offset_to_accuracy_avg_pooling_20_plus_1 = dict()\n",
    "for key in offset_to_accuracy_avg_pooling_20:\n",
    "    offset_to_accuracy_avg_pooling_20_plus_1[key+1] = offset_to_accuracy_avg_pooling_20[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load a font\n",
    "font_path = '/usr/share/fonts/urw-base35/NimbusMonoPS-Italic.otf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "max_line = plt.plot(list(offset_to_accuracy_20_plus_1.keys()), list(offset_to_accuracy_20_plus_1.values()),label='max')\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('accuracy')\n",
    "# the interval on x should be 1\n",
    "plt.xticks(np.arange(min(list(offset_to_accuracy_20_plus_1.keys())), max(list(offset_to_accuracy_20_plus_1.keys()))+1, 1.0))\n",
    "# add a dot at each point\n",
    "plt.scatter(list(offset_to_accuracy_20_plus_1.keys()), list(offset_to_accuracy_20_plus_1.values()))\n",
    "\n",
    "# draw the avg pooling\n",
    "avg_line = plt.plot(list(offset_to_accuracy_avg_pooling_20_plus_1.keys()), list(offset_to_accuracy_avg_pooling_20_plus_1.values()), color='red', label='avg')\n",
    "plt.scatter(list(offset_to_accuracy_avg_pooling_20_plus_1.keys()), list(offset_to_accuracy_avg_pooling_20_plus_1.values()), color='red')\n",
    "\n",
    "# add a yellow horizontal line at y=offset_to_accuracy[0]\n",
    "plt.axhline(y=offset_to_accuracy_avg_pooling_20_plus_1[1], color='y', linestyle='--')\n",
    "# add the word \"baseline\" at the end of the yellow line in the font of calibri\n",
    "plt.text(17, offset_to_accuracy_avg_pooling_20_plus_1[1] - 0.0025, 'baseline', fontproperties=font_prop, fontsize=13)\n",
    "plt.scatter(1, offset_to_accuracy_avg_pooling_20_plus_1[1], color='y')\n",
    "\n",
    "plt.legend(handles=[max_line[0], avg_line[0]], loc='upper center', bbox_to_anchor=(0.9, 0.45), ncol=1, fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_to_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(offset_to_accuracy_avg_pooling.keys()), list(offset_to_accuracy_avg_pooling.values()))\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('accuracy')\n",
    "# the interval on x should be 1\n",
    "plt.xticks(np.arange(min(list(offset_to_accuracy_avg_pooling.keys())), max(list(offset_to_accuracy_avg_pooling.keys()))+1, 1.0))\n",
    "# add a dot at each point\n",
    "plt.scatter(list(offset_to_accuracy_avg_pooling.keys()), list(offset_to_accuracy_avg_pooling.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_to_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del options_batch\n",
    "del input_ids_batch\n",
    "del outputs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory allocation: 12.651713848114014 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the current GPU memory allocation\n",
    "allocated_memory_bytes = torch.cuda.memory_allocated(device=0)\n",
    "# Convert the allocated memory to gigabytes\n",
    "allocated_memory_gb = allocated_memory_bytes / (1024 ** 3)\n",
    "print(f\"Current GPU memory allocation: {allocated_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32099,  8029,    15,    17,     3,     5], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Concatenate all options to get a huge tensor'''\n",
    "all_options = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32099,  3957,     3,     5,     0,     0,     0],\n",
       "        [32099,    24,     3,     5,     0,     0,     0],\n",
       "        [32099,  3957,     3,     5,     0,     0,     0],\n",
       "        [32099, 11831,     7,     3,     5,     0,     0],\n",
       "        [32099,     3,     7,  2618,    15,     3,     5],\n",
       "        [32099,  2049,     3,     5,     0,     0,     0],\n",
       "        [32099,   278,     3,    31,     0,     0,     0],\n",
       "        [32099,     3,     7,  2618,    15,     3,     6],\n",
       "        [32099,     3,     7,  2618,    15,     3,    55]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_batch = torch.nn.utils.rnn.pad_sequence(id_to_options[0] + id_to_options[1] , batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "options_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
       " tensor([32099,    24,     3,     5], device='cuda:0'),\n",
       " tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
       " tensor([32099, 11831,     7,     3,     5], device='cuda:0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = data_appended[0]['inputs_pretokenized']\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "id_to_options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.290122258878323"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_no_options = 0\n",
    "no_options_list = []\n",
    "for id in id_to_options:\n",
    "    total_no_options += len(id_to_options[id])\n",
    "    no_options_list.append(len(id_to_options[id]))\n",
    "# avg\n",
    "total_no_options/len(id_to_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test to make sure that padding and concatenating individual examaples doesn't mess up results'''\n",
    "\n",
    "# options is below\n",
    "# [tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,    24,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099, 11831,     7,     3,     5], device='cuda:0')]\n",
    "\n",
    "#convert to options_batch by padding to the max length\n",
    "options_batch = torch.nn.utils.rnn.pad_sequence(id_to_options[0], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "# options_batch\n",
    "# create a number of input_ids same to the number of elements in ids_to_options[0]\n",
    "input_ids_batch = torch.cat([input_ids for i in range(len(id_to_options[0]))], dim=0)\n",
    "\n",
    "outputs = model(input_ids_batch, labels=options_batch)\n",
    "# outputs = model(input_ids_batch[1].unsqueeze(0), labels=options_batch[1].unsqueeze(0))\n",
    "# outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_batch_1 = id_to_options[0][1].unsqueeze(0)\n",
    "input_ids_batch_1 = input_ids\n",
    "outputs_1 = model(input_ids_batch_1, labels=options_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-18.2500, -20.6250, -19.7500,  ..., -46.5000, -48.5000, -46.7500],\n",
      "        [-21.3750, -14.9375, -11.7500,  ..., -47.0000, -49.2500, -48.0000],\n",
      "        [-18.8750, -14.7500,  -8.5000,  ..., -47.2500, -49.5000, -48.2500],\n",
      "        [-24.0000, -14.8750,  -4.2812,  ..., -54.2500, -57.0000, -55.2500]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "tensor([[-18.3750, -20.6250, -19.7500,  ..., -46.5000, -48.7500, -47.0000],\n",
      "        [-21.2500, -14.9375, -11.7500,  ..., -47.0000, -49.2500, -48.0000],\n",
      "        [-18.8750, -14.6875,  -8.3750,  ..., -47.0000, -49.2500, -48.0000],\n",
      "        [-23.7500, -14.8125,  -4.2188,  ..., -54.0000, -56.7500, -55.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)\n",
      "tensor([[32099,    24,     3,     5]], device='cuda:0')\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8281, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs.logits[1][:-1] == outputs_1.logits[0]\n",
    "print(outputs.logits[1][:-1])\n",
    "print(outputs_1.logits[0])\n",
    "print(options_batch_1)\n",
    "\n",
    "loss0 = loss_fn(outputs.logits[1][:-1], options_batch_1[0])\n",
    "loss0_padded = loss_fn(outputs.logits[1], options_batch[1])\n",
    "loss0_1 = loss_fn(outputs_1.logits[0], options_batch_1[0])\n",
    "print(loss0)\n",
    "print(loss0_padded)\n",
    "print(loss0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([32099, 17162,     3,     5], device='cuda:0'),\n",
       " tensor([32099,  1907,    51,     9,     3,     5], device='cuda:0'),\n",
       " tensor([32099,   103,     3,    29,    31], device='cuda:0'),\n",
       " tensor([32099, 17162,     3,     6], device='cuda:0'),\n",
       " tensor([32099,   470,     3,     5], device='cuda:0'),\n",
       " tensor([32099, 17162,     3,     6], device='cuda:0'),\n",
       " tensor([32099,  1907,    51,     9,     3,     6], device='cuda:0'),\n",
       " tensor([32099, 17162,     3,     6], device='cuda:0'),\n",
       " tensor([32099,   103,     3,    29,    31], device='cuda:0'),\n",
       " tensor([32099, 17162,     3,     5], device='cuda:0'),\n",
       " tensor([32099, 17162,     3,     5], device='cuda:0')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
       " tensor([32099,    24,     3,     5], device='cuda:0'),\n",
       " tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
       " tensor([32099, 11831,     7,     3,     5], device='cuda:0')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32128])\n",
      "torch.Size([4, 32128])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits[1].shape)\n",
    "print(outputs_1.logits[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3438, device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss_fn(outputs.logits, options_batch)\n",
    "logits_0 = outputs.logits[1]\n",
    "options_batch_0 = options_batch[1]\n",
    "# logits_0.shape\n",
    "\n",
    "loss = loss_fn(logits_0, options_batch_0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3621e-08, 1.1132e-09, 3.0414e-09,  ..., 7.3057e-21, 9.9262e-22,\n",
      "        5.6910e-21], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(32099, device='cuda:0')\n",
      "tensor(1., device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([4.6202e-10, 2.5518e-07, 6.1691e-06,  ..., 3.0308e-21, 3.1929e-22,\n",
      "        1.1117e-21], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(8, device='cuda:0')\n",
      "tensor(0.1855, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([9.3860e-10, 1.2442e-06, 3.0160e-05,  ..., 2.4458e-20, 3.3087e-21,\n",
      "        8.9997e-21], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(11831, device='cuda:0')\n",
      "tensor(0.2373, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([7.8944e-10, 3.6135e-07, 1.1349e-04,  ..., 1.1581e-21, 1.2160e-22,\n",
      "        4.2352e-22], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0.5703, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([8.4128e-11, 3.8743e-07, 4.8340e-02,  ..., 1.6647e-23, 1.0663e-24,\n",
      "        4.7821e-24], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(5, device='cuda:0')\n",
      "tensor(0.8008, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([9.1968e-09, 1.6689e-05, 1.5831e-04,  ..., 5.3363e-20, 4.3940e-21,\n",
      "        1.1911e-20], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(0.6641, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([1.3024e-09, 1.2591e-06, 7.8125e-02,  ..., 3.7554e-22, 3.0812e-23,\n",
      "        1.0753e-22], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(23, device='cuda:0')\n",
      "tensor(0.8438, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "outputs['scores']\n",
    "for i in range(len(outputs['scores'])):\n",
    "    probs_outputs_beam_scores = torch.nn.functional.softmax(outputs['scores'][i], dim=-1)\n",
    "    print(probs_outputs_beam_scores[0])\n",
    "    # argmax \n",
    "    print(torch.argmax(probs_outputs_beam_scores[0]))\n",
    "    # decode it\n",
    "    # print(tokenizer.decode(torch.argmax(probs_outputs_beam_scores[0])))\n",
    "    # the value\n",
    "    print(probs_outputs_beam_scores[0][torch.argmax(probs_outputs_beam_scores[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/09127/tomyoung/.local/lib/python3.9/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_string = data_appended[0]['inputs_pretokenized']\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "num_beams = 2\n",
    "outputs = model.generate(inputs, \n",
    "                        # max_length=8, \n",
    "                        num_beams=num_beams, \n",
    "                        num_return_sequences=num_beams,\n",
    "                        # eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'),\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6948, device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sequences_scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad><extra_id_0> signs anymore. i'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<extra_id_0>', 'signs', 'anymore', '', '.', '', 'i']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the ids in outputs['sequences'][0] one by one\n",
    "decoded = []\n",
    "for id in outputs['sequences'][0]:\n",
    "    decoded.append(tokenizer.decode(id))\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NLU] in my palm is a clear stone , and inside it is a small ivory statuette . a guardian angel . `` figured if you 're going to be out at night getting hit by cars , you might as well have some backup . '' i look at him , feeling stunned . like this is some sort of sign . but as i stare at harlin , his mouth curved in a confident grin , i do n't care about <extra_id_0>\n",
      "<extra_id_0> signs. <extra_id_1>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-18.6250, -20.8750, -19.6250,  ..., -46.5000, -48.5000, -46.7500],\n",
       "         [-21.5000, -14.5625, -12.0625,  ..., -47.0000, -49.2500, -47.7500],\n",
       "         [-20.3750, -15.2500, -10.1250,  ..., -50.7500, -53.2500, -52.0000],\n",
       "         [-16.8750, -11.0625,  -6.5000,  ..., -42.2500, -44.7500, -43.7500],\n",
       "         [-18.6250, -14.0000,  -4.1562,  ..., -41.5000, -43.7500, -43.0000]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(data_appended[0]['inputs_pretokenized'], return_tensors=\"pt\").input_ids.to(device)\n",
    "print(data_appended[0]['inputs_pretokenized'])\n",
    "labels = tokenizer(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\", return_tensors=\"pt\").input_ids.to(device)\n",
    "print(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\")\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0312, device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss with cross entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1477073/3438722888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[NLG] A man is having a bun for <extra_id_0>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# outputs = model.generate(inputs, num_beams=num_beams, max_length=3, num_return_sequences=num_beams, output_scores=True, return_dict_in_generate=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "input_string = \"[NLG] A man is having a bun for <extra_id_0>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "num_beams = 1\n",
    "# outputs = model.generate(inputs, num_beams=num_beams, max_length=3, num_return_sequences=num_beams, output_scores=True, return_dict_in_generate=True)\n",
    "outputs = model.generate(inputs, max_length=3, output_scores=True, return_dict_in_generate=True)\n",
    "# outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "for i in range(num_beams):\n",
    "    # print(outputs['sequences'][i])\n",
    "    print(tokenizer.decode(outputs['sequences'][i]))\n",
    "    # decode outputs['sequences'][i] one by one token\n",
    "    print('-------------')\n",
    "    # print(outputs['sequences_scores'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl'\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl', 'rb') as f:\n",
    "    acceptable_alternatives_1000_ignore_cws_nos_50_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_json_file = '/work/09127/tomyoung/ls6/data/jsons/c4-validation.00000-of-00001-list-of-lists.json'\n",
    "import json\n",
    "with open(c4_json_file, 'r', encoding='utf8') as f:\n",
    "    dicts_realnewslike = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/09127/tomyoung/.local/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, BartTokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-3b')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed_bigram:  upgrade package\n",
      "preceding_tokens: <s>Also included in\n",
      "following_tokens:  is a new Linux Cluster Install Tool, which automates much of the installation work, IBM officials said.</s>\n"
     ]
    }
   ],
   "source": [
    "'''based on url_to_probs_c4_dict_with_labels_t5_11b_valid and acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "generate the input string and target string for the models like ul2 and glm'''\n",
    "proposed_bigram = t5_tokenizer.decode(url_to_probs_c4_dict_with_labels_t5_11b_valid[((0,17,4),(0,17,5),1)]['proposed bigram'])\n",
    "\n",
    "acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1]\n",
    "\n",
    "proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+1:4+3])\n",
    "print('proposed_bigram:', proposed_bigram)\n",
    "\n",
    "preceding_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][:4])\n",
    "print('preceding_tokens:', preceding_tokens)\n",
    "\n",
    "following_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+3:])\n",
    "print('following_tokens:', following_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22152/22152 [1:55:33<00:00,  3.20it/s]  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "url_to_probs_c4_dict_with_labels_t5_11b_valid_keys = list(url_to_probs_c4_dict_with_labels_t5_11b_valid.keys())\n",
    "\n",
    "url_to_ul2_probs_dict = {}\n",
    "import math\n",
    "for i in tqdm(range(len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys))):#len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys)\n",
    "    key = url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]\n",
    "    url_to_ul2_probs_dict[url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]] = {}\n",
    "    show(key)\n",
    "    show(key[0])\n",
    "    show(acceptable_alternatives_1000_ignore_cws_nos_50_valid[key[0]])\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "    # process the proposed bigram and token\n",
    "    url = key[0]\n",
    "    story_id = key[0][0]\n",
    "    paragraph_id = key[0][1]\n",
    "    option_id = key[2]\n",
    "    proposed_token_pos = key[0][2]\n",
    "\n",
    "    ''' proposed token '''\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid has an <s> </s>\n",
    "    proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos+1:proposed_token_pos+2])\n",
    "    preceding_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][:proposed_token_pos+1])\n",
    "    following_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos+2:])\n",
    "    show('proposed_token:', proposed_token)\n",
    "    show('preceding_tokens_to_proposed_token:', preceding_tokens_to_proposed_token)\n",
    "    show('following_tokens_to_proposed_token:', following_tokens_to_proposed_token)\n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_token + ' <extra_id_0>' + following_tokens_to_proposed_token\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_token + ' <extra_id_1>'\n",
    "    show('target_string_proposed_token:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    show('labels_proposed_token:', labels)\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p = -loss_fn_sum(outputs.logits[0][1:], labels[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_proposed_token:', log_p)\n",
    "    example_raw_sequence = dicts_realnewslike[story_id][paragraph_id]\n",
    "    show('example_raw_sequence:', example_raw_sequence)\n",
    "    # tokenize the raw sequence\n",
    "    example_raw_sequence_bart_tokenized = bart_tokenizer.tokenize(example_raw_sequence)\n",
    "    bart_ids_original = bart_tokenizer.convert_tokens_to_ids(example_raw_sequence_bart_tokenized)\n",
    "    \n",
    "    ''' original token '''\n",
    "    original_token = bart_tokenizer.decode(bart_ids_original[proposed_token_pos])\n",
    "    show('original_token:', original_token)\n",
    "    target_string_original_token = '<extra_id_0>' + original_token + ' <extra_id_1>'\n",
    "    show('target_string_original_token:', target_string_original_token)\n",
    "    labels_original_token = tokenizer(target_string_original_token, return_tensors=\"pt\").input_ids.to(device)\n",
    "    labels_original_token = labels_original_token[:, :-1].contiguous()\n",
    "    show('labels_original_token:', labels_original_token)\n",
    "    outputs = model(inputs, labels=labels_original_token)\n",
    "    log_p_original_token = -loss_fn_sum(outputs.logits[0][1:], labels_original_token[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_original_token:', log_p_original_token)\n",
    "    \n",
    "    \n",
    "    ''' proposed bigram '''\n",
    "    constant_token_pos = key[1][2]\n",
    "    proposed_token_is_to_the_left = proposed_token_pos < constant_token_pos\n",
    "    if proposed_token_is_to_the_left:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos+1:proposed_token_pos+3])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][:proposed_token_pos+1])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos+3:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos:proposed_token_pos+2])\n",
    "    else:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos:proposed_token_pos+2])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][:proposed_token_pos])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][option_id][proposed_token_pos+2:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos-1:proposed_token_pos+1])\n",
    "    show('proposed_bigram:', proposed_bigram)\n",
    "    show('preceding_tokens_to_proposed_bigram:', preceding_tokens_to_proposed_bigram)\n",
    "    show('following_tokens_to_proposed_bigram:', following_tokens_to_proposed_bigram)\n",
    "    show('original_bigram:', original_bigram)\n",
    "    \n",
    "    \n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_bigram + ' <extra_id_0>' + following_tokens_to_proposed_bigram\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_bigram + ' <extra_id_1>'\n",
    "    show('target_string:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_proposed_bigram = -loss_fn_sum(outputs.logits[0][1:], labels[0][1:]) #\n",
    "    show('log_p_proposed_bigram:', log_p_proposed_bigram)\n",
    "\n",
    "\n",
    "    '''original bigram'''\n",
    "    target_string_original_bigram = '<extra_id_0>' + original_bigram + ' <extra_id_1>'\n",
    "    show('target_string_original_bigram:', target_string_original_bigram)\n",
    "    labels_original_bigram = tokenizer(target_string_original_bigram, return_tensors=\"pt\").input_ids.to(device)\n",
    "    labels_original_bigram = labels_original_bigram[:, :-1].contiguous()\n",
    "    outputs = model(inputs, labels=labels_original_bigram)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_original_bigram = -loss_fn_sum(outputs.logits[0][1:], labels_original_bigram[0][1:]) #\n",
    "    show('log_p_original_bigram:', log_p_original_bigram)\n",
    "    # put the log_p's into the dictionary\n",
    "    url_to_ul2_probs_dict[key] = {'proposed token': math.exp(log_p.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original token': math.exp(log_p_original_token.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'proposed bigram': math.exp(log_p_proposed_bigram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original bigram': math.exp(log_p_original_bigram.to(torch.float32).detach().cpu().numpy())}\n",
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_probs_c4_dict_with_labels_t5_11b_valid[((0,17,4),(0,17,5),1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
