{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import pickle\n",
    "# clear GPU memory\n",
    "from utils import general_utils, eoc\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from typing import Tuple, List\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Load model\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\",\n",
    "                                                        cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2',\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "UL2_MODE = '[NLG]' # '[S2S]' is not supported by some functions (poor accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and specify partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"Rowan/hellaswag\" # NOTE: hellaswag only has the validation set\n",
    "\n",
    "# validation lens\n",
    "# inputs max: 130, min: 19, avg: 66.98307110137422\n",
    "# completions max: 125, min: 7, avg: 45.010655247958574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEVELOPMENT = True\n",
    "data = load_dataset(DATASET_PATH)\n",
    "set_partition = 'validation' if IS_DEVELOPMENT else 'test' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bd403791b54a53d53a955db1ef38500eb6b7ad4fe29e1164e8d455e82a529ad2\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True # shuffle data['validation']\n",
    "if RUN_CELL:\n",
    "    data['validation'] = data['validation'].shuffle(seed=42)\n",
    "    # check sha256 to make sure shuffled data is the same\n",
    "    print(general_utils.hash_object(data['validation'][:1000]))\n",
    "    # bd403791b54a53d53a955db1ef38500eb6b7ad4fe29e1164e8d455e82a529ad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use 1000 samples for development\n",
    "if IS_DEVELOPMENT:\n",
    "    data['validation'] = data['validation'].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and get extra ids\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'\n",
    "extra_id_0 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")])\n",
    "extra_id_1 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_1>\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define example generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False\n",
    "show = print if RUN_CELL else lambda *args, **kwargs: None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_generator(docs, tokenizer) -> Tuple:\n",
    "    '''\n",
    "        docs: DATA_SET[SUBJECTS_NAME], ex:HELLASWG_DATAS[validation]\n",
    "        return: Tuple(input_ids, completions_ids_padded, labels)\n",
    "\n",
    "        input[example]: ctx:<prompt> \n",
    "        label[example]: endings list -> list[] \n",
    "\n",
    "        Todo: few-shot data prompting\n",
    "    '''\n",
    "    example_id = 0\n",
    "    for doc in docs:\n",
    "        endings_list = doc['endings']\n",
    "        input_ = UL2_MODE + \" \" + doc[\"activity_label\"] + \": \" + doc['ctx'] + \" \" + \"<extra_id_0>\"\n",
    "        if UL2_MODE == \"[NLG]\":\n",
    "            completions = [f\"<extra_id_0> {ending}\" for ending in endings_list]\n",
    "        elif UL2_MODE == \"[S2S]\":\n",
    "            completions = [f\"{ending}\" for ending in endings_list]\n",
    "        \n",
    "        # label = f\"{endings_list[int(index)]}\"\n",
    "        show(input_)\n",
    "        show(completions)\n",
    "\n",
    "        input_ids = tokenizer(input_, return_tensors=\"pt\").input_ids\n",
    "        # label_id = tokenizer(label, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        completions_ids = [tokenizer(completion, return_tensors=\"pt\").input_ids[:,:-1]\\\n",
    "                                                                for completion in completions] # remove <eos> token with [:,:-1]\n",
    "\n",
    "        # Assuming `max_length` is the maximum length you want to pad sequences to\n",
    "        max_length = max(seq.size(1) for seq in completions_ids)\n",
    "\n",
    "        # Pad sequences to the common length\n",
    "        padded_sequences = [F.pad(seq, (0, max_length - seq.size(1)), value=tokenizer.pad_token_id) for seq in completions_ids]\n",
    "        # Use pad_sequence\n",
    "        completions_ids_padded = torch.nn.utils.rnn.pad_sequence(padded_sequences, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        completions_ids_padded = torch.squeeze(completions_ids_padded, dim = 1)\n",
    "        yield example_id, input_ids, completions_ids_padded, int(doc['label'])\n",
    "        example_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 934.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input max: 123, min: 19, avg: 67.378\n",
      "completion max: 85, min: 10, avg: 44.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True    # get input and completion lens\n",
    "if RUN_CELL:\n",
    "    input_lens = []\n",
    "    completion_lens = []\n",
    "    gen = example_generator(data[set_partition], tokenizer)\n",
    "    for example_id, input_ids, completions_batch, label in tqdm(gen, total=len(data[set_partition])):\n",
    "        input_lens.append(len(input_ids[0]))\n",
    "        for completion in completions_batch:\n",
    "            completion_lens.append(len(completion))\n",
    "    print(f\"input max: {max(input_lens)}, min: {min(input_lens)}, avg: {sum(input_lens)/len(input_lens)}\")\n",
    "    print(f\"completion max: {max(completion_lens)}, min: {min(completion_lens)}, avg: {sum(completion_lens)/len(completion_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:50<00:00,  9.09it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = False   # generate baseline info and conditionals\n",
    "\n",
    "if RUN_CELL:\n",
    "    baseline = dict() \n",
    "    # save the label and the number of completions\n",
    "    gen = example_generator(data[set_partition], tokenizer)\n",
    "    for example_id, input_ids, completions_batch, label in tqdm(gen, total=len(data[set_partition])):\n",
    "        baseline[example_id] = dict()\n",
    "        baseline[example_id]['label'] = label\n",
    "        baseline[example_id]['no_completions'] = len(completions_batch)\n",
    "        baseline[example_id]['p_map'] = []\n",
    "        p_and_completion = []\n",
    "\n",
    "        outputs = eoc.multi_labels_forward(model, input_ids.cuda(), completions_batch.cuda())\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            if UL2_MODE == \"[NLG]\":            \n",
    "                p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1:], \n",
    "                    completions_batch[completion_index][1:].cuda()\n",
    "                )\n",
    "            elif UL2_MODE == \"[S2S]\":\n",
    "                p = -ce_loss(\n",
    "                    outputs.logits[completion_index], \n",
    "                    completions_batch[completion_index].cuda()\n",
    "                )\n",
    "\n",
    "            baseline[example_id]['p_map'] += [p.detach().cpu().tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Save baseline\n",
    "if RUN_CELL:\n",
    "    pickle.dump(baseline, open(\"data/pkls/hellaswg/ul2/baseline_1000.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = True   # Load baseline\n",
    "if RUN_CELL:\n",
    "    '''Load the baseline p_maps'''\n",
    "    with open(\"data/pkls/hellaswg/ul2/baseline_1000.pkl\", \"rb\") as handle:\n",
    "        baseline = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-offset Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:53<00:00,  8.81it/s]\n",
      "100%|██████████| 1000/1000 [01:46<00:00,  9.38it/s]\n",
      "100%|██████████| 1000/1000 [01:49<00:00,  9.14it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True \n",
    "if RUN_CELL:\n",
    "    MAX_OFFSET = 3\n",
    "    p_map_offset = dict() # maps (example_id, offset, completion_index) -> p_map\n",
    "    for offset in range(1, MAX_OFFSET+1):\n",
    "        gen = example_generator(data[set_partition], tokenizer)\n",
    "        for example_id, input_ids, completions_batch, label in tqdm(gen, total=len(data[set_partition])):\n",
    "            input_ids_offset, labels_offset = eoc.create_offset_sample_from_batch(\n",
    "                tokenizer,\n",
    "                input_ids,\n",
    "                completions_batch,\n",
    "                offset\n",
    "            )\n",
    "            outputs = eoc.multi_labels_forward(model, input_ids_offset.cuda(), labels_offset.cuda())\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the original completion and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1+offset:], \n",
    "                    labels_offset[completion_index][1+offset:].cuda()\n",
    "                )\n",
    "                p_map_offset[(example_id, offset, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multispan Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # generate multispan conditionals\n",
    "if RUN_CELL:\n",
    "    if UL2_MODE != \"[NLG]\":\n",
    "        raise ValueError(\"Only NLG mode is supported for multispan conditionals for now\")\n",
    "    span_length = 1\n",
    "    gap_between_spans = 1\n",
    "    num_spans = 1\n",
    "    p_map_multispan = dict()\n",
    "    gen = example_generator(data[set_partition], tokenizer)\n",
    "\n",
    "    for example_id, input_ids, completions_batch, label in tqdm(gen, total=len(data[set_partition])):\n",
    "        # print(input_ids.shape)\n",
    "        # continue\n",
    "        inputs_ids_multispan, labels_multispan = eoc.create_multiple_span_sample_from_batch(\n",
    "            tokenizer,\n",
    "            input_ids[0], # squeeze 1st dim\n",
    "            completions_batch,\n",
    "            span_length,\n",
    "            gap_between_spans,\n",
    "            num_spans,\n",
    "        )\n",
    "        outputs = eoc.multi_labels_forward(model, inputs_ids_multispan.cuda(), labels_multispan.cuda())\n",
    "\n",
    "        # assert multispan samples are correct \n",
    "        assert completions_batch[completion_index].nonzero().shape[0] == \\\n",
    "            labels_multispan[completion_index][num_spans * (span_length + 1) :].nonzero().shape[0]\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            avg_log_p = -ce_loss(\n",
    "                # Only care about the tokens corresponding to the completion (see assert below)); \n",
    "                # so the first <extra_id_0> is omitted, and for each span, the span + <extra_id_k> is omitted;\n",
    "                # totally 1 + num_spans * (span_length + 1) tokens are omitted;\n",
    "                # labels_multispan contains paddings.\n",
    "                outputs.logits[completion_index][1 + num_spans * (span_length + 1) :], \n",
    "                labels_multispan[completion_index][1 + num_spans * (span_length + 1) :].cuda()\n",
    "            )\n",
    "            p_map_multispan[(example_id, span_length, gap_between_spans, num_spans, completion_index)] = \\\n",
    "                avg_log_p.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 365739.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True    # Max reduction to emsemble conditionals for the same last word\n",
    "'''Max reduction to emsemble conditionals, i.e., only the maximum avg_log_p is kept for each completion.\n",
    "Emsemble the baseline conditionals with the K-offset conditionals and middle-off conditionals.'''\n",
    "\n",
    "if RUN_CELL:\n",
    "    # Add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "    ADD_BASELINE = True\n",
    "\n",
    "    # Add K-offset conditionals to the list\n",
    "    ADD_K_OFFSET = False\n",
    "    MAX_OFFSET = 3\n",
    "\n",
    "    # Add multispan conditionals to the list\n",
    "    ADD_MULTISPAN = False\n",
    "    LENGTH_GAP_NUM_TUPLES = [\n",
    "        (1, 1, 1), \n",
    "    ] # SPAN_LENGTH, GAP_LENGTH, NUM_SPANS. NUM_SPANS can be float, which is treated as auto_ratio.\n",
    "\n",
    "    count_correct = 0\n",
    "    for example_index in tqdm(range(len(data[set_partition]))): # len(lambada)\n",
    "        no_completions = baseline[example_index]['no_completions']\n",
    "        # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "        p_and_completion = []\n",
    "        \n",
    "        # add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "        if ADD_BASELINE:\n",
    "            p_and_completion += [\n",
    "                (baseline[example_index]['p_map'][completion_index], completion_index)\n",
    "                for completion_index in range(no_completions)\n",
    "            ]\n",
    "            \n",
    "        # add the whole K-offset ensemble to the list\n",
    "        if ADD_K_OFFSET:\n",
    "            for offset in range(1, MAX_OFFSET + 1):\n",
    "                p_and_completion += [\n",
    "                    (p_map_offset[(example_index, offset, completion_index)], completion_index)\n",
    "                    for completion_index in range(no_completions)\n",
    "                ]\n",
    "                \n",
    "        if ADD_MULTISPAN:\n",
    "            p_and_completion += [\n",
    "                (p_map_multispan[(example_index, *length_gap_num, completion_index)], completion_index)\n",
    "                for completion_index in range(no_completions)\n",
    "                for length_gap_num in LENGTH_GAP_NUM_TUPLES\n",
    "            ]\n",
    "\n",
    "        # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "        _, best_completion_index = max(p_and_completion, key=lambda x: x[0])\n",
    "        if best_completion_index == baseline[example_index]['label']:\n",
    "            count_correct += 1\n",
    "    print(\"accuracy:\", count_correct / len(data[set_partition]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Log\n",
    "\n",
    "#### NLG\n",
    "baseline: 0.758\n",
    "\n",
    "3,3,1 works poorly \n",
    "\n",
    "3,5,1 works poorly: 0.732 (w/o baseline)\n",
    "\n",
    "2,1,1 works poorly: 0.735 (w/o baseline)\n",
    "    \n",
    "1,1,1 works poorly: 0.729 (w/o baseline)\n",
    "\n",
    "#### S2S\n",
    "baseline: 0.712\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
