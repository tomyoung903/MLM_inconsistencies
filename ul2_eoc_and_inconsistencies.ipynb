{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use UL2 to \n",
    "\n",
    "(1) measure inconsistencies in its bidirectional conditionals; \n",
    "\n",
    "(2) improve llm inference with Emsemble of Conditionals.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [Imports and global utils](#0)\n",
    "\n",
    "* [Load tokenizer and model](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px;\"><a class=\"anchor\" id=\"0\"></a>Imports and global utils</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/nus/tomyoung/miniconda3/envs/mlm_inconsistencies/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3\"\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''a simple way to toggle print statements'''\n",
    "print_status = True\n",
    "if print_status:\n",
    "    show = print\n",
    "else:\n",
    "    show = lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px;\"><a class=\"anchor\" id=\"1\"></a>Load tokenizer and model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 15.89s/it]\n",
      "/scratch/users/nus/tomyoung/miniconda3/envs/mlm_inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1587: FutureWarning: `T5ForConditionalGeneration.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'encoder.block.0': 0, 'encoder.block.1': 1, ...}\n",
      "  warnings.warn(\n",
      "/scratch/users/nus/tomyoung/miniconda3/envs/mlm_inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:925: FutureWarning: `T5Stack.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0, 'block.1': 1, ...}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", \n",
    "                                                   cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2', \n",
    "                                                   low_cpu_mem_usage=True, \n",
    "                                                   torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model.parallelize() # TODO: the first gpu doesn't reduce its memory usage, but 9GB/18GB is on every other GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBADA_TEST_DATA_PATH = \"data/jsonls/test.jsonl\"\n",
    "\n",
    "with open(LAMBADA_TEST_DATA_PATH, \"r\") as f:\n",
    "    lambada = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "# To use the NLG mode of UL2, append [NLG] to the beginning of each input, and <extra_id_0> to the end\n",
    "lambada = [\n",
    "    {\n",
    "        \"inputs_pretokenized\": \"[NLG] \" + x['inputs_pretokenized'] + \" <extra_id_0>\",\n",
    "        \"targets_pretokenized\": x['targets_pretokenized']\n",
    "    } \n",
    "    for x in lambada\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get a list of valid punctuations that can END a sentence. If the model generates one, it is considered that the sentence is complete\n",
    "and we can parse for the last word'''\n",
    "\n",
    "ENDING_PUNCTUATIONS = ',!.:;?'\n",
    "_vocab = tokenizer.get_vocab()\n",
    "ENDING_PUNCTUATIONS_IDS_LIST = [_vocab[p] for p in ENDING_PUNCTUATIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get the first word from each of the given completions (completions by the model [String]). Return the words.'''\n",
    "def get_words_from_completions(completions):\n",
    "    # if a punctuation can be found in the completion, get the word before the punctuation\n",
    "    words = []\n",
    "    for completion in completions:\n",
    "        # find the punctuation\n",
    "        for i in range(len(completion)):\n",
    "            if completion[i] in ENDING_PUNCTUATIONS:\n",
    "                word = completion[:i]\n",
    "                words.append(word)\n",
    "                # print(words)\n",
    "                break\n",
    "\n",
    "    # if the word starts with <pad>, remove it\n",
    "    words = [word[5:] if word.startswith(\"<pad>\") else word for word in words]\n",
    "\n",
    "    # check it it the case that, assert that if the word starts with <extra_id_0>, ' ' follows. print the word if it is not the case\n",
    "    for word in words:\n",
    "        if word.startswith(\"<extra_id_0>\") and len(word) > 13:\n",
    "            if word[12] != \" \":\n",
    "                print('word[12] != \\\" \\\"')\n",
    "                print(word)\n",
    "\n",
    "    # if the word starts with <extra_id_0>, remove it\n",
    "    words = [word[12:] if word.startswith(\"<extra_id_0>\") else word for word in words]\n",
    "    # if the word starts with ' ', remove it\n",
    "    words = [word[1:] if word.startswith(\" \") else word for word in words]\n",
    "    # if the word ends with ' ', remove it\n",
    "    words = [word[:-1] if word.endswith(\" \") else word for word in words]\n",
    "    # if the word is empty, remove it\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    # if there are multiple words in word, remove it\n",
    "    words = [word for word in words if len(word.split(\" \")) == 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_completion(completion):\n",
    "    '''Get the first word from the given completion (a completion by the model [String]). Return the word.'''\n",
    "    found = False\n",
    "    # if a punctuation can be found in the completion, get the string before the punctuation\n",
    "    for i in range(len(completion)):\n",
    "        if completion[i] in ENDING_PUNCTUATIONS:\n",
    "            word = completion[:i]\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        return None\n",
    "\n",
    "\n",
    "    '''postprocess the string to remove the <pad> and <extra_id_0> tokens to get the word'''\n",
    "    # if the word starts with <pad>, remove it\n",
    "    word = word[5:] if word.startswith(\"<pad>\") else word\n",
    "\n",
    "    # check it it the case that, assert that if the word starts with <extra_id_0>, ' ' follows. print the word if it is not the case\n",
    "    # if word.startswith(\"<extra_id_0>\") and len(word) > 13:\n",
    "    #     if word[12] != \" \":\n",
    "    #         show('word[12] != \\\" \\\"')\n",
    "    #         show(word)\n",
    "\n",
    "    # if the word starts with <extra_id_0>, remove it\n",
    "    word = word[12:] if word.startswith(\"<extra_id_0>\") else word\n",
    "    # if the word starts with ' ', remove it\n",
    "    word = word[1:] if word.startswith(\" \") else word\n",
    "    # if the word ends with ' ', remove it\n",
    "    word = word[:-1] if word.endswith(\" \") else word\n",
    "    # if the word is empty, discount it\n",
    "    word = word if word != \"\" else None\n",
    "    # if there are multiple words in it, discount it\n",
    "    if word:\n",
    "        word = word if len(word.split(\" \")) == 1 else None\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_punc_pairs(completions):\n",
    "    '''given a list of completions (completions by the LLM), return a list of word-punc pairs'''\n",
    "    # show(completions)\n",
    "    # if a punctuation can be found in the completion, get the word before the punctuation\n",
    "    words = []\n",
    "    for completion in completions:\n",
    "        # find the punctuation\n",
    "        for i in range(len(completion)):\n",
    "            if completion[i] in ENDING_PUNCTUATIONS:\n",
    "                word = completion[:i+1]\n",
    "                words.append(word)\n",
    "                # show(words)\n",
    "                break\n",
    "    \n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    words = [word[5:] if word.startswith(\"<pad>\") else word for word in words]\n",
    "    # if the word starts with <extra_id_0>, remove the <extra_id_0>\n",
    "    words = [word[12:] if word.startswith(\"<extra_id_0>\") else word for word in words]\n",
    "    # if the word starts with ' ', remove it\n",
    "    words = [word[1:] if word.startswith(\" \") else word for word in words]\n",
    "    # if the word ends with ' ', remove it\n",
    "    words = [word[:-1] if word.endswith(\" \") else word for word in words]\n",
    "    # if the word is empty, remove it\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    # if there are multiple words in word, remove it\n",
    "    words = [word for word in words if len(word.split(\" \")) == 1]\n",
    "    # if the length is 1, remove it (to prevent the case where it is just a punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    # if the word contains <unk>, remove it\n",
    "    words = [word for word in words if \"<unk>\" not in word]\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pad(completions):\n",
    "    '''given a list of completions (completions by the LLM), remove the <pad>'''\n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    completions = [completion[5:] if completion.startswith(\"<pad>\") else completion for completion in completions]\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pad_id(completions):\n",
    "    '''given a list of completions of ids (completions by the LLM), remove the <pad>'''\n",
    "    pad_id = tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    "    # if the word starts with <pad>, remove the <pad>\n",
    "    completions_return = []\n",
    "    for completion in completions:\n",
    "        if completion[0] == pad_id:\n",
    "            completions_return.append(completion[1:])\n",
    "        else:\n",
    "            completions_return.append(completion)\n",
    "    return completions_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_first_punc(completions):\n",
    "    '''given a list of completions (completions by the LLM), return the string before the first punctuation'''\n",
    "    completions_return = []\n",
    "    for completion in completions:\n",
    "        for i in range(len(completion)):\n",
    "            if completion[i] in ENDING_PUNCTUATIONS_IDS_LIST:\n",
    "                completions_return.append(completion[:i+1])\n",
    "                break\n",
    "    return completions_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entroy loss with logits and labels\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='sum'\n",
    "loss_fn_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [00:01<00:00, 3835.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''temporary testing stuff'''\n",
    "min_len = 100000\n",
    "max_len = 0\n",
    "for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "    input_string = lambada[example_index]['inputs_pretokenized']\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    if len(inputs[0]) < min_len:\n",
    "        min_len = len(inputs[0])\n",
    "    if len(inputs[0]) > max_len:\n",
    "        max_len = len(inputs[0])\n",
    "# min_len\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n"
     ]
    }
   ],
   "source": [
    "'''On lambada, generate the top completions (completions) for each example, and get the word from each completion'''\n",
    "# generate for all examples, and then get the words from the completions, and compare the first one with the target\n",
    "count_correct = 0\n",
    "count_correct_top_num_beams = 0\n",
    "count_no_words_found = 0\n",
    "id_to_word_and_punc_pairs = {}\n",
    "id_to_word_and_punc_pairs_processed = {}\n",
    "id_to_completions = {}\n",
    "\n",
    "MAX_COMPLETION_LENGTH = 8\n",
    "NUM_BEAMS = 20\n",
    "\n",
    "for example_index in tqdm(range(1)): # len(lambada)\n",
    "    input_string = lambada[example_index]['inputs_pretokenized']\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=MAX_COMPLETION_LENGTH, # for last word prediction, 8 is sufficient\n",
    "                             num_beams=NUM_BEAMS, \n",
    "                             num_return_sequences=NUM_BEAMS, \n",
    "                             output_scores=True,\n",
    "                             eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                             return_dict_in_generate=True)\n",
    "    \n",
    "    completions = [tokenizer.decode(outputs['sequences'][i]) for i in range(NUM_BEAMS)]\n",
    "    # print([tokenizer.batch_decode(outputs['sequences'][i]) for i in range(num_beams)])\n",
    "    completions_ids = [outputs['sequences'][i] for i in range(NUM_BEAMS) \n",
    "                   if get_word_from_completion(completions[i]) is not None]\n",
    "\n",
    "    words = get_words_from_completions(completions)\n",
    "    completions_without_pad = remove_pad_id(completions_ids)\n",
    "    completions_without_pad_before_punctution = before_first_punc(completions_without_pad)\n",
    "    # print(words)\n",
    "    if words:\n",
    "        # print(completions)\n",
    "        # print(words[0], lambada[example_index]['targets_pretokenized'])\n",
    "        if words[0] == lambada[example_index]['targets_pretokenized'][0]:\n",
    "            count_correct += 1\n",
    "    else:\n",
    "        count_no_words_found += 1\n",
    "        print(\"no words found\")\n",
    "    word_and_punc_pairs = get_word_punc_pairs(completions)\n",
    "    id_to_word_and_punc_pairs[example_index] = word_and_punc_pairs\n",
    "    words_unique = list(set(words))\n",
    "    id_to_word_and_punc_pairs_processed[example_index] = []\n",
    "    id_to_completions[example_index] = completions_without_pad_before_punctution\n",
    "    for word in words_unique:\n",
    "        found = 0\n",
    "        # iterate through the word and punc pairs, and find the one that matches the word\n",
    "        for word_and_punc_pair in word_and_punc_pairs:\n",
    "            # it is a match if pair = word + punc\n",
    "            for punc in ENDING_PUNCTUATIONS:\n",
    "                if word_and_punc_pair == word + punc:\n",
    "                    id_to_word_and_punc_pairs_processed[example_index].append(word_and_punc_pair)\n",
    "                    found = 1\n",
    "                    break\n",
    "            if found == 1:\n",
    "                break\n",
    "    # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "    for word in words_unique:\n",
    "        if word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "            count_correct_top_num_beams += 1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><extra_id_0> any of that. ',\n",
       " '<pad><extra_id_0> signs anymore. i',\n",
       " '<pad><extra_id_0> any of that anymore.',\n",
       " '<pad><extra_id_0> that anymore. i',\n",
       " '<pad><extra_id_0> the sign. i',\n",
       " '<pad><extra_id_0> the angel. i',\n",
       " '<pad><extra_id_0> a sign. ',\n",
       " '<pad><extra_id_0> signs. i just',\n",
       " '<pad><extra_id_0> the sign anymore. ',\n",
       " '<pad><extra_id_0> any of that. all',\n",
       " '<pad><extra_id_0> the angel anymore. ',\n",
       " '<pad><extra_id_0> any of it. ',\n",
       " '<pad><extra_id_0> anything else. i',\n",
       " '<pad><extra_id_0> that. i just',\n",
       " '<pad><extra_id_0> that anymore. <unk>',\n",
       " '<pad><extra_id_0> signs anymore. <unk>',\n",
       " '<pad><extra_id_0> signs. i ',\n",
       " '<pad><extra_id_0> the signs anymore. ',\n",
       " '<pad><extra_id_0> angels. i',\n",
       " '<pad><extra_id_0> signs or backup. ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,   136,    13,    24,     3,     5,     3],\n",
       "        [    0, 32099,  3957,  7595,     3,     5,     3,    23],\n",
       "        [    0, 32099,   136,    13,    24,  7595,     3,     5],\n",
       "        [    0, 32099,    24,  7595,     3,     5,     3,    23],\n",
       "        [    0, 32099,     8,  1320,     3,     5,     3,    23],\n",
       "        [    0, 32099,     8, 11831,     3,     5,     3,    23],\n",
       "        [    0, 32099,     3,     9,  1320,     3,     5,     3],\n",
       "        [    0, 32099,  3957,     3,     5,     3,    23,   131],\n",
       "        [    0, 32099,     8,  1320,  7595,     3,     5,     3],\n",
       "        [    0, 32099,   136,    13,    24,     3,     5,    66],\n",
       "        [    0, 32099,     8, 11831,  7595,     3,     5,     3],\n",
       "        [    0, 32099,   136,    13,    34,     3,     5,     3],\n",
       "        [    0, 32099,   959,  1307,     3,     5,     3,    23],\n",
       "        [    0, 32099,    24,     3,     5,     3,    23,   131],\n",
       "        [    0, 32099,    24,  7595,     3,     5,     3,     2],\n",
       "        [    0, 32099,  3957,  7595,     3,     5,     3,     2],\n",
       "        [    0, 32099,  3957,     3,     5,     3,    23,     3],\n",
       "        [    0, 32099,     8,  3957,  7595,     3,     5,     3],\n",
       "        [    0, 32099, 11831,     7,     3,     5,     3,    23],\n",
       "        [    0, 32099,  3957,    42,  7169,     3,     5,     3]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_completions_numpy = {}\n",
    "for key in id_to_completions:\n",
    "    completions_numpy = []\n",
    "    for completion in id_to_completions[key]:\n",
    "        completions_numpy.append(np.array(completion.cpu()))\n",
    "    id_to_completions_numpy[key] = completions_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed_pickle_file_name = 'ul2_lambada_vanilla_beam_search_results_' + str(time.time()) + '.pickle'\n",
    "# Save your data to a pickle file\n",
    "with open(timed_pickle_file_name, 'wb') as fp:\n",
    "    pickle.dump({'count_correct': count_correct,\n",
    "                 'count_correct_top_num_beams': count_correct_top_num_beams,\n",
    "                 'count_no_words_found': count_no_words_found,\n",
    "                 'id_to_word_and_punc_pairs': id_to_word_and_punc_pairs,\n",
    "                 'id_to_word_and_punc_pairs_processed': id_to_word_and_punc_pairs_processed,\n",
    "                 'id_to_completions_numpy': id_to_completions_numpy}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "# /work/09127/tomyoung/ls6/inconsistencies_project/ul2_lambada_vanilla_beam_search_results_1683476272.4741185.pickle\n",
    "timed_pickle_file_name = '/work/09127/tomyoung/ls6/inconsistencies_project/ul2_lambada_vanilla_beam_search_results_1683476272.4741185.pickle'\n",
    "with open(timed_pickle_file_name, 'rb') as fp:\n",
    "    ul2_lambada_vanilla_beam_search_results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_completions = {}\n",
    "for key in ul2_lambada_vanilla_beam_search_results['id_to_completions_numpy']:\n",
    "    completions = []\n",
    "    for completion in ul2_lambada_vanilla_beam_search_results['id_to_completions_numpy'][key]:\n",
    "        completions.append(torch.from_numpy(completion))\n",
    "    id_to_completions[key] = completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(lambada[0]['inputs_pretokenized'], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(lambada[0]['inputs_pretokenized'])\n",
    "labels = tokenizer(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\")\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_log_p_of_completion_without_pad(inputs_pretokenized, completion, offset=0):\n",
    "    # input_ids: 1*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # labels: 1*len = 32099 + words\n",
    "    labels = completion.unsqueeze(0).to(\"cuda\")\n",
    "    # print('input_ids', input_ids)\n",
    "    # print('labels', labels)\n",
    "    # when offset is used, we move the last offset from input_ids to the front of labels.\n",
    "    if offset != 0:\n",
    "        to_move = input_ids[0][-offset-2:-2]\n",
    "        labels = torch.cat((labels[0][0].unsqueeze(0), to_move, labels[0][1:]), dim=0).unsqueeze(0)\n",
    "        input_ids = torch.cat((input_ids[0][:-offset-2], input_ids[0][-2:]), dim=0).unsqueeze(0)\n",
    "    # print('input_ids offset', input_ids)\n",
    "    # print('labels offset', labels)\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    return -outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsetted(inputs_pretokenized, completion, offset=0):\n",
    "    # input_ids: 1*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # labels: 1*len = 32099 + words\n",
    "    labels = completion.unsqueeze(0).to(\"cuda\")\n",
    "    # print('input_ids', input_ids)\n",
    "    # print('labels', labels)\n",
    "    # when offset is used, we move the last offset from input_ids to the front of labels.\n",
    "    if offset != 0:\n",
    "        to_move = input_ids[0][-offset-2:-2]\n",
    "        labels = torch.cat((labels[0][0].unsqueeze(0), to_move, labels[0][1:]), dim=0)\n",
    "        input_ids = torch.cat((input_ids[0][:-offset-2], input_ids[0][-2:]), dim=0)\n",
    "    else:\n",
    "        # squeeze the batch dimension\n",
    "        labels = labels[0]\n",
    "        input_ids = input_ids[0]\n",
    "    # print('input_ids offset', input_ids)\n",
    "    # print('labels offset', labels)\n",
    "    return (input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''obtain the offsetted input_ids and labels for each completion for each id'''\n",
    "id_and_offset_to_input_and_completions = {}\n",
    "max_offset = 5\n",
    "for id in range(len(id_to_completions)): #len(id_to_completions)\n",
    "    # # offset = 0\n",
    "    # id_to_offset_to_input_and_completions[(id, 0)] = []\n",
    "    # for completion in id_to_completions[id]:\n",
    "    #     id_to_offset_to_input_and_completions[(id, 0)].append(get_offsetted(lambada[id]['inputs_pretokenized'], completion, offset=0))\n",
    "    # print(id_to_offset_to_input_and_completions[(id, 0)])\n",
    "    # print('---------------')\n",
    "    # print('id:', id)\n",
    "    for offset in range(max_offset):\n",
    "        # print('offset:', offset)\n",
    "        id_and_offset_to_input_and_completions[(id, offset)] = []\n",
    "        for completion in id_to_completions[id]:\n",
    "            id_and_offset_to_input_and_completions[(id, offset)].append(get_offsetted(lambada[id]['inputs_pretokenized'], completion, offset=offset))\n",
    "            # print(get_offsetted(lambada[id]['inputs_pretokenized'], completion, offset=offset))\n",
    "            # print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_log_p_of_completion_without_pad_batch(inputs_pretokenized_batch, completions_batch):\n",
    "    # input_ids: batch_size*len = words + 32099 + 1\n",
    "    input_ids = tokenizer(inputs_pretokenized_batch, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "    labels = completions_batch.to(\"cuda\")\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    return -outputs.loss, outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' count the number of correct predictions again using get_avg_log_p_of_completions_without_pad and ids_to_completions_without_pad'''\n",
    "count_correct_avg_log_p_reranking_without_pad = 0\n",
    "for example_index in range(100): # len(lambada)\n",
    "    # print(example_index)\n",
    "    input_string = lambada[example_index]['inputs_pretokenized']\n",
    "    completion_avg_log_p_max = -10000000\n",
    "    best_completion =  \"\"\n",
    "    print('-------------')\n",
    "    for completion in id_to_completions[example_index]:\n",
    "        avg_log_p, logits = get_avg_log_p_of_completion_without_pad(input_string, completion, offset=0)\n",
    "        # print(avg_log_p)\n",
    "        # print(logits)\n",
    "        # probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        # argmax for each index\n",
    "        # for i in range(probs.shape[1]):\n",
    "            # print(torch.argmax(probs[0][i]))\n",
    "            # print(probs[0][i][torch.argmax(probs[0][i])])\n",
    "        print('avg_log_p', avg_log_p)\n",
    "        if avg_log_p > completion_avg_log_p_max:\n",
    "            completion_avg_log_p_max = avg_log_p\n",
    "            best_completion = completion\n",
    "    if best_completion != \"\":\n",
    "        best_completion_string = tokenizer.decode(best_completion)\n",
    "        if get_words_from_completions([best_completion_string]) != []:\n",
    "            best_word = get_words_from_completions([best_completion_string])[0]\n",
    "            # print(best_word)\n",
    "            # print(best_completion)\n",
    "            if best_word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_avg_log_p_reranking_without_pad += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' count the number of correct predictions again using get_avg_log_p_of_completions_without_pad and ids_to_completions_without_pad using batch processing'''\n",
    "count_correct_avg_log_p_reranking_without_pad_batch = 0\n",
    "id_to_offset_to_completion_probs = dict()\n",
    "for example_index in tqdm(range(100)): # len(lambada)\n",
    "    # print(example_index)\n",
    "    input_string = lambada[example_index]['inputs_pretokenized']\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    if len(id_to_completions[example_index]) == 0:\n",
    "        continue\n",
    "    completion_avg_log_p_max = -10000000\n",
    "    best_completion =  \"\"\n",
    "    # for completion in id_to_completions[example_index]:\n",
    "    #     avg_log_p, logits = get_avg_log_p_of_completion_without_pad(input_string, completion, offset=1)\n",
    "    #     if avg_log_p > completion_avg_log_p_max:\n",
    "    #         completion_avg_log_p_max = avg_log_p\n",
    "    #         best_completion = completion\n",
    "    completions_batch = torch.nn.utils.rnn.pad_sequence(id_to_completions[example_index], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # completions_batch\n",
    "    # create a number of input_ids same to the number of elements in ids_to_completions[0]\n",
    "    input_ids_batch = torch.cat([input_ids for i in range(len(id_to_completions[example_index]))], dim=0)\n",
    "    # print('input_ids_batch', input_ids_batch)\n",
    "    # print('completions_batch', completions_batch)\n",
    "    outputs = model(input_ids_batch, labels=completions_batch)\n",
    "    # print('-------------')\n",
    "    for completion_index in range(len(id_to_completions[example_index])):\n",
    "        avg_log_p = -loss_fn(outputs.logits[completion_index][1:], completions_batch[completion_index][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "        # print('avg_log_p', avg_log_p)\n",
    "        if avg_log_p > completion_avg_log_p_max:\n",
    "            completion_avg_log_p_max = avg_log_p\n",
    "            best_completion = completions_batch[completion_index]\n",
    "\n",
    "    if best_completion != \"\":\n",
    "        best_completion_string = tokenizer.decode(best_completion)\n",
    "        if get_words_from_completions([best_completion_string]) != []:\n",
    "            best_word = get_words_from_completions([best_completion_string])[0]\n",
    "            if best_word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_avg_log_p_reranking_without_pad_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_input_length = 0\n",
    "for example_index in tqdm(range(100)): # len(lambada)\n",
    "    input_string = lambada[example_index]['inputs_pretokenized']\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    min_input_length = min(min_input_length, len(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' obtain the avg_log_ps '''\n",
    "import traceback\n",
    "import datetime\n",
    "\n",
    "id_and_offset_to_completion_probs = dict()\n",
    "failed_example_indices = []\n",
    "for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "    try:\n",
    "        if len(id_to_completions[example_index]) == 0:\n",
    "            continue\n",
    "        for offset in range(max_offset):\n",
    "            completions_batch = torch.nn.utils.rnn.pad_sequence([id_and_offset_to_input_and_completions[(example_index, offset)][i][1] for i in range(len(id_to_completions[example_index]))], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "            input_ids_batch = torch.cat([id_and_offset_to_input_and_completions[(example_index, offset)][i][0].unsqueeze(0) for i in range(len(id_to_completions[example_index]))], dim=0)\n",
    "            outputs = model(input_ids_batch, labels=completions_batch)\n",
    "            for completion_index in range(len(id_to_completions[example_index])):\n",
    "                avg_log_p = -loss_fn(outputs.logits[completion_index][1+offset:], completions_batch[completion_index][1+offset:]) # [1:] to remove the first token <extra_id_0>\n",
    "                id_and_offset_to_completion_probs[(example_index, offset, completion_index)] = avg_log_p.detach().cpu().tolist()\n",
    "            \n",
    "            # allocated_memory_bytes = torch.cuda.memory_allocated()\n",
    "            # # Convert the allocated memory to gigabytes\n",
    "            # allocated_memory_gb = allocated_memory_bytes / (1024 ** 3)\n",
    "            # print(f\"Current GPU memory allocation: {allocated_memory_gb} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print('example_index:', example_index, ' failed')\n",
    "        failed_example_indices.append(example_index)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# save avg_log_ps into a pickle file with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f'id_and_offset_to_completion_probs_{timestamp}.pickle', 'wb') as handle:\n",
    "    pickle.dump(id_and_offset_to_completion_probs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_example_indices\n",
    "# save failed_example_indices into a pickle file with timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f'failed_example_indices_{timestamp}.pickle', 'wb') as handle:\n",
    "    pickle.dump(failed_example_indices, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_and_offset_to_completion_probs_20230509-155934.pickle\n",
    "import pickle\n",
    "with open('id_and_offset_to_completion_probs_20230528-064154_max_offset_61.pickle', 'rb') as handle:\n",
    "    id_and_offset_to_completion_probs = pickle.load(handle)\n",
    "# failed_example_indices_20230509-155934.pickle\n",
    "with open('failed_example_indices_20230528-064154_61.pickle', 'rb') as handle:\n",
    "    failed_example_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_example_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# id_and_offset_to_completion_probs_20230516-222830_max_offset_21.pickle\n",
    "with open('id_and_offset_to_completion_probs_20230516-222830_max_offset_21.pickle', 'rb') as handle:\n",
    "    id_and_offset_to_completion_probs = pickle.load(handle)\n",
    "# failed_example_indices_20230516-222830_max_offset_21.pickle\n",
    "with open('failed_example_indices_20230516-222830_21.pickle', 'rb') as handle:\n",
    "    failed_example_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(id_and_offset_to_completion_probs.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "'''EOC with max pooling'''\n",
    "max_offset_test = 60\n",
    "offset_to_accuracy = dict()\n",
    "for offset_test in range(max_offset_test):\n",
    "    count_eoc = 0\n",
    "    # postprocess the id_and_offset_to_completion_probs to get the best completion\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        if len(id_to_completions[example_index]) == 0 or example_index in failed_example_indices:\n",
    "            continue\n",
    "        completion_avg_log_p_max = -10000000\n",
    "        best_completion =  \"\"\n",
    "        for offset in range(offset_test+1):\n",
    "            for completion_index in range(len(id_to_completions[example_index])):\n",
    "                avg_log_p = id_and_offset_to_completion_probs[(example_index, offset, completion_index)]\n",
    "                if avg_log_p > completion_avg_log_p_max:\n",
    "                    completion_avg_log_p_max = avg_log_p\n",
    "                    best_completion = id_to_completions[example_index][completion_index]\n",
    "\n",
    "        best_completion_string = tokenizer.decode(best_completion)\n",
    "        # print('best_completion_string', best_completion_string)\n",
    "        if get_words_from_completions([best_completion_string]) != []:\n",
    "            best_word = get_words_from_completions([best_completion_string])[0]\n",
    "            if best_word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_eoc += 1\n",
    "    offset_to_accuracy[offset_test] = count_eoc / (len(lambada) - len(failed_example_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EOC with avg pooling'''\n",
    "max_offset_test = 60\n",
    "offset_to_accuracy_avg_pooling = dict()\n",
    "for offset_test in range(max_offset_test):\n",
    "    count_eoc = 0\n",
    "    # postprocess the id_and_offset_to_completion_probs to get the best completion\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        if len(id_to_completions[example_index]) == 0 or example_index in failed_example_indices:\n",
    "            continue\n",
    "        completion_avg_log_p_avg_over_offset_max = -10000000\n",
    "        best_completion =  \"\"\n",
    "        for completion_index in range(len(id_to_completions[example_index])):\n",
    "            completion_avg_log_p_avg_over_offset = 0\n",
    "            for offset in range(offset_test+1):\n",
    "                avg_log_p = id_and_offset_to_completion_probs[(example_index, offset, completion_index)]\n",
    "                completion_avg_log_p_avg_over_offset += avg_log_p\n",
    "            completion_avg_log_p_avg_over_offset /= (offset_test+1)\n",
    "            if completion_avg_log_p_avg_over_offset > completion_avg_log_p_avg_over_offset_max:\n",
    "                completion_avg_log_p_avg_over_offset_max = completion_avg_log_p_avg_over_offset\n",
    "                best_completion = id_to_completions[example_index][completion_index]\n",
    "        best_completion_string = tokenizer.decode(best_completion)\n",
    "        # print('best_completion_string', best_completion_string)\n",
    "        if get_words_from_completions([best_completion_string]) != []:\n",
    "            best_word = get_words_from_completions([best_completion_string])[0]\n",
    "            if best_word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_eoc += 1\n",
    "    offset_to_accuracy_avg_pooling[offset_test] = count_eoc / (len(lambada) - len(failed_example_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load a font\n",
    "font_path = '/usr/share/fonts/urw-base35/NimbusMonoPS-Italic.otf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# offset = 0 corresponds to the baseline, which is no. ensembled conditionals = 1; adjust the offset by 1\n",
    "no_ensembled_conditionals_to_accuracy = dict()\n",
    "for offset in range(1, max_offset_test+1):\n",
    "    no_ensembled_conditionals_to_accuracy[offset] = offset_to_accuracy[offset-1]\n",
    "\n",
    "\n",
    "max_line = plt.plot(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()), label='max')\n",
    "plt.xlabel('No. ensembled conditionals', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "# the interval on x should be 10\n",
    "plt.xticks(np.arange(10, max(list(no_ensembled_conditionals_to_accuracy.keys()))+1, 10))\\\n",
    "# add a tick at 1 on the x axis\n",
    "plt.xticks(list(plt.xticks()[0]) + [1])\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# add a dot at each point\n",
    "plt.scatter(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()))\n",
    "\n",
    "\n",
    "# add a yellow horizontal line at y=offset_to_accuracy[0]\n",
    "plt.axhline(y=no_ensembled_conditionals_to_accuracy[1], color='y', linestyle='--')\n",
    "# add the word \"baseline\" at the end of the yellow line in the font of calibri\n",
    "plt.text(48, no_ensembled_conditionals_to_accuracy[1] + 0.0002, 'baseline', fontproperties=font_prop, fontsize=13)\n",
    "\n",
    "# plot the accuracy with avg pooling\n",
    "avg_line = plt.plot([item+1 for item in list(offset_to_accuracy_avg_pooling.keys())], list(offset_to_accuracy_avg_pooling.values()), color='r', label='avg')\n",
    "# add a dot at each point\n",
    "plt.scatter([item+1 for item in list(offset_to_accuracy_avg_pooling.keys())], list(offset_to_accuracy_avg_pooling.values()), color='r')\n",
    "\n",
    "plt.scatter(1, no_ensembled_conditionals_to_accuracy[1], color='y')\n",
    "\n",
    "\n",
    "plt.legend(handles=[max_line[0], avg_line[0]], loc='upper center', bbox_to_anchor=(0.9, 0.45), ncol=1, fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot at a high resolution\n",
    "plt.savefig('no_ensembled_conditionals_to_accuracy_combined.png', dpi=1200)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_to_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "\n",
    "print('Number of fonts: ', len(fonts))\n",
    "for font in fonts:\n",
    "    print(font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(offset_to_accuracy.keys()))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(offset_to_accuracy_avg_pooling.keys()), list(offset_to_accuracy_avg_pooling.values()))\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('accuracy')\n",
    "# the interval on x should be 1\n",
    "plt.xticks(np.arange(min(list(offset_to_accuracy_avg_pooling.keys())), max(list(offset_to_accuracy_avg_pooling.keys()))+1, 1.0))\n",
    "# add a dot at each point\n",
    "plt.scatter(list(offset_to_accuracy_avg_pooling.keys()), list(offset_to_accuracy_avg_pooling.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the current GPU memory allocation\n",
    "allocated_memory_bytes = torch.cuda.memory_allocated(device=0)\n",
    "# Convert the allocated memory to gigabytes\n",
    "allocated_memory_gb = allocated_memory_bytes / (1024 ** 3)\n",
    "print(f\"Current GPU memory allocation: {allocated_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Concatenate all completions to get a huge tensor'''\n",
    "all_completions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_batch = torch.nn.utils.rnn.pad_sequence(id_to_completions[0] + id_to_completions[1] , batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "completions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = lambada[0]['inputs_pretokenized']\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "id_to_completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_completions = 0\n",
    "no_completions_list = []\n",
    "for id in id_to_completions:\n",
    "    total_no_completions += len(id_to_completions[id])\n",
    "    no_completions_list.append(len(id_to_completions[id]))\n",
    "# avg\n",
    "total_no_completions/len(id_to_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test to make sure that padding and concatenating individual examaples doesn't mess up results'''\n",
    "\n",
    "# completions is below\n",
    "# [tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,    24,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099, 11831,     7,     3,     5], device='cuda:0')]\n",
    "\n",
    "#convert to completions_batch by padding to the max length\n",
    "completions_batch = torch.nn.utils.rnn.pad_sequence(id_to_completions[0], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "# completions_batch\n",
    "# create a number of input_ids same to the number of elements in ids_to_completions[0]\n",
    "input_ids_batch = torch.cat([input_ids for i in range(len(id_to_completions[0]))], dim=0)\n",
    "\n",
    "outputs = model(input_ids_batch, labels=completions_batch)\n",
    "# outputs = model(input_ids_batch[1].unsqueeze(0), labels=completions_batch[1].unsqueeze(0))\n",
    "# outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_batch_1 = id_to_completions[0][1].unsqueeze(0)\n",
    "input_ids_batch_1 = input_ids\n",
    "outputs_1 = model(input_ids_batch_1, labels=completions_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits[1][:-1] == outputs_1.logits[0]\n",
    "print(outputs.logits[1][:-1])\n",
    "print(outputs_1.logits[0])\n",
    "print(completions_batch_1)\n",
    "\n",
    "loss0 = loss_fn(outputs.logits[1][:-1], completions_batch_1[0])\n",
    "loss0_padded = loss_fn(outputs.logits[1], completions_batch[1])\n",
    "loss0_1 = loss_fn(outputs_1.logits[0], completions_batch_1[0])\n",
    "print(loss0)\n",
    "print(loss0_padded)\n",
    "print(loss0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits[1].shape)\n",
    "print(outputs_1.logits[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn(outputs.logits, completions_batch)\n",
    "logits_0 = outputs.logits[1]\n",
    "completions_batch_0 = completions_batch[1]\n",
    "# logits_0.shape\n",
    "\n",
    "loss = loss_fn(logits_0, completions_batch_0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['scores']\n",
    "for i in range(len(outputs['scores'])):\n",
    "    probs_outputs_beam_scores = torch.nn.functional.softmax(outputs['scores'][i], dim=-1)\n",
    "    print(probs_outputs_beam_scores[0])\n",
    "    # argmax \n",
    "    print(torch.argmax(probs_outputs_beam_scores[0]))\n",
    "    # decode it\n",
    "    # print(tokenizer.decode(torch.argmax(probs_outputs_beam_scores[0])))\n",
    "    # the value\n",
    "    print(probs_outputs_beam_scores[0][torch.argmax(probs_outputs_beam_scores[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = lambada[0]['inputs_pretokenized']\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "num_beams = 2\n",
    "outputs = model.generate(inputs, \n",
    "                        # max_length=8, \n",
    "                        num_beams=num_beams, \n",
    "                        num_return_sequences=num_beams,\n",
    "                        # eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'),\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(lambada[0]['inputs_pretokenized'], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(lambada[0]['inputs_pretokenized'])\n",
    "labels = tokenizer(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\")\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss with cross entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] A man is having a bun for <extra_id_0>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "num_beams = 1\n",
    "# outputs = model.generate(inputs, num_beams=num_beams, max_length=3, num_return_sequences=num_beams, output_scores=True, return_dict_in_generate=True)\n",
    "outputs = model.generate(inputs, max_length=3, output_scores=True, return_dict_in_generate=True)\n",
    "# outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "for i in range(num_beams):\n",
    "    # print(outputs['sequences'][i])\n",
    "    print(tokenizer.decode(outputs['sequences'][i]))\n",
    "    # decode outputs['sequences'][i] one by one token\n",
    "    print('-------------')\n",
    "    # print(outputs['sequences_scores'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl'\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl', 'rb') as f:\n",
    "    acceptable_alternatives_1000_ignore_cws_nos_50_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_alternatives_1000_ignore_cws_nos_50_valid[(180, 11, 8)][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(acceptable_alternatives_1000_ignore_cws_nos_50_valid.keys())[320:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(acceptable_alternatives_1000_ignore_cws_nos_50_valid.keys())\n",
    "for key in all_keys[1000:2000]:\n",
    "    if len(acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3]) > 1:\n",
    "        print(key)\n",
    "        for item in acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3]:\n",
    "            print(item)\n",
    "        # print(*acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_realnewslike[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_json_file = '/work/09127/tomyoung/ls6/data/jsons/c4-validation.00000-of-00001-list-of-lists.json'\n",
    "import json\n",
    "with open(c4_json_file, 'r', encoding='utf8') as f:\n",
    "    dicts_realnewslike = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, BartTokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-3b')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''based on url_to_probs_c4_dict_with_labels_t5_11b_valid and acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "generate the input string and target string for the models like ul2 and glm'''\n",
    "proposed_bigram = t5_tokenizer.decode(url_to_probs_c4_dict_with_labels_t5_11b_valid[((0,17,4),(0,17,5),1)]['proposed bigram'])\n",
    "\n",
    "acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1]\n",
    "\n",
    "proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+1:4+3])\n",
    "print('proposed_bigram:', proposed_bigram)\n",
    "\n",
    "preceding_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][:4])\n",
    "print('preceding_tokens:', preceding_tokens)\n",
    "\n",
    "following_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+3:])\n",
    "print('following_tokens:', following_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_probs_c4_dict_with_labels_t5_11b_valid_keys = list(url_to_probs_c4_dict_with_labels_t5_11b_valid.keys())\n",
    "\n",
    "url_to_ul2_probs_dict = {}\n",
    "import math\n",
    "for i in tqdm(range(len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys))):#len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys)\n",
    "    key = url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]\n",
    "    url_to_ul2_probs_dict[url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]] = {}\n",
    "    show(key)\n",
    "    show(key[0])\n",
    "    show(acceptable_alternatives_1000_ignore_cws_nos_50_valid[key[0]])\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "    # process the proposed bigram and token\n",
    "    url = key[0]\n",
    "    story_id = key[0][0]\n",
    "    paragraph_id = key[0][1]\n",
    "    completion_id = key[2]\n",
    "    proposed_token_pos = key[0][2]\n",
    "\n",
    "    ''' proposed token '''\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid has an <s> </s>\n",
    "    proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+1:proposed_token_pos+2])\n",
    "    preceding_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos+1])\n",
    "    following_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+2:])\n",
    "    show('proposed_token:', proposed_token)\n",
    "    show('preceding_tokens_to_proposed_token:', preceding_tokens_to_proposed_token)\n",
    "    show('following_tokens_to_proposed_token:', following_tokens_to_proposed_token)\n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_token + ' <extra_id_0>' + following_tokens_to_proposed_token\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_token + ' <extra_id_1>'\n",
    "    show('target_string_proposed_token:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    show('labels_proposed_token:', labels)\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p = -loss_fn_sum(outputs.logits[0][1:], labels[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_proposed_token:', log_p)\n",
    "    example_raw_sequence = dicts_realnewslike[story_id][paragraph_id]\n",
    "    show('example_raw_sequence:', example_raw_sequence)\n",
    "    # tokenize the raw sequence\n",
    "    example_raw_sequence_bart_tokenized = bart_tokenizer.tokenize(example_raw_sequence)\n",
    "    bart_ids_original = bart_tokenizer.convert_tokens_to_ids(example_raw_sequence_bart_tokenized)\n",
    "    \n",
    "    ''' original token '''\n",
    "    original_token = bart_tokenizer.decode(bart_ids_original[proposed_token_pos])\n",
    "    show('original_token:', original_token)\n",
    "    target_string_original_token = '<extra_id_0>' + original_token + ' <extra_id_1>'\n",
    "    show('target_string_original_token:', target_string_original_token)\n",
    "    labels_original_token = tokenizer(target_string_original_token, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_token = labels_original_token[:, :-1].contiguous()\n",
    "    show('labels_original_token:', labels_original_token)\n",
    "    outputs = model(inputs, labels=labels_original_token)\n",
    "    log_p_original_token = -loss_fn_sum(outputs.logits[0][1:], labels_original_token[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_original_token:', log_p_original_token)\n",
    "    \n",
    "    \n",
    "    ''' proposed bigram '''\n",
    "    constant_token_pos = key[1][2]\n",
    "    proposed_token_is_to_the_left = proposed_token_pos < constant_token_pos\n",
    "    if proposed_token_is_to_the_left:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+1:proposed_token_pos+3])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos+1])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+3:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos:proposed_token_pos+2])\n",
    "    else:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos:proposed_token_pos+2])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+2:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos-1:proposed_token_pos+1])\n",
    "    show('proposed_bigram:', proposed_bigram)\n",
    "    show('preceding_tokens_to_proposed_bigram:', preceding_tokens_to_proposed_bigram)\n",
    "    show('following_tokens_to_proposed_bigram:', following_tokens_to_proposed_bigram)\n",
    "    show('original_bigram:', original_bigram)\n",
    "    \n",
    "    \n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_bigram + ' <extra_id_0>' + following_tokens_to_proposed_bigram\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_bigram + ' <extra_id_1>'\n",
    "    show('target_string:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_proposed_bigram = -loss_fn_sum(outputs.logits[0][1:], labels[0][1:]) #\n",
    "    show('log_p_proposed_bigram:', log_p_proposed_bigram)\n",
    "\n",
    "\n",
    "    '''original bigram'''\n",
    "    target_string_original_bigram = '<extra_id_0>' + original_bigram + ' <extra_id_1>'\n",
    "    show('target_string_original_bigram:', target_string_original_bigram)\n",
    "    labels_original_bigram = tokenizer(target_string_original_bigram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_bigram = labels_original_bigram[:, :-1].contiguous()\n",
    "    outputs = model(inputs, labels=labels_original_bigram)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_original_bigram = -loss_fn_sum(outputs.logits[0][1:], labels_original_bigram[0][1:]) #\n",
    "    show('log_p_original_bigram:', log_p_original_bigram)\n",
    "    # put the log_p's into the dictionary\n",
    "    url_to_ul2_probs_dict[key] = {'proposed token': math.exp(log_p.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original token': math.exp(log_p_original_token.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'proposed bigram': math.exp(log_p_proposed_bigram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original bigram': math.exp(log_p_original_bigram.to(torch.float32).detach().cpu().numpy())}\n",
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] The headquarters of microsoft is in <extra_id_0>\"                                               \n",
    "\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, max_length=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] The headquarters of microsoft is in <extra_id_0>\"\n",
    "target_string = \"<extra_id_0> redmond washington <extra_id_1>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = labels[:, :-1].contiguous()\n",
    "outputs = model(inputs, labels=labels)\n",
    "print('outputs:', outputs)\n",
    "log_p = -loss_fn_sum(outputs.logits[0][1:-1], labels[0][1:-1]) #\n",
    "print('log_p:', log_p)\n",
    "\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "for i in range(len(labels[0])):\n",
    "    print('label:', labels[0][i], 'prob:', probs[0][i][labels[0][i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/dict_url_to_completions_5_grams.pkl','rb') as f:\n",
    "    dict_url_to_completions_5_grams = pickle.load(f)\n",
    "# get a list of punctuations\n",
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_url_to_completions_5_grams_keys = list(dict_url_to_completions_5_grams.keys())\n",
    "url_to_ul2_5_gram_probs_dict = {}\n",
    "# make sure it ends with a punctuation\n",
    "for i in tqdm(range(1)):\n",
    "    key = dict_url_to_completions_5_grams_keys[i]\n",
    "    # print('key:', key)\n",
    "    # print(dict_url_to_completions_5_grams[key])\n",
    "    # alternative: make sure it ends with a punctuation\n",
    "    if dict_url_to_completions_5_grams[key]['alternative'][-1] not in punctuations:\n",
    "        continue\n",
    "    # for 10-grams\n",
    "    # get the input string \n",
    "    original_sentence_words = dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')\n",
    "    # len should >= 15\n",
    "    if len(original_sentence_words) < 15:\n",
    "        continue\n",
    "    input_for_10_grams = '[NLG] ' + ' '.join(original_sentence_words[:-10]) + ' <extra_id_0>'\n",
    "    input_for_10_grams_ids = tokenizer(input_for_10_grams, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # original 10-gram\n",
    "    original_10_gram = \"<extra_id_0> \" + ' '.join(original_sentence_words[-10:]) + ' <extra_id_1>'\n",
    "    labels_original_10_gram = tokenizer(original_10_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_10_gram = labels_original_10_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_10_grams_ids, labels=labels_original_10_gram)\n",
    "    log_p_original_10_gram = -loss_fn_sum(outputs.logits[0][1:-1], labels_original_10_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    \n",
    "    # probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # for i in range(len(labels_original_10_gram[0])):\n",
    "    #     print('label:', labels_original_10_gram[0][i], 'prob:', probs[0][i][labels_original_10_gram[0][i]])\n",
    "\n",
    "    # continue\n",
    "    # proposed 10-gram\n",
    "    proposed_10_gram = \"<extra_id_0> \" + \\\n",
    "     ' '.join(original_sentence_words[-10:-5]) + ' ' + \\\n",
    "    dict_url_to_completions_5_grams[key]['alternative'] + ' <extra_id_1>'\n",
    "    labels_proposed_10_gram = tokenizer(proposed_10_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_proposed_10_gram = labels_proposed_10_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_10_grams_ids, labels=labels_proposed_10_gram)\n",
    "    log_p_proposed_10_gram = -loss_fn_sum(outputs.logits[0][1:-1], labels_proposed_10_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # for 5-grams\n",
    "    # get the input string\n",
    "    input_for_5_grams = '[NLG] ' + ' '.join(original_sentence_words[:-5]) + ' <extra_id_0>'\n",
    "    input_for_5_grams_ids = tokenizer(input_for_5_grams, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # original 5-gram\n",
    "    original_5_gram = \"<extra_id_0> \" + ' '.join(original_sentence_words[-5:]) + ' <extra_id_1>'\n",
    "    labels_original_5_gram = tokenizer(original_5_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_5_gram = labels_original_5_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_5_grams_ids, labels=labels_original_5_gram)\n",
    "    log_p_original_5_gram = -loss_fn_sum(outputs.logits[0][1:-1], labels_original_5_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # proposed 5-gram\n",
    "    proposed_5_gram = \"<extra_id_0> \" + dict_url_to_completions_5_grams[key]['alternative'] + ' <extra_id_1>'\n",
    "    labels_proposed_5_gram = tokenizer(proposed_5_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_proposed_5_gram = labels_proposed_5_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_5_grams_ids, labels=labels_proposed_5_gram)\n",
    "    log_p_proposed_5_gram = -loss_fn_sum(outputs.logits[0][1:-1], labels_proposed_5_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # add them to the dictionary\n",
    "    url_to_ul2_5_gram_probs_dict[key] = {'proposed 5_gram': math.exp(log_p_proposed_5_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'original 5_gram': math.exp(log_p_original_5_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'proposed 10_gram': math.exp(log_p_proposed_10_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'original 10_gram': math.exp(log_p_original_10_gram.to(torch.float32).detach().cpu().numpy())}                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save url_to_ul2_5_gram_probs_dict as a pkl\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_5_gram_probs_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_5_gram_probs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of different tokens between the two 5-grams\n",
    "for key in dict_url_to_completions_5_grams.keys():\n",
    "    proposed_5_gram = dict_url_to_completions_5_grams[key]['alternative'].split(' ')\n",
    "    original_5_gram = dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')[-5:]\n",
    "    number_of_different_tokens = 0\n",
    "    for i in range(5):\n",
    "        if proposed_5_gram[i] != original_5_gram[i]:\n",
    "            number_of_different_tokens += 1\n",
    "    print('key:', key)\n",
    "    print('original_sentence:', dict_url_to_completions_5_grams[key]['original_sentence'])\n",
    "    print('proposed_5_gram:', dict_url_to_completions_5_grams[key]['alternative'])\n",
    "    print('original_5_gram:', ' '.join(dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')[-5:]))\n",
    "    print(number_of_different_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
