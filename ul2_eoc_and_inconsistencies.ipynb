{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code Use UL2 to \n",
    "\n",
    "(1) measure inconsistencies in its bidirectional conditionals; \n",
    "\n",
    "(2) improve llm inference with Emsemble of Conditionals.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "\n",
    "* [Imports and global utils](#0)\n",
    "\n",
    "* [Load tokenizer and model](#1)\n",
    "\n",
    "* [Ensemble of Conditionals](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px;\"><a class=\"anchor\" id=\"0\"></a>Imports and global utils</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "import general_utils\n",
    "# clear GPU memory\n",
    "if True:\n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from lambada_utils import LambadaOutputProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''toggle 'print' on and off by creating 'show' as a proxy'''\n",
    "enable_print = True\n",
    "if enable_print:\n",
    "    show = print\n",
    "else:\n",
    "    show = lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px;\"><a class=\"anchor\" id=\"1\"></a>Load tokenizer and model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a91352e56488998d790f922f30033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/nus/tomyoung/miniconda3/envs/mlm_inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1587: FutureWarning: `T5ForConditionalGeneration.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'encoder.block.0': 0, 'encoder.block.1': 1, ...}\n",
      "  warnings.warn(\n",
      "/scratch/users/nus/tomyoung/miniconda3/envs/mlm_inconsistencies/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:925: FutureWarning: `T5Stack.parallelize` is deprecated and will be removed in v5 of Transformers, you should load your model with `device_map='balanced'` in the call to `from_pretrained`. You can also provide your own `device_map` but it needs to be a dictionary module_name to device, so for instance {'block.0': 0, 'block.1': 1, ...}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", \n",
    "                                                   cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2', \n",
    "                                                   low_cpu_mem_usage=True, \n",
    "                                                   torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model.parallelize() # TODO: the first gpu doesn't reduce its memory usage, but 9GB/18GB is on every other GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32099"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px;\"><a class=\"anchor\" id=\"2\"></a>Ensemble of Conditionals</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBADA_TEST_DATA_PATH = \"data/jsonls/test.jsonl\"\n",
    "\n",
    "with open(LAMBADA_TEST_DATA_PATH, \"r\") as f:\n",
    "    lambada = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "# TODO: explain the term pretokenized\n",
    "UL2_MODE = \"[NLG]\"\n",
    "\n",
    "# To use the NLG mode of UL2, append [NLG] to the beginning of each input, and <extra_id_0> to the end\n",
    "lambada = [\n",
    "    {\n",
    "        \"inputs_pretokenized\": UL2_MODE + \" \" + x['inputs_pretokenized'] + \" <extra_id_0>\",\n",
    "        \"targets_pretokenized\": x['targets_pretokenized']\n",
    "    } \n",
    "    for x in lambada\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the output processor\n",
    "processor = LambadaOutputProcessor(tokenizer, lambada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Strategy for different punctuations: (click to expand)</summary>\n",
    "\n",
    "In the LAMBADA last word prediction task, natural language models (LLMs) may append various punctuations to the same last word, leading to different completions. For example, to complete the sentence \"My color of my pet dog is\":\n",
    "\n",
    "Possible Completions:\n",
    "\n",
    "1. _white._ with probability `p_1`\n",
    "2. _white!_ with probability `p_2` (assuming `p_1 > p_2`)\n",
    "3. _black,_ with probability `p_3`\n",
    "4. _black?_ with probability `p_4` (assuming `p_3 > p_4`)\n",
    "\n",
    "Strategies to Rank _white_ and _black_:\n",
    "\n",
    "1. Maximum Probability Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1`\n",
    "- Probability of _black_: `p(black) = p_3`\n",
    "\n",
    "2. Sum of Probabilities Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1 + p_2`\n",
    "- Probability of _black_: `p(black) = p_3 + p_4`\n",
    "\n",
    "Afterwards `p(_white_)` and `p(_black_)` may need normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''On lambada, generate the top completions (through beam search) for each example, and get the word from each completion.'''\n",
    "RUN_BEAM_SEARCH_CELL = False\n",
    "if RUN_BEAM_SEARCH_CELL:\n",
    "    # generate for all examples, and then get the words from the completions, and compare the first one with the target\n",
    "    count_correct = 0 # No. correct last word predictions if only the top completion is considered\n",
    "    count_correct_top_num_beams = 0 # ... if the top num_beams completions are considered\n",
    "    count_no_words_found = 0  # No. examples where no valid last word is found\n",
    "\n",
    "    # punctuated_word: the last word and the punctuation that follows it\n",
    "    id_to_punctuated_words = {} # maps example index to a list of word and punc pairs; every punc is kept for each word\n",
    "    id_to_punctuated_words_unique = {} # ...; every punc is kept for each word\n",
    "    id_to_completions_ids = {}\n",
    "\n",
    "    MAX_COMPLETION_LENGTH = 8 # for last word prediction, 8 is sufficient\n",
    "    NUM_BEAMS = 20 # 20 is sufficient; more doesn't help\n",
    "\n",
    "    # for example_index in tqdm(range(10)): # len(lambada)\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        input_string = lambada[example_index]['inputs_pretokenized']\n",
    "        inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs,\n",
    "                                max_length=MAX_COMPLETION_LENGTH, \n",
    "                                num_beams=NUM_BEAMS, \n",
    "                                num_return_sequences=NUM_BEAMS, \n",
    "                                output_scores=True,\n",
    "                                eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                                return_dict_in_generate=True)\n",
    "        \n",
    "        completions = [tokenizer.decode(outputs['sequences'][i]) for i in range(NUM_BEAMS)]\n",
    "        completions_ids = [\n",
    "            outputs['sequences'][i].cpu()\n",
    "            for i in range(NUM_BEAMS)\n",
    "            if processor.get_word_from_completion(completions[i]) is not None # if the completion has a valid last word\n",
    "        ]\n",
    "\n",
    "        words = processor.get_words_from_completions(completions)\n",
    "\n",
    "        # TODO: combine them and move to utils.py\n",
    "        completions_without_pad = processor.remove_pad_id(completions_ids)\n",
    "        completions_without_pad_before_punctution = processor.before_first_punc(completions_without_pad)\n",
    "        \n",
    "        \n",
    "        if words:\n",
    "            if words[0] == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct += 1\n",
    "        else:\n",
    "            count_no_words_found += 1\n",
    "            show(\"no words found\")\n",
    "        punctuated_words = processor.get_punctuated_words(completions)\n",
    "        id_to_punctuated_words[example_index] = punctuated_words\n",
    "        words_unique = list(set(words))\n",
    "        id_to_punctuated_words_unique[example_index] = []\n",
    "        \n",
    "        id_to_completions_ids[example_index] = completions_without_pad_before_punctution\n",
    "\n",
    "        # find the best punctuatuation for each unique word (Maximum Probability Strategy, \n",
    "        # completions are naturally ordered by probs by generate()) TODO: move this for loop to utils.py\n",
    "        for word in words_unique:\n",
    "            found = 0\n",
    "            # iterate through the word and punc pairs, and find the one that matches the word\n",
    "            for punctuated_word in punctuated_words:\n",
    "                # it is a match if pair = word + punc\n",
    "                ENDING_PUNCTUATIONS = ',!.:;?'\n",
    "                for punc in ENDING_PUNCTUATIONS:\n",
    "                    if punctuated_word == word + punc:\n",
    "                        id_to_punctuated_words_unique[example_index].append(punctuated_word)\n",
    "                        found = 1\n",
    "                        break\n",
    "                if found == 1:\n",
    "                    break\n",
    "        \n",
    "        # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "        for word in words_unique:\n",
    "            if word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_top_num_beams += 1\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the beam search results by generate()'''\n",
    "RUN_SAVE_BEAM_SEARCH_RESULTS_CELL = False\n",
    "if RUN_SAVE_BEAM_SEARCH_RESULTS_CELL:\n",
    "    timed_pickle_filename = 'data/pkls/ul2_lambada_vanilla_beam_search_results_' + general_utils.get_time() + '.pickle'\n",
    "    print(timed_pickle_filename)\n",
    "\n",
    "    data_keys = ['count_correct', 'count_correct_top_num_beams', 'count_no_words_found',\n",
    "                'id_to_punctuated_words', 'id_to_punctuated_words_unique', 'id_to_completions_ids']\n",
    "    data = {}\n",
    "    for key in data_keys:\n",
    "        data[key] = locals()[key]\n",
    "\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the beam search results'''\n",
    "timed_pickle_filename = 'data/pkls/ul2_lambada_vanilla_beam_search_results_2023-11-11 20:08:17.pickle'\n",
    "with open(timed_pickle_filename, 'rb') as fp:\n",
    "    ul2_lambada_vanilla_beam_search_results = pickle.load(fp)\n",
    "id_to_completions_ids = ul2_lambada_vanilla_beam_search_results['id_to_completions_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>K-offset Ensemble (click to expand)</summary>\n",
    "\n",
    "__K-offset Ensemble__ is a particular type of __Ensemble of Conditionals__ for last word prediction tasks like lambada.\n",
    "\n",
    "It aims to augment the only conditional distribution obtained by masking the last word with more distributions. The new distributions are obtained by masking the last __offset__ + 1 words.\n",
    "\n",
    "An example with the _lambada[0]_\n",
    "\n",
    "_lambada[0]['input_pretokenized']_: `... his mouth curved in a confident grin , i do n't care about <last_word>`\n",
    "\n",
    "We consider candidates `['angels.', 'signs.', 'that.']`.\n",
    "\n",
    "The baseline approach is to input `... his mouth curved in a confident grin , i do n't care about <extra_id_0>` to UL2 and obtain the distribution containing the 3 candidates.\n",
    "\n",
    "For the offset=1 case in K-offset Ensemble, we mask an extra token `about` in the end and input instead\n",
    "\n",
    "`... his mouth curved in a confident grin , i do n't care <extra_id_1>`\n",
    "\n",
    "This gives us a different distribution regarding `['about angels.', 'about signs.', 'about that.']`. They are given in an autoregressive manner\n",
    "e.g., `p(about angels) = p(about) * p(angels|about)`. Therefore we will use conditionals in the style of `p(angels|about)` to augment the baseline conditionals.\n",
    "\n",
    "Cases where __K__ is larger are simple to extend into.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OFFSET = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate the offset samples'''\n",
    "RUN_GENERATE_OFFSET_SAMPLES_CELL = False\n",
    "if RUN_GENERATE_OFFSET_SAMPLES_CELL:\n",
    "    id_and_offset_to_inputs_and_completions = \\\n",
    "        processor.get_offset_samples(\n",
    "            ul2_lambada_vanilla_beam_search_results['id_to_completions_ids'], \n",
    "            max_offset=MAX_OFFSET\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the offset samples'''\n",
    "RUN_SAVE_OFFSET_SAMPLES_CELL = False\n",
    "if RUN_SAVE_OFFSET_SAMPLES_CELL:    \n",
    "    timed_pickle_filename = 'data/pkls/offset_samples_' + 'max_offset_' + str(MAX_OFFSET) + '_' + general_utils.get_time() + '.pickle'\n",
    "    print(timed_pickle_filename)\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(id_and_offset_to_inputs_and_completions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the offset samples'''\n",
    "timed_pickle_filename = 'data/pkls/offset_samples_max_offset_30_2023-11-13-01:28:11.pickle'\n",
    "with open(timed_pickle_filename, 'rb') as fp:\n",
    "    id_and_offset_to_inputs_and_completions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/5153 [00:56<9:52:06,  6.91s/it] "
     ]
    }
   ],
   "source": [
    "''' obtain the avg_log_p_map '''\n",
    "RUN_GET_AVG_LOG_PS_CELL = True\n",
    "if RUN_GET_AVG_LOG_PS_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> [(input_ids_0, completion_ids_0), (input_ids_1, completion_ids_1), ...]\n",
    "    avg_log_p_map = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "    ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'\n",
    "\n",
    "    for example_index in tqdm(range(len(lambada))): \n",
    "    # for example_index in tqdm(range(1)): \n",
    "        if len(id_to_completions_ids[example_index]) == 0:\n",
    "            continue\n",
    "        for offset in range(MAX_OFFSET):\n",
    "            # we batch the completions for each (example, offset)\n",
    "            completions_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "                [\n",
    "                    id_and_offset_to_inputs_and_completions[(example_index, offset)][i][1] # [1] is the completion_ids\n",
    "                    for i in range(len(id_to_completions_ids[example_index]))\n",
    "                ], \n",
    "                batch_first=True, \n",
    "                padding_value=tokenizer.pad_token_id\n",
    "            ).cuda()\n",
    "\n",
    "            input_ids_batch = torch.cat(\n",
    "                [\n",
    "                    id_and_offset_to_inputs_and_completions[(example_index, offset)][i][0].unsqueeze(0) \n",
    "                    for i in range(len(id_to_completions_ids[example_index]))\n",
    "                ], \n",
    "                dim=0\n",
    "            ).cuda()\n",
    "\n",
    "            outputs = model(input_ids_batch, labels=completions_batch)\n",
    "\n",
    "            for completion_index in range(len(id_to_completions_ids[example_index])):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1+offset:], \n",
    "                    completions_batch[completion_index][1+offset:]\n",
    "                )\n",
    "                avg_log_p_map[(example_index, offset, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the avg_log_p_map'''\n",
    "RUN_SAVE_AVG_LOG_P_MAP_CELL = False\n",
    "if RUN_SAVE_AVG_LOG_P_MAP_CELL:\n",
    "    pickle_filename = 'data/pkls/avg_log_p_map_' + 'max_offset_' + str(MAX_OFFSET) + '_' + general_utils.get_time() + '.pickle'\n",
    "    print(pickle_filename)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(avg_log_p_map, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the avg_log_p_map'''\n",
    "pickle_filename = 'data/pkls/avg_log_p_map_2023-11-12 21:51:05.pickle'\n",
    "# avg_log_p_map (Dict): (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "with open(pickle_filename, 'rb') as handle:\n",
    "    avg_log_p_map = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5153/5153 [00:00<00:00, 48211.36it/s]\n",
      "100%|██████████| 5153/5153 [00:00<00:00, 45864.34it/s]\n",
      "100%|██████████| 5153/5153 [00:00<00:00, 39298.96it/s]\n",
      "100%|██████████| 5153/5153 [00:00<00:00, 34342.56it/s]\n",
      "100%|██████████| 5153/5153 [00:00<00:00, 30638.27it/s]\n"
     ]
    }
   ],
   "source": [
    "'''K-offset Ensemble with max reduction to emsemble the K different conditionals for the same last word, \n",
    "i.e., only the maximum avg_log_p is kept for each last word across different offsets. \n",
    "'''\n",
    "\n",
    "# We test K-offset ensemble for K up to MAX_OFFSET_TEST; MAX_OFFSET_TEST should be <= MAX_OFFSET used during avg_log_p_map generation\n",
    "MAX_OFFSET_TEST = 5 \n",
    "offset_to_accuracy = dict()\n",
    "for offset_test in range(MAX_OFFSET_TEST):\n",
    "    count_correct = 0 # No. correct last word predictions with K-offset\n",
    "    # Get the best completion based on avg_log_p_map\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        \n",
    "        # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "        avg_log_p_and_completion = [\n",
    "            (avg_log_p_map[(example_index, offset, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "            for offset in range(offset_test + 1)\n",
    "            for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "        ]\n",
    "\n",
    "        # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "        if len(avg_log_p_and_completion) == 0:\n",
    "            continue\n",
    "        best_avg_log_p, best_completion = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "        if processor.is_correct_completion(example_index, best_completion):\n",
    "            count_correct += 1\n",
    "    offset_to_accuracy[offset_test] = count_correct / (len(lambada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7719774888414516,\n",
       " 1: 0.7745002910925675,\n",
       " 2: 0.7758587230739376,\n",
       " 3: 0.7745002910925675,\n",
       " 4: 0.7756646613623133}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_to_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EOC with avg reduction'''\n",
    "# avg reduction is not to be confused with avg_log_p, which is the average of log_p of the tokens constituting the last (punctuated) word\n",
    "max_offset_test = 60\n",
    "offset_to_accuracy_avg_reduction = dict()\n",
    "for offset_test in range(max_offset_test):\n",
    "    count_eoc = 0\n",
    "    # postprocess the avg_log_p_map to get the best completion\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        if len(id_to_completions[example_index]) == 0 or example_index in failed_example_indices:\n",
    "            continue\n",
    "        completion_avg_log_p_avg_over_offset_max = -10000000\n",
    "        best_completion =  \"\"\n",
    "        for completion_index in range(len(id_to_completions[example_index])):\n",
    "            completion_avg_log_p_avg_over_offset = 0\n",
    "            for offset in range(offset_test+1):\n",
    "                avg_log_p = avg_log_p_map[(example_index, offset, completion_index)]\n",
    "                completion_avg_log_p_avg_over_offset += avg_log_p\n",
    "            completion_avg_log_p_avg_over_offset /= (offset_test+1)\n",
    "            if completion_avg_log_p_avg_over_offset > completion_avg_log_p_avg_over_offset_max:\n",
    "                completion_avg_log_p_avg_over_offset_max = completion_avg_log_p_avg_over_offset\n",
    "                best_completion = id_to_completions[example_index][completion_index]\n",
    "        best_completion_string = tokenizer.decode(best_completion)\n",
    "        # print('best_completion_string', best_completion_string)\n",
    "        if processor.get_words_from_completions([best_completion_string]) != []:\n",
    "            best_word = processor.get_words_from_completions([best_completion_string])[0]\n",
    "            if best_word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_eoc += 1\n",
    "    offset_to_accuracy_avg_reduction[offset_test] = count_eoc / (len(lambada) - len(failed_example_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load a font\n",
    "font_path = '/usr/share/fonts/urw-base35/NimbusMonoPS-Italic.otf'\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# offset = 0 corresponds to the baseline, which is no. ensembled conditionals = 1; adjust the offset by 1\n",
    "no_ensembled_conditionals_to_accuracy = dict()\n",
    "for offset in range(1, max_offset_test+1):\n",
    "    no_ensembled_conditionals_to_accuracy[offset] = offset_to_accuracy[offset-1]\n",
    "\n",
    "\n",
    "max_line = plt.plot(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()), label='max')\n",
    "plt.xlabel('No. ensembled conditionals', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "# the interval on x should be 10\n",
    "plt.xticks(np.arange(10, max(list(no_ensembled_conditionals_to_accuracy.keys()))+1, 10))\\\n",
    "# add a tick at 1 on the x axis\n",
    "plt.xticks(list(plt.xticks()[0]) + [1])\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# add a dot at each point\n",
    "plt.scatter(list(no_ensembled_conditionals_to_accuracy.keys()), list(no_ensembled_conditionals_to_accuracy.values()))\n",
    "\n",
    "\n",
    "# add a yellow horizontal line at y=offset_to_accuracy[0]\n",
    "plt.axhline(y=no_ensembled_conditionals_to_accuracy[1], color='y', linestyle='--')\n",
    "# add the word \"baseline\" at the end of the yellow line in the font of calibri\n",
    "plt.text(48, no_ensembled_conditionals_to_accuracy[1] + 0.0002, 'baseline', fontproperties=font_prop, fontsize=13)\n",
    "\n",
    "# plot the accuracy with avg reduction\n",
    "avg_line = plt.plot([item+1 for item in list(offset_to_accuracy_avg_reduction.keys())], list(offset_to_accuracy_avg_reduction.values()), color='r', label='avg')\n",
    "# add a dot at each point\n",
    "plt.scatter([item+1 for item in list(offset_to_accuracy_avg_reduction.keys())], list(offset_to_accuracy_avg_reduction.values()), color='r')\n",
    "\n",
    "plt.scatter(1, no_ensembled_conditionals_to_accuracy[1], color='y')\n",
    "\n",
    "\n",
    "plt.legend(handles=[max_line[0], avg_line[0]], loc='upper center', bbox_to_anchor=(0.9, 0.45), ncol=1, fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot at a high resolution\n",
    "plt.savefig('no_ensembled_conditionals_to_accuracy_combined.png', dpi=1200)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_to_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "\n",
    "print('Number of fonts: ', len(fonts))\n",
    "for font in fonts:\n",
    "    print(font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(offset_to_accuracy.keys()))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(offset_to_accuracy_avg_reduction.keys()), list(offset_to_accuracy_avg_reduction.values()))\n",
    "plt.xlabel('offset')\n",
    "plt.ylabel('accuracy')\n",
    "# the interval on x should be 1\n",
    "plt.xticks(np.arange(min(list(offset_to_accuracy_avg_reduction.keys())), max(list(offset_to_accuracy_avg_reduction.keys()))+1, 1.0))\n",
    "# add a dot at each point\n",
    "plt.scatter(list(offset_to_accuracy_avg_reduction.keys()), list(offset_to_accuracy_avg_reduction.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the current GPU memory allocation\n",
    "allocated_memory_bytes = torch.cuda.memory_allocated(device=0)\n",
    "# Convert the allocated memory to gigabytes\n",
    "allocated_memory_gb = allocated_memory_bytes / (1024 ** 3)\n",
    "print(f\"Current GPU memory allocation: {allocated_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Concatenate all completions to get a huge tensor'''\n",
    "all_completions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_batch = torch.nn.utils.rnn.pad_sequence(id_to_completions[0] + id_to_completions[1] , batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "completions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = lambada[0]['inputs_pretokenized']\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "id_to_completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_completions = 0\n",
    "no_completions_list = []\n",
    "for id in id_to_completions:\n",
    "    total_no_completions += len(id_to_completions[id])\n",
    "    no_completions_list.append(len(id_to_completions[id]))\n",
    "# avg\n",
    "total_no_completions/len(id_to_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test to make sure that padding and concatenating individual examaples doesn't mess up results'''\n",
    "\n",
    "# completions is below\n",
    "# [tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,    24,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099,  3957,     3,     5], device='cuda:0'),\n",
    "#  tensor([32099, 11831,     7,     3,     5], device='cuda:0')]\n",
    "\n",
    "#convert to completions_batch by padding to the max length\n",
    "completions_batch = torch.nn.utils.rnn.pad_sequence(id_to_completions[0], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "# completions_batch\n",
    "# create a number of input_ids same to the number of elements in ids_to_completions[0]\n",
    "input_ids_batch = torch.cat([input_ids for i in range(len(id_to_completions[0]))], dim=0)\n",
    "\n",
    "outputs = model(input_ids_batch, labels=completions_batch)\n",
    "# outputs = model(input_ids_batch[1].unsqueeze(0), labels=completions_batch[1].unsqueeze(0))\n",
    "# outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_batch_1 = id_to_completions[0][1].unsqueeze(0)\n",
    "input_ids_batch_1 = input_ids\n",
    "outputs_1 = model(input_ids_batch_1, labels=completions_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits[1][:-1] == outputs_1.logits[0]\n",
    "print(outputs.logits[1][:-1])\n",
    "print(outputs_1.logits[0])\n",
    "print(completions_batch_1)\n",
    "\n",
    "loss0 = ce_loss(outputs.logits[1][:-1], completions_batch_1[0])\n",
    "loss0_padded = ce_loss(outputs.logits[1], completions_batch[1])\n",
    "loss0_1 = ce_loss(outputs_1.logits[0], completions_batch_1[0])\n",
    "print(loss0)\n",
    "print(loss0_padded)\n",
    "print(loss0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits[1].shape)\n",
    "print(outputs_1.logits[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce_loss(outputs.logits, completions_batch)\n",
    "logits_0 = outputs.logits[1]\n",
    "completions_batch_0 = completions_batch[1]\n",
    "# logits_0.shape\n",
    "\n",
    "loss = ce_loss(logits_0, completions_batch_0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['scores']\n",
    "for i in range(len(outputs['scores'])):\n",
    "    probs_outputs_beam_scores = torch.nn.functional.softmax(outputs['scores'][i], dim=-1)\n",
    "    print(probs_outputs_beam_scores[0])\n",
    "    # argmax \n",
    "    print(torch.argmax(probs_outputs_beam_scores[0]))\n",
    "    # decode it\n",
    "    # print(tokenizer.decode(torch.argmax(probs_outputs_beam_scores[0])))\n",
    "    # the value\n",
    "    print(probs_outputs_beam_scores[0][torch.argmax(probs_outputs_beam_scores[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = lambada[0]['inputs_pretokenized']\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "num_beams = 2\n",
    "outputs = model.generate(inputs, \n",
    "                        # max_length=8, \n",
    "                        num_beams=num_beams, \n",
    "                        num_return_sequences=num_beams,\n",
    "                        # eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'),\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(lambada[0]['inputs_pretokenized'], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(lambada[0]['inputs_pretokenized'])\n",
    "labels = tokenizer(\"<extra_id_0> \" + id_to_punctuated_words_processed[0][2] + \" <extra_id_1>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(\"<extra_id_0> \" + id_to_punctuated_words_processed[0][2] + \" <extra_id_1>\")\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss with cross entropy loss function\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] A man is having a bun for <extra_id_0>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "num_beams = 1\n",
    "# outputs = model.generate(inputs, num_beams=num_beams, max_length=3, num_return_sequences=num_beams, output_scores=True, return_dict_in_generate=True)\n",
    "outputs = model.generate(inputs, max_length=3, output_scores=True, return_dict_in_generate=True)\n",
    "# outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "for i in range(num_beams):\n",
    "    # print(outputs['sequences'][i])\n",
    "    print(tokenizer.decode(outputs['sequences'][i]))\n",
    "    # decode outputs['sequences'][i] one by one token\n",
    "    print('-------------')\n",
    "    # print(outputs['sequences_scores'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl'\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50_valid.pkl', 'rb') as f:\n",
    "    acceptable_alternatives_1000_ignore_cws_nos_50_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_alternatives_1000_ignore_cws_nos_50_valid[(180, 11, 8)][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(acceptable_alternatives_1000_ignore_cws_nos_50_valid.keys())[320:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(acceptable_alternatives_1000_ignore_cws_nos_50_valid.keys())\n",
    "for key in all_keys[1000:2000]:\n",
    "    if len(acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3]) > 1:\n",
    "        print(key)\n",
    "        for item in acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3]:\n",
    "            print(item)\n",
    "        # print(*acceptable_alternatives_1000_ignore_cws_nos_50_valid[key][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_realnewslike[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_json_file = '/work/09127/tomyoung/ls6/data/jsons/c4-validation.00000-of-00001-list-of-lists.json'\n",
    "import json\n",
    "with open(c4_json_file, 'r', encoding='utf8') as f:\n",
    "    dicts_realnewslike = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, BartTokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-3b')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''based on url_to_probs_c4_dict_with_labels_t5_11b_valid and acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "generate the input string and target string for the models like ul2 and glm'''\n",
    "proposed_bigram = t5_tokenizer.decode(url_to_probs_c4_dict_with_labels_t5_11b_valid[((0,17,4),(0,17,5),1)]['proposed bigram'])\n",
    "\n",
    "acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1]\n",
    "\n",
    "proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+1:4+3])\n",
    "print('proposed_bigram:', proposed_bigram)\n",
    "\n",
    "preceding_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][:4])\n",
    "print('preceding_tokens:', preceding_tokens)\n",
    "\n",
    "following_tokens = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[(0,17,4)][0][1][4+3:])\n",
    "print('following_tokens:', following_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_probs_c4_dict_with_labels_t5_11b_valid_keys = list(url_to_probs_c4_dict_with_labels_t5_11b_valid.keys())\n",
    "\n",
    "url_to_ul2_probs_dict = {}\n",
    "import math\n",
    "for i in tqdm(range(len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys))):#len(url_to_probs_c4_dict_with_labels_t5_11b_valid_keys)\n",
    "    key = url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]\n",
    "    url_to_ul2_probs_dict[url_to_probs_c4_dict_with_labels_t5_11b_valid_keys[i]] = {}\n",
    "    show(key)\n",
    "    show(key[0])\n",
    "    show(acceptable_alternatives_1000_ignore_cws_nos_50_valid[key[0]])\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid\n",
    "    # process the proposed bigram and token\n",
    "    url = key[0]\n",
    "    story_id = key[0][0]\n",
    "    paragraph_id = key[0][1]\n",
    "    completion_id = key[2]\n",
    "    proposed_token_pos = key[0][2]\n",
    "\n",
    "    ''' proposed token '''\n",
    "    # acceptable_alternatives_1000_ignore_cws_nos_50_valid has an <s> </s>\n",
    "    proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+1:proposed_token_pos+2])\n",
    "    preceding_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos+1])\n",
    "    following_tokens_to_proposed_token = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+2:])\n",
    "    show('proposed_token:', proposed_token)\n",
    "    show('preceding_tokens_to_proposed_token:', preceding_tokens_to_proposed_token)\n",
    "    show('following_tokens_to_proposed_token:', following_tokens_to_proposed_token)\n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_token + ' <extra_id_0>' + following_tokens_to_proposed_token\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_token + ' <extra_id_1>'\n",
    "    show('target_string_proposed_token:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    show('labels_proposed_token:', labels)\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p = -ce_loss_sum(outputs.logits[0][1:], labels[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_proposed_token:', log_p)\n",
    "    example_raw_sequence = dicts_realnewslike[story_id][paragraph_id]\n",
    "    show('example_raw_sequence:', example_raw_sequence)\n",
    "    # tokenize the raw sequence\n",
    "    example_raw_sequence_bart_tokenized = bart_tokenizer.tokenize(example_raw_sequence)\n",
    "    bart_ids_original = bart_tokenizer.convert_tokens_to_ids(example_raw_sequence_bart_tokenized)\n",
    "    \n",
    "    ''' original token '''\n",
    "    original_token = bart_tokenizer.decode(bart_ids_original[proposed_token_pos])\n",
    "    show('original_token:', original_token)\n",
    "    target_string_original_token = '<extra_id_0>' + original_token + ' <extra_id_1>'\n",
    "    show('target_string_original_token:', target_string_original_token)\n",
    "    labels_original_token = tokenizer(target_string_original_token, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_token = labels_original_token[:, :-1].contiguous()\n",
    "    show('labels_original_token:', labels_original_token)\n",
    "    outputs = model(inputs, labels=labels_original_token)\n",
    "    log_p_original_token = -ce_loss_sum(outputs.logits[0][1:], labels_original_token[0][1:]) # [1:] to remove the first token <extra_id_0>\n",
    "    show('log_p_original_token:', log_p_original_token)\n",
    "    \n",
    "    \n",
    "    ''' proposed bigram '''\n",
    "    constant_token_pos = key[1][2]\n",
    "    proposed_token_is_to_the_left = proposed_token_pos < constant_token_pos\n",
    "    if proposed_token_is_to_the_left:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+1:proposed_token_pos+3])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos+1])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+3:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos:proposed_token_pos+2])\n",
    "    else:\n",
    "        proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos:proposed_token_pos+2])\n",
    "        preceding_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][:proposed_token_pos])\n",
    "        following_tokens_to_proposed_bigram = bart_tokenizer.decode(acceptable_alternatives_1000_ignore_cws_nos_50_valid[url][0][completion_id][proposed_token_pos+2:])\n",
    "        original_bigram = bart_tokenizer.decode(bart_ids_original[proposed_token_pos-1:proposed_token_pos+1])\n",
    "    show('proposed_bigram:', proposed_bigram)\n",
    "    show('preceding_tokens_to_proposed_bigram:', preceding_tokens_to_proposed_bigram)\n",
    "    show('following_tokens_to_proposed_bigram:', following_tokens_to_proposed_bigram)\n",
    "    show('original_bigram:', original_bigram)\n",
    "    \n",
    "    \n",
    "    # generate the input string by adding a <extra_id_0> in the middle of the sentence\n",
    "    input_string = preceding_tokens_to_proposed_bigram + ' <extra_id_0>' + following_tokens_to_proposed_bigram\n",
    "    # remove <s> and </s> in the input string\n",
    "    input_string = input_string.replace('<s>', '')\n",
    "    input_string = input_string.replace('</s>', '')\n",
    "    # add [NLU] to the input string\n",
    "    input_string = '[NLU] ' + input_string\n",
    "    show('input_string:', input_string)\n",
    "    target_string = '<extra_id_0>' + proposed_bigram + ' <extra_id_1>'\n",
    "    show('target_string:', target_string)\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # remove the last </s> token from labels\n",
    "    labels = labels[:, :-1].contiguous()\n",
    "    show('inputs:', inputs)\n",
    "    show('labels:', labels)\n",
    "    outputs = model(inputs, labels=labels)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_proposed_bigram = -ce_loss_sum(outputs.logits[0][1:], labels[0][1:]) #\n",
    "    show('log_p_proposed_bigram:', log_p_proposed_bigram)\n",
    "\n",
    "\n",
    "    '''original bigram'''\n",
    "    target_string_original_bigram = '<extra_id_0>' + original_bigram + ' <extra_id_1>'\n",
    "    show('target_string_original_bigram:', target_string_original_bigram)\n",
    "    labels_original_bigram = tokenizer(target_string_original_bigram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_bigram = labels_original_bigram[:, :-1].contiguous()\n",
    "    outputs = model(inputs, labels=labels_original_bigram)\n",
    "    # print('outputs:', outputs)\n",
    "    log_p_original_bigram = -ce_loss_sum(outputs.logits[0][1:], labels_original_bigram[0][1:]) #\n",
    "    show('log_p_original_bigram:', log_p_original_bigram)\n",
    "    # put the log_p's into the dictionary\n",
    "    url_to_ul2_probs_dict[key] = {'proposed token': math.exp(log_p.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original token': math.exp(log_p_original_token.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'proposed bigram': math.exp(log_p_proposed_bigram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                'original bigram': math.exp(log_p_original_bigram.to(torch.float32).detach().cpu().numpy())}\n",
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_ul2_probs_dict_filepath = '/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_probs_dict_valid.pkl'\n",
    "with open(url_to_ul2_probs_dict_filepath, 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_probs_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl\n",
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_probs_c4_dict_with_labels_t5_11b_valid.pkl', 'rb') as f:\n",
    "    url_to_probs_c4_dict_with_labels_t5_11b_valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] The headquarters of microsoft is in <extra_id_0>\"                                               \n",
    "\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, max_length=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"[NLG] The headquarters of microsoft is in <extra_id_0>\"\n",
    "target_string = \"<extra_id_0> redmond washington <extra_id_1>\"\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = labels[:, :-1].contiguous()\n",
    "outputs = model(inputs, labels=labels)\n",
    "print('outputs:', outputs)\n",
    "log_p = -ce_loss_sum(outputs.logits[0][1:-1], labels[0][1:-1]) #\n",
    "print('log_p:', log_p)\n",
    "\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "for i in range(len(labels[0])):\n",
    "    print('label:', labels[0][i], 'prob:', probs[0][i][labels[0][i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer(target_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/dict_url_to_completions_5_grams.pkl','rb') as f:\n",
    "    dict_url_to_completions_5_grams = pickle.load(f)\n",
    "# get a list of punctuations\n",
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_url_to_completions_5_grams_keys = list(dict_url_to_completions_5_grams.keys())\n",
    "url_to_ul2_5_gram_probs_dict = {}\n",
    "# make sure it ends with a punctuation\n",
    "for i in tqdm(range(1)):\n",
    "    key = dict_url_to_completions_5_grams_keys[i]\n",
    "    # print('key:', key)\n",
    "    # print(dict_url_to_completions_5_grams[key])\n",
    "    # alternative: make sure it ends with a punctuation\n",
    "    if dict_url_to_completions_5_grams[key]['alternative'][-1] not in punctuations:\n",
    "        continue\n",
    "    # for 10-grams\n",
    "    # get the input string \n",
    "    original_sentence_words = dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')\n",
    "    # len should >= 15\n",
    "    if len(original_sentence_words) < 15:\n",
    "        continue\n",
    "    input_for_10_grams = '[NLG] ' + ' '.join(original_sentence_words[:-10]) + ' <extra_id_0>'\n",
    "    input_for_10_grams_ids = tokenizer(input_for_10_grams, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # original 10-gram\n",
    "    original_10_gram = \"<extra_id_0> \" + ' '.join(original_sentence_words[-10:]) + ' <extra_id_1>'\n",
    "    labels_original_10_gram = tokenizer(original_10_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_10_gram = labels_original_10_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_10_grams_ids, labels=labels_original_10_gram)\n",
    "    log_p_original_10_gram = -ce_loss_sum(outputs.logits[0][1:-1], labels_original_10_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    \n",
    "    # probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # for i in range(len(labels_original_10_gram[0])):\n",
    "    #     print('label:', labels_original_10_gram[0][i], 'prob:', probs[0][i][labels_original_10_gram[0][i]])\n",
    "\n",
    "    # continue\n",
    "    # proposed 10-gram\n",
    "    proposed_10_gram = \"<extra_id_0> \" + \\\n",
    "     ' '.join(original_sentence_words[-10:-5]) + ' ' + \\\n",
    "    dict_url_to_completions_5_grams[key]['alternative'] + ' <extra_id_1>'\n",
    "    labels_proposed_10_gram = tokenizer(proposed_10_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_proposed_10_gram = labels_proposed_10_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_10_grams_ids, labels=labels_proposed_10_gram)\n",
    "    log_p_proposed_10_gram = -ce_loss_sum(outputs.logits[0][1:-1], labels_proposed_10_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # for 5-grams\n",
    "    # get the input string\n",
    "    input_for_5_grams = '[NLG] ' + ' '.join(original_sentence_words[:-5]) + ' <extra_id_0>'\n",
    "    input_for_5_grams_ids = tokenizer(input_for_5_grams, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # original 5-gram\n",
    "    original_5_gram = \"<extra_id_0> \" + ' '.join(original_sentence_words[-5:]) + ' <extra_id_1>'\n",
    "    labels_original_5_gram = tokenizer(original_5_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_original_5_gram = labels_original_5_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_5_grams_ids, labels=labels_original_5_gram)\n",
    "    log_p_original_5_gram = -ce_loss_sum(outputs.logits[0][1:-1], labels_original_5_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # proposed 5-gram\n",
    "    proposed_5_gram = \"<extra_id_0> \" + dict_url_to_completions_5_grams[key]['alternative'] + ' <extra_id_1>'\n",
    "    labels_proposed_5_gram = tokenizer(proposed_5_gram, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels_proposed_5_gram = labels_proposed_5_gram[:, :-1].contiguous() # remove the last token '</s>'\n",
    "    outputs = model(input_for_5_grams_ids, labels=labels_proposed_5_gram)\n",
    "    log_p_proposed_5_gram = -ce_loss_sum(outputs.logits[0][1:-1], labels_proposed_5_gram[0][1:-1]) # lose the <extra_id_0> and <extra_id_1>\n",
    "    # add them to the dictionary\n",
    "    url_to_ul2_5_gram_probs_dict[key] = {'proposed 5_gram': math.exp(log_p_proposed_5_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'original 5_gram': math.exp(log_p_original_5_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'proposed 10_gram': math.exp(log_p_proposed_10_gram.to(torch.float32).detach().cpu().numpy()),\n",
    "                                         'original 10_gram': math.exp(log_p_original_10_gram.to(torch.float32).detach().cpu().numpy())}                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save url_to_ul2_5_gram_probs_dict as a pkl\n",
    "with open('/work/09127/tomyoung/ls6/data/pkls/url_to_ul2_5_gram_probs_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(url_to_ul2_5_gram_probs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of different tokens between the two 5-grams\n",
    "for key in dict_url_to_completions_5_grams.keys():\n",
    "    proposed_5_gram = dict_url_to_completions_5_grams[key]['alternative'].split(' ')\n",
    "    original_5_gram = dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')[-5:]\n",
    "    number_of_different_tokens = 0\n",
    "    for i in range(5):\n",
    "        if proposed_5_gram[i] != original_5_gram[i]:\n",
    "            number_of_different_tokens += 1\n",
    "    print('key:', key)\n",
    "    print('original_sentence:', dict_url_to_completions_5_grams[key]['original_sentence'])\n",
    "    print('proposed_5_gram:', dict_url_to_completions_5_grams[key]['alternative'])\n",
    "    print('original_5_gram:', ' '.join(dict_url_to_completions_5_grams[key]['original_sentence'].split(' ')[-5:]))\n",
    "    print(number_of_different_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
