{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/**\n",
    "#* @file ul2_winogrande.ipynb\n",
    "#* @author chenyunan (chen.yunan_01@nus.edu.sg)\n",
    "#* @brief\n",
    "#* @version 0.1\n",
    "#* @date 2024-01-01\n",
    "#*\n",
    "#* @copyright Copyright (c) 2023 \n",
    "#*\n",
    "#*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killed process 108295 on GPU 0\n"
     ]
    }
   ],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import general_utils\n",
    "# clear GPU memory\n",
    "if True:   \n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import lambada_utils\n",
    "from lambada_utils import LambadaProcessor\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6720f24a825430fa975bcaee9a04862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n",
    "RUN_CELL = 1 # Load model 1\n",
    "# device_map=general_utils.get_ul2_device_map('2,3')\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\",\n",
    "                                                        cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2',\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/oem/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Mon Jan  1 21:34:08 2024) since it couldn't be found locally at winogrande., or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SUBJECTS = ['winogrande_debiased','winogrande_l','winogrande_m','winogrande_s',\\\n",
    "            'winogrande_xl','winogrande_xs']\n",
    "\n",
    "DATASET_PATH = os.path.join(\"winogrande\")\n",
    "WINOGRANDE_DATAS = [load_dataset(DATASET_PATH, sub) for sub in SUBJECTS]\n",
    "INDEX = [i for i in range(len(SUBJECTS))]\n",
    "NAMES_WITH_DATAS = zip(INDEX, SUBJECTS, WINOGRANDE_DATAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf81205e50b4c8487411f317a6a2bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n",
    "RUN_CELL = 1 # Load model 1\n",
    "# device_map=general_utils.get_ul2_device_map('2,3')\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\",\n",
    "                                                        cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2',\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_id_0 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")])\n",
    "extra_id_1 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_1>\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Question Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UL2_MODE = \"[NLG]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def data_prompting(docs, tokenizer) -> Tuple:\n",
    "    '''\n",
    "        docs: DATA_SET[SUBJECTS_NAME], ex:HELLASWG_DATAS[validation]\n",
    "        return: Tuple(input_ids, completions_ids_padded, labels)\n",
    "\n",
    "        input[example]: ctx:<prompt> \n",
    "        label[example]: endings list -> list[] \n",
    "\n",
    "        Todo: few-shot data prompting\n",
    "    '''\n",
    "\n",
    "    for doc in docs:\n",
    "        options = list((doc['option1'], doc['option2']))\n",
    "        index = doc['answer']\n",
    "\n",
    "        input_ = UL2_MODE + \" \" + doc['sentence'] + \" \" + \"<extra_id_0>\"\n",
    "        completions = [f\"<extra_id_0> {option} <extra_id_1>\" for option in options]\n",
    "        label = f\"{options[int(index)-1]}\"\n",
    "        \n",
    "        input_id = tokenizer(input_, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        # label_id = tokenizer(label, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        completions_ids = [tokenizer(completion, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\\\n",
    "                                                                for completion in completions]\n",
    "\n",
    "        # Assuming `max_length` is the maximum length you want to pad sequences to\n",
    "        max_length = max(seq.size(1) for seq in completions_ids)\n",
    "\n",
    "        # Pad sequences to the common length\n",
    "        padded_sequences = [F.pad(seq, (0, max_length - seq.size(1)), value=tokenizer.pad_token_id) for seq in completions_ids]\n",
    "\n",
    "        # Use pad_sequence\n",
    "        completions_ids_padded = torch.nn.utils.rnn.pad_sequence(padded_sequences, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "        completions_ids_padded = torch.squeeze(completions_ids_padded, dim = 1)\n",
    "        yield input_id, completions_ids_padded, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-offset Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OFFSET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEVELOPMENT = True\n",
    "set_partition = 'validation' if IS_DEVELOPMENT else 'test' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDING_PUNCTUATIONS = '<' # If the model generates one, it is considered that the sentence is complete and we can parse for the last word\n",
    "\n",
    "def get_word_from_completion(completion: str):\n",
    "    '''Get the last word from the given completion, if there is a valid one. Return the word.'''\n",
    "    found = False\n",
    "    word = None\n",
    "    # if a punctuation can be found in the completion, get the string before the punctuation\n",
    "    for i in range(len(completion)):\n",
    "        if completion[i] in ENDING_PUNCTUATIONS:\n",
    "            if completion[i+1] == 'e':\n",
    "                word = completion[:i]\n",
    "                found = True\n",
    "                break\n",
    "    if not found:\n",
    "        return None\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_completion(completion:torch.Tensor, label:str):\n",
    "    if not isinstance(completion, torch.Tensor):\n",
    "        return False\n",
    "    completion_string = tokenizer.decode(completion)\n",
    "\n",
    "    # print(f'completion_string:{completion_string}')\n",
    "    if not isinstance(completion_string, str):\n",
    "        return False\n",
    "    word = get_word_from_completion(completion_string)\n",
    "    # print(f'word:{word}')\n",
    "    if not isinstance(word, str):\n",
    "        return False\n",
    "    if word == label:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [08:21<00:00, 83.55s/it]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = 1 # Obtain the avg_log_p_map_offset\n",
    "\n",
    "TOTAL_CASE = 0\n",
    "ACUURACTE_CASE = 0\n",
    "\n",
    "if RUN_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> input_ids, [completion_ids_0, completion_ids_1, completion_ids_2,...]\n",
    "    avg_log_p_map_offset = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    \n",
    "    for example_index in tqdm(range(len(INDEX))): \n",
    "        data = WINOGRANDE_DATAS[example_index]\n",
    "        \n",
    "        for offset in range(MAX_OFFSET):\n",
    "            gen = data_prompting(data[set_partition], tokenizer)\n",
    "\n",
    "            for input_ids, completions_batch, label in gen:\n",
    "                avg_log_p_and_completion = []\n",
    "                outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "                for completion_index in range(len(completions_batch)):\n",
    "                    avg_log_p = -ce_loss(\n",
    "                        # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                        # the first one is <extra_id_0> and omitted\n",
    "                        outputs.logits[completion_index][1+offset:], \n",
    "                        completions_batch[completion_index][1+offset:]\n",
    "                    )\n",
    "                    avg_log_p_map_offset[(example_index, offset, completion_index)] = \\\n",
    "                        avg_log_p.detach().cpu().tolist()\n",
    "                    \n",
    "                    avg_log_p_and_completion.append([avg_log_p.detach().cpu().tolist(), completions_batch[completion_index]])\n",
    "\n",
    "                best_avg_log_p, best_completion = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "                # print(tokenizer.decode(best_completion))\n",
    "                # print(f'label:{label}')\n",
    "\n",
    "                if is_correct_completion(best_completion[1+offset:], label):\n",
    "                    ACUURACTE_CASE += 1\n",
    "                    # print(f'count_correct +1 : {ACUURACTE_CASE}')\n",
    "                TOTAL_CASE += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505130228887135"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACUURACTE_CASE/TOTAL_CASE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
