{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses UL2 to \n",
    "\n",
    "(1) measure inconsistencies in its conditionals; \n",
    "\n",
    "(2) improve its inference with Emsemble of Conditionals.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nus-ytj/miniconda3/envs/inconsistencies/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,6,7\"\n",
    "import utils.general_utils as general_utils\n",
    "# clear GPU memory\n",
    "if False:   \n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import utils.lambada_utils as lambada_utils\n",
    "from utils.lambada_utils import LambadaProcessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "'''Load tokenizer'''\n",
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False # Load model 0\n",
    "# device_map = general_utils.get_ul2_device_map('4')\n",
    "device_map = \"balanced\"\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", \n",
    "                                                    cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2', \n",
    "                                                    low_cpu_mem_usage=True, \n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    device_map=device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Specify set partition and UL2 mode; and instantiate the lambada processor'''\n",
    "IS_DEVELOPMENT = True # Set to False to run on the test set\n",
    "set_partition = 'validation_' if IS_DEVELOPMENT else 'test_' # filename part for saving results \n",
    "LAMBADA_TEST_DATA_PATH = \"data/jsonls/validation.jsonl\" if IS_DEVELOPMENT else \"data/jsonls/test.jsonl\"\n",
    "UL2_MODE = \"[NLG]\"\n",
    "\n",
    "processor = LambadaProcessor(tokenizer, \n",
    "                             ul2_mode=UL2_MODE, \n",
    "                             lambada_dataset_path=LAMBADA_TEST_DATA_PATH, \n",
    "                             rm_punc_space=True)\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'\n",
    "lambada = processor.dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy for different punctuations\n",
    "<details>\n",
    "<summary>click to expand</summary>\n",
    "\n",
    "In the LAMBADA last word prediction task, natural language models (LLMs) may append various punctuations to the same last word, leading to different completions. For example, to complete the sentence \"My color of my pet dog is\":\n",
    "\n",
    "Possible Completions:\n",
    "\n",
    "1. _white._ with probability `p_1`\n",
    "2. _white!_ with probability `p_2` (assuming `p_1 > p_2`)\n",
    "3. _black,_ with probability `p_3`\n",
    "4. _black?_ with probability `p_4` (assuming `p_3 > p_4`)\n",
    "\n",
    "Strategies to Rank _white_ and _black_:\n",
    "\n",
    "1. Maximum Probability Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1`\n",
    "- Probability of _black_: `p(black) = p_3`\n",
    "\n",
    "2. Sum of Probabilities Strategy\n",
    "\n",
    "- Probability of _white_: `p(white) = p_1 + p_2`\n",
    "- Probability of _black_: `p(black) = p_3 + p_4`\n",
    "\n",
    "Afterwards `p(_white_)` and `p(_black_)` may need normalization.\n",
    "\n",
    "WE ARE STICKING WITH MAXIMUM PROBABILITY STRAGEGY ACCORDING TO ACCURACIES OBTAINED FROM TRIAL RUNS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = False # '''Generate the top completions (through beam search) for each example, and get the word from each completion.'''\n",
    "if RUN_CELL:\n",
    "    # generate for all examples, and then get the words from the completions, and compare the first one with the target\n",
    "    count_correct = 0 # No. correct last word predictions if only the top completion is considered\n",
    "    count_correct_top_num_beams = 0 # ... if the top num_beams completions are considered\n",
    "    count_no_words_found = 0  # No. examples where no valid last word is found\n",
    "\n",
    "    # punctuated_word: the last word and the punctuation that follows it\n",
    "    id_to_punctuated_words = {} # maps example index to a list of word and punc pairs; every punc is kept for each word\n",
    "    id_to_completions_ids = {}\n",
    "\n",
    "    MAX_COMPLETION_LENGTH = 8 # for last word prediction, 8 is sufficient\n",
    "    NUM_BEAMS = 20 # 20 is sufficient; more doesn't help\n",
    "\n",
    "    # for example_index in tqdm(range(10)): # len(lambada)\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        input_string = lambada[example_index]['inputs_pretokenized']\n",
    "        inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs,\n",
    "                                max_length=MAX_COMPLETION_LENGTH, \n",
    "                                num_beams=NUM_BEAMS, \n",
    "                                num_return_sequences=NUM_BEAMS, \n",
    "                                output_scores=True,\n",
    "                                eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                                return_dict_in_generate=True)\n",
    "        \n",
    "        completions = [tokenizer.decode(outputs['sequences'][i]) for i in range(NUM_BEAMS)]\n",
    "        completions_ids_with_valid_lastword = [\n",
    "            outputs['sequences'][i].cpu()\n",
    "            for i in range(NUM_BEAMS)\n",
    "            if processor.get_word_from_completion(completions[i]) is not None # if the completion has a valid last word\n",
    "        ]\n",
    "\n",
    "        words = processor.get_words_from_completions(completions)\n",
    "        words_unique = list(set(words))\n",
    "\n",
    "        # TODO: combine them and move to lambada_utils.py\n",
    "        completions_without_pad = processor.remove_pad_id(completions_ids_with_valid_lastword)\n",
    "        completions_without_pad_before_punctuation = processor.before_first_punc_including(completions_without_pad) # including the first punc\n",
    "        \n",
    "        if words:\n",
    "            if words[0] == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct += 1\n",
    "        else:\n",
    "            count_no_words_found += 1\n",
    "            # print(\"no words found\")\n",
    "        punctuated_words = processor.get_punctuated_words(completions)\n",
    "        id_to_punctuated_words[example_index] = punctuated_words\n",
    "        \n",
    "        completions_without_pad_before_punctuation_unique = list(set([tuple(x.numpy()) for x in completions_without_pad_before_punctuation]))\n",
    "        id_to_completions_ids[example_index] = [torch.tensor(x) for x in completions_without_pad_before_punctuation_unique]\n",
    "\n",
    "        # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "        for word in words_unique:\n",
    "            if word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_top_num_beams += 1\n",
    "                break\n",
    "    print(\"count_correct\", count_correct)\n",
    "    # count_correct, NLU: 0.7595\n",
    "    # count_correct, NLG: 0.7680 (test)\n",
    "    # count_correct, S2S: 0.3743 (could be because how the mode handles extra_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False # '''Save the beam search results by generate()'''\n",
    "if RUN_CELL:\n",
    "    note = ''\n",
    "    filename = 'data/pkls/lambada_ul2/' + set_partition + UL2_MODE + '_ul2_lambada_vanilla_beam_search_results' + note + '.pickle'\n",
    "    print('\\''+filename+'\\'')\n",
    "\n",
    "    data_keys = ['count_correct', 'count_correct_top_num_beams', 'count_no_words_found',\n",
    "                'id_to_punctuated_words', 'id_to_completions_ids']\n",
    "    data = {}\n",
    "    for key in data_keys:\n",
    "        data[key] = locals()[key]\n",
    "\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the beam search results'''\n",
    "filename = 'data/pkls/lambada_ul2/' + set_partition + UL2_MODE + '_ul2_lambada_vanilla_beam_search_results.pickle' \n",
    "with open(filename, 'rb') as fp:\n",
    "    ul2_lambada_vanilla_beam_search_results = pickle.load(fp)\n",
    "id_to_completions_ids = ul2_lambada_vanilla_beam_search_results['id_to_completions_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter by completion length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = True   # select a subset by completion length \n",
    "if RUN_CELL:\n",
    "    id_to_completions_ids_subset = {}\n",
    "    lambada_subset = []\n",
    "    new_id_to_old_id = {}\n",
    "    old_id_to_new_id = {}\n",
    "    def tensors_filtering_criterion(completions_batch):\n",
    "        if len(completions_batch) < 2:\n",
    "            return False\n",
    "        # return True\n",
    "        avg_len = sum([len(general_utils.remove_trailing_zeros_from_1d_tensor(completion)) for completion in completions_batch]) / len(completions_batch)\n",
    "        # return avg_len > 3.9\n",
    "        return all([len(general_utils.remove_trailing_zeros_from_1d_tensor(completion)) < 100 for completion in completions_batch]) \\\n",
    "            and (not all([len(general_utils.remove_trailing_zeros_from_1d_tensor(completion)) < 5 for completion in completions_batch]))\n",
    "    new_id = 0\n",
    "    for id, completions in id_to_completions_ids.items():\n",
    "        if tensors_filtering_criterion(completions):\n",
    "            id_to_completions_ids_subset[new_id] = completions\n",
    "            lambada_subset.append(lambada[id])\n",
    "            new_id_to_old_id[new_id] = id\n",
    "            old_id_to_new_id[id] = new_id\n",
    "            new_id += 1\n",
    "    processor.dataset = lambada_subset\n",
    "    lambada = lambada_subset\n",
    "    id_to_completions_ids = id_to_completions_ids_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-offset conditionals\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "__K-offset Ensemble__ is a particular type of __Ensemble of Conditionals__.\n",
    "\n",
    "It aims to augment the only conditional distribution obtained by masking the last word with more distributions. The new distributions are obtained by masking the last __offset__ words.\n",
    "\n",
    "An example with the _lambada[0]_\n",
    "\n",
    "_lambada[0]['input_pretokenized']_: `... his mouth curved in a confident grin , i do n't care about <last_word>`\n",
    "\n",
    "We consider candidates `['angels.', 'signs.', 'that.']`.\n",
    "\n",
    "The baseline approach is to input `... his mouth curved in a confident grin , i do n't care about <extra_id_0>` to UL2 and obtain the distribution containing the 3 candidates.\n",
    "\n",
    "For the offset=1 case in K-offset Ensemble, we mask an extra token `about` in the end and input instead\n",
    "\n",
    "`... his mouth curved in a confident grin , i do n't care <extra_id_1>`\n",
    "\n",
    "This gives us a different distribution regarding `['about angels.', 'about signs.', 'about that.']`. They are given in an autoregressive manner\n",
    "e.g., `p(about angels) = p(about) * p(angels|about)`. Therefore we will use conditionals in the style of `p(angels|about)` to augment the baseline conditionals.\n",
    "\n",
    "Cases where __K__ is larger can be similarly derived.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OFFSET = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = False  # '''Generate and save the offset samples'''\n",
    "if RUN_CELL:\n",
    "    id_and_offset_to_inputs_and_completions = \\\n",
    "        processor.get_offset_samples(\n",
    "            id_to_completions_ids, \n",
    "            max_offset=MAX_OFFSET,\n",
    "            to_gpu=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False # '''Save the offset samples'''\n",
    "if RUN_CELL:\n",
    "    note = ''\n",
    "    filename = 'data/pkls/lambada_ul2/offset_samples_' + set_partition + 'max_offset_' + str(MAX_OFFSET) + note + '.pickle'\n",
    "    print('\\''+filename+'\\'')\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(id_and_offset_to_inputs_and_completions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Load the offset samples\n",
    "if RUN_CELL:\n",
    "    filename = 'data/pkls/lambada_ul2/offset_samples_' + set_partition + 'max_offset_' + str(MAX_OFFSET) + '.pickle'\n",
    "    with open(filename, 'rb') as fp:\n",
    "        id_and_offset_to_inputs_and_completions = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False  # Obtain p_map_offset\n",
    "if RUN_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> input_ids, [completion_ids_0, completion_ids_1, completion_ids_2,...]\n",
    "    p_map_offset = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    \n",
    "    for example_index in tqdm(range(len(lambada))): \n",
    "    # for example_index in tqdm(range(1)): \n",
    "        if len(id_to_completions_ids[example_index]) == 0:\n",
    "            continue\n",
    "        for offset in range(MAX_OFFSET):\n",
    "            completions_batch = id_and_offset_to_inputs_and_completions[(example_index, offset)]['labels']\n",
    "            input_ids = id_and_offset_to_inputs_and_completions[(example_index, offset)]['inputs'].unsqueeze(0)\n",
    "            outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "            for completion_index in range(len(id_to_completions_ids[example_index])):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1+offset:], \n",
    "                    completions_batch[completion_index][1+offset:]\n",
    "                )\n",
    "                p_map_offset[(example_index, offset, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False  # Save the p_map_offset\n",
    "if RUN_CELL:\n",
    "    note = ''\n",
    "    pickle_filename = 'data/pkls/lambada_ul2/p_map_' + set_partition + 'max_offset_' + str(MAX_OFFSET) + note + '.pickle'\n",
    "    print('\\''+pickle_filename+'\\'')\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(p_map_offset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = True  # Load the p_map for the offset samples\n",
    "if RUN_CELL:\n",
    "    pickle_filename = 'data/pkls/lambada_ul2/p_map_' + set_partition +  'max_offset_' + str(MAX_OFFSET) +'.pickle'\n",
    "    # p_map_offset (Dict): (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        p_map_offset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimask conditionals  \n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "__Span-corruption Ensemble__ is a particular type of __Ensemble of Conditionals__.\n",
    "\n",
    "\n",
    "It aims to augment the only conditional distribution obtained by masking a number of additional spans in the middle of the input for additional distributions. \n",
    "\n",
    "\n",
    "(inaccurate notes copied from K offset ensemble)\n",
    "The key sample generation function is create_middle_off_sample() in lambada_utils, which is controlled by\n",
    "`middle_span_length`: the length of the masked span in the middle\n",
    "and \n",
    "`middle_to_end_gap`： the gap between the middle_span and the last word\n",
    "\n",
    "on average, the len of a lambada completion is 4.8 (including punc and extra_token_id)\n",
    "\n",
    "An example with the _lambada[0]_\n",
    "\n",
    "_lambada[0]['input_pretokenized']_: `... his mouth curved in a confident grin , i do n't care about <last_word>`\n",
    "\n",
    "We consider candidates `['angels.', 'signs.', 'that.']`.\n",
    "\n",
    "The baseline approach is to input `... his mouth curved in a confident grin , i do n't care about <extra_id_0>` to UL2 and obtain the distribution containing the 3 candidates.\n",
    "\n",
    "\n",
    "\n",
    "completion_lengths = [\n",
    "    id_and_offset_to_inputs_and_completions[example_index,0][completion_index][1].shape[0] - 1\n",
    "    for example_index in range(len(lambada)) \n",
    "    for completion_index in range(len(id_and_offset_to_inputs_and_completions[example_index,0]))\n",
    "] \n",
    "np.mean(completion_lengths) == 3.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param. for getting span-corruption (multispan) samples\n",
    "# num_spans_decision_method = 'auto'\n",
    "num_spans_decision_method = 'manual'\n",
    "if num_spans_decision_method == 'auto':\n",
    "    LENGTH_GAP_NUM_TUPLE = (3, 5, None)\n",
    "    AUTO_RATIO = 0.1\n",
    "    multiple_spans_filename_part = 'length_3_gap_5_numspan_auto_point_1'\n",
    "else:\n",
    "    LENGTH_GAP_NUM_TUPLE = (3, 25, 1)\n",
    "    AUTO_RATIO = None\n",
    "    # auto generates the part\n",
    "    multiple_spans_filename_part = 'length_' + str(LENGTH_GAP_NUM_TUPLE[0]) + '_gap_' + str(LENGTH_GAP_NUM_TUPLE[1]) + '_numspan_' + str(LENGTH_GAP_NUM_TUPLE[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Generate the multispan samples\n",
    "if RUN_CELL:\n",
    "    # id_span_gap_num_to_input_and_completions: maps (id, span_length, gap_between_spans, num_spans) to a input_ids(Tensor) and multiple completion_ids(List[Tensor])\n",
    "    id_span_gap_num_to_input_and_completions = \\\n",
    "        processor.get_multiple_span_samples(\n",
    "            id_to_completions_ids, \n",
    "            length_gap_num_tuple=LENGTH_GAP_NUM_TUPLE,\n",
    "            to_gpu=False,\n",
    "            auto_ratio=AUTO_RATIO\n",
    "        )\n",
    "# INFO: input length range: 70 ~ 234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Obtain the p_map for multispan samples\n",
    "if RUN_CELL:\n",
    "    # id_span_gap_num_to_input_and_completions: maps (id, span_length, gap_length, num_spans) to a input_ids(Tensor) and completion_ids(List[Tensor])\n",
    "    # p_map_multiple_spans: maps (id, middle_span_length, middle_to_end_gap, completion_index) to avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    p_map_multiple_spans = dict()\n",
    "    for id_span_gap_num in tqdm(id_span_gap_num_to_input_and_completions):\n",
    "        input_ids = id_span_gap_num_to_input_and_completions[id_span_gap_num]['inputs'].unsqueeze(0)\n",
    "        completions_batch = id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels']\n",
    "        outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "        span_length = id_span_gap_num[1]\n",
    "        if num_spans_decision_method == 'auto':\n",
    "            num_spans = processor.calculate_num_spans(\n",
    "                id_span_gap_num[0], \n",
    "                span_length, \n",
    "                id_span_gap_num[2], \n",
    "                id_span_gap_num[3],  # auto ratio\n",
    "                max_num_spans = 99\n",
    "            )\n",
    "        elif num_spans_decision_method == 'manual':\n",
    "            num_spans = id_span_gap_num[3]\n",
    "        else:\n",
    "            raise ValueError('num_spans_decision_method must be either \\'auto\\' or \\'manual\\'')\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            avg_log_p = -ce_loss(\n",
    "                # Only care about the tokens corresponding to the last word (see assert below)); \n",
    "                # so the first <extra_id_0> is omitted, and for each span, the span + <extra_id_k> is omitted;\n",
    "                # totally 1 + num_spans * (span_length + 1) tokens are omitted;\n",
    "                # contains paddings.\n",
    "                outputs.logits[completion_index][1 + num_spans * (span_length + 1) :], \n",
    "                completions_batch[completion_index][1 + num_spans * (span_length + 1) :]\n",
    "            )\n",
    "            \n",
    "            assert id_to_completions_ids[id_span_gap_num[0]][completion_index].shape[0] - 1 == \\\n",
    "                completions_batch[completion_index][1 + num_spans * (span_length + 1) :].nonzero().shape[0]\n",
    "\n",
    "            p_map_multiple_spans[(*id_span_gap_num, completion_index)] = \\\n",
    "                avg_log_p.detach().cpu().tolist()    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False   # Save the p_map_multiple_spans\n",
    "if RUN_CELL:\n",
    "    pickle_filename = 'data/pkls/lambada_ul2/p_map_multiple_spans_' + set_partition + multiple_spans_filename_part + '.pickle'\n",
    "    print('\\'' + pickle_filename + '\\'')\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(p_map_multiple_spans, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = True   # Load the p_map_multiple_spans\n",
    "if RUN_CELL:\n",
    "    # load multiple dicts and combine them.\n",
    "    multispan_types = [\n",
    "        'length_3_gap_5_numspan_1',\n",
    "        'length_3_gap_5_numspan_2',\n",
    "        'length_3_gap_5_numspan_3',\n",
    "        'length_3_gap_5_numspan_4',\n",
    "        'length_3_gap_5_numspan_5',\n",
    "        'length_3_gap_10_numspan_1',\n",
    "        'length_3_gap_15_numspan_1',\n",
    "        'length_3_gap_20_numspan_1',\n",
    "        'length_3_gap_25_numspan_1',\n",
    "        'length_3_gap_30_numspan_1',\n",
    "        'length_3_gap_35_numspan_1',\n",
    "        'length_3_gap_40_numspan_1',\n",
    "        'length_3_gap_45_numspan_1',\n",
    "        'length_3_gap_5_numspan_auto_point_3',\n",
    "        'length_3_gap_5_numspan_auto_point_1'\n",
    "    ]\n",
    "    filenames = [\n",
    "        'data/pkls/lambada_ul2/p_map_multiple_spans_' + set_partition + multispan_type + '.pickle'\n",
    "        for multispan_type in multispan_types\n",
    "    ]\n",
    "    p_map_multiple_spans = dict()\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'rb') as handle:\n",
    "            p_map_multiple_spans.update(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = True  # Filter saved p_maps by length\n",
    "if RUN_CELL:\n",
    "    p_map_offset_new = {}\n",
    "    for key in p_map_offset:\n",
    "        old_example_index = key[0]\n",
    "        if old_example_index in old_id_to_new_id:\n",
    "            new_example_index = old_id_to_new_id[old_example_index]\n",
    "            p_map_offset_new[(new_example_index, key[1], key[2])] = p_map_offset[key]\n",
    "    p_map_offset = p_map_offset_new\n",
    "\n",
    "    p_map_multiple_spans_new = {}\n",
    "    for key in p_map_multiple_spans:\n",
    "        old_example_index = key[0]\n",
    "        if old_example_index in old_id_to_new_id:\n",
    "            new_example_index = old_id_to_new_id[old_example_index]\n",
    "            p_map_multiple_spans_new[(new_example_index, key[1], key[2], key[3], key[4])] = p_map_multiple_spans[key]\n",
    "    p_map_multiple_spans = p_map_multiple_spans_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble of Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [00:00<00:00, 51980.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7606334841628959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True # Max reduction to emsemble conditionals for the same last word\n",
    "'''Max reduction to emsemble conditionals for the same last word, \n",
    "i.e., only the maximum avg_log_p is kept for each last word across different range_middle_span_length's and range_middle_to_end_gap's.\n",
    "Emsemble the baseline conditionals with the K-offset conditionals and middle-off conditionals.'''\n",
    "\n",
    "if RUN_CELL:\n",
    "    # Add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "    ADD_BASELINE = True\n",
    "\n",
    "    # Add K-offset conditionals to the list\n",
    "    ADD_K_OFFSET = False\n",
    "    MAX_OFFSET = 4\n",
    "\n",
    "    # Add multispan conditionals to the list\n",
    "    ADD_MULTISPAN = False\n",
    "    LENGTH_GAP_NUM_TUPLES = [\n",
    "        (3, 5, 1), \n",
    "        (3, 5, 2), \n",
    "        # (3, 5, 3), \n",
    "        # (3, 5, 4),\n",
    "        # (3, 5, 5),\n",
    "        (3, 10, 1),\n",
    "        # (3, 15, 1),\n",
    "        # (3, 20, 1),\n",
    "        # (3, 25, 1),\n",
    "        # (3, 30, 1),\n",
    "        # (3, 35, 1),\n",
    "        # (3, 40, 1),\n",
    "        # (3, 45, 1),\n",
    "        # (3, 5, 0.3),\n",
    "        # (3, 5, 0.1),\n",
    "    ] # SPAN_LENGTH, GAP_LENGTH, NUM_SPANS. NUM_SPANS can be float, which is treated as auto_ratio.\n",
    "\n",
    "    count_correct = 0\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "        p_and_completion = []\n",
    "                \n",
    "        # add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "        if ADD_BASELINE:\n",
    "            p_and_completion += [\n",
    "                (p_map_offset[(example_index, 0, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "            ]\n",
    "            \n",
    "        # add the whole K-offset ensemble to the list\n",
    "        if ADD_K_OFFSET:\n",
    "            for offset in range(1, MAX_OFFSET + 1):\n",
    "                p_and_completion += [\n",
    "                    (p_map_offset[(example_index, offset, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                    for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "                ]\n",
    "                \n",
    "        if ADD_MULTISPAN:\n",
    "            p_and_completion += [\n",
    "                (p_map_multiple_spans[(example_index, *length_gap_num, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "                for length_gap_num in LENGTH_GAP_NUM_TUPLES\n",
    "            ]\n",
    "            \n",
    "        if len(p_and_completion) == 0: # if no completions are found\n",
    "            continue\n",
    "        # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "        best_avg_log_p, best_word = max(p_and_completion, key=lambda x: x[0])\n",
    "        if processor.is_correct_result(example_index, best_word):\n",
    "            count_correct += 1\n",
    "    print(\"accuracy:\", count_correct / len(lambada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a set of conditionals (from offsets, multispans, etc.), find the best combination by trying out all the possible combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    " Hypothesis: conditionals based on the mask patterns used during pretraining are more powerful; ->  change your story: smartly pick mask patterns to only use one conditional\n",
    "\n",
    "\n",
    " just ensemble with one LENGTH_GAP_TUPLE == (3,5) leads to accuracy: 0.7814865127110421\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls\n",
    "\n",
    "#### tokenizer.decode() can ignore spaces\n",
    "tokenizer.get_vocab()['▁'] == 3\n",
    "tokenizer.decode([32099,  4340,     3,     5]) = 0 '<extra_id_0> cake.' # the lowline token is  \n",
    "\n",
    "This happens a lot: tokenizer.decode(tokenizer.encode(text)) != text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = False # Quantify disagreement on last word predictions among K-offset conditionals\n",
    "if RUN_CELL: \n",
    "    for NUM_CONDITIONALS in range(2, 6): # 2, 3, 4, 5; how many sets of conditionals to consider; offset = 0 and offset = 1 are 2 different sets of conditionals\n",
    "        id_offset_to_lastword = dict()\n",
    "        id_to_lastwords_by_offsets = dict()\n",
    "        for offset in range(NUM_CONDITIONALS): # if NUM_CONDITIONALS = 2, then offset = 0, 1\n",
    "            for example_index in range(len(lambada)): # len(lambada)\n",
    "                # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "                p_and_completion = [\n",
    "                    (p_map_offset[(example_index, offset, completion_index)], id_to_completions_ids[example_index][completion_index])\n",
    "                    for completion_index in range(len(id_to_completions_ids[example_index]))\n",
    "                ]\n",
    "                if len(p_and_completion) == 0:\n",
    "                    continue\n",
    "                # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "                best_avg_log_p, best_completion = max(p_and_completion, key=lambda x: x[0])\n",
    "                lastword = processor.get_word_from_completion(tokenizer.decode(best_completion))\n",
    "                id_offset_to_lastword[(example_index, offset)] = lastword\n",
    "                if example_index not in id_to_lastwords_by_offsets:\n",
    "                    id_to_lastwords_by_offsets[example_index] = []\n",
    "                id_to_lastwords_by_offsets[example_index].append(lastword)\n",
    "        no_disagreement_count = 0\n",
    "        for example_index in id_to_lastwords_by_offsets:\n",
    "            if len(set(id_to_lastwords_by_offsets[example_index])) > 1:\n",
    "                no_disagreement_count += 1\n",
    "        ratio_disagreement = no_disagreement_count / (len(lambada) - ul2_lambada_vanilla_beam_search_results['count_no_words_found'])\n",
    "        print(\"NUM_CONDITIONALS\", NUM_CONDITIONALS, \"ratio_disagreement\", ratio_disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: (2936409): Operation not permitted\n",
      "kill: (2388587): No such process\n",
      "kill: (3326970): Operation not permitted\n",
      "kill: (2934557): Operation not permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killed process 2936409 on GPU 2\n",
      "Killed process 2388587 on GPU 6\n",
      "Killed process 3326970 on GPU 6\n",
      "Killed process 2934557 on GPU 7\n"
     ]
    }
   ],
   "source": [
    "general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
