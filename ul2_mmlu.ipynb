{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/**\n",
    "#* @file ul2_mmlu.ipynb\n",
    "#* @author chenyunan (chen.yunan_01@nus.edu.sg)\n",
    "#* @brief\n",
    "#* @version 0.1\n",
    "#* @date 2023-12-04\n",
    "#*\n",
    "#* @copyright Copyright (c) 2023 \n",
    "#*\n",
    "#*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import general_utils\n",
    "# clear GPU memory\n",
    "if True:   \n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import lambada_utils\n",
    "from lambada_utils import LambadaProcessor\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6101481c9a41e1aca55ecc7c66387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n",
    "RUN_CELL = 1 # Load model 1\n",
    "# device_map=general_utils.get_ul2_device_map('2,3')\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\",\n",
    "                                                        cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2',\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MMLU datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/oem/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/134145dc2582b9a08b42d1f4b828f84a0066e9cc2e7dd8c1d83bee475746ecc3 (last modified on Tue Dec  5 19:07:00 2023) since it couldn't be found locally at lukaemon/mmlu., or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/oem/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/134145dc2582b9a08b42d1f4b828f84a0066e9cc2e7dd8c1d83bee475746ecc3 (last modified on Tue Dec  5 19:07:00 2023) since it couldn't be found locally at lukaemon/mmlu., or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/oem/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/134145dc2582b9a08b42d1f4b828f84a0066e9cc2e7dd8c1d83bee475746ecc3 (last modified on Tue Dec  5 19:07:00 2023) since it couldn't be found locally at lukaemon/mmlu., or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/oem/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/134145dc2582b9a08b42d1f4b828f84a0066e9cc2e7dd8c1d83bee475746ecc3 (last modified on Tue Dec  5 19:07:00 2023) since it couldn't be found locally at lukaemon/mmlu., or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SUBJECTS = ['high_school_european_history', 'business_ethics', 'clinical_knowledge', 'medical_genetics', \\\n",
    "            'high_school_us_history', 'high_school_physics', 'high_school_world_history', 'virology', \\\n",
    "            'high_school_microeconomics', 'econometrics', 'college_computer_science', 'high_school_biology', \\\n",
    "            'abstract_algebra', 'professional_accounting', 'philosophy', 'professional_medicine', 'nutrition', \\\n",
    "            'global_facts', 'machine_learning', 'security_studies', 'public_relations', 'professional_psychology', \\\n",
    "            'prehistory', 'anatomy', 'human_sexuality', 'college_medicine', 'high_school_government_and_politics', \\\n",
    "            'college_chemistry', 'logical_fallacies', 'high_school_geography', 'elementary_mathematics', 'human_aging', \\\n",
    "            'college_mathematics', 'high_school_psychology', 'formal_logic', 'high_school_statistics', 'international_law', \\\n",
    "            'high_school_mathematics', 'high_school_computer_science', 'conceptual_physics', 'miscellaneous', 'high_school_chemistry', \\\n",
    "            'marketing', 'professional_law', 'management', 'college_physics', 'jurisprudence', 'world_religions', 'sociology', 'us_foreign_policy', \\\n",
    "            'high_school_macroeconomics', 'computer_security', 'moral_scenarios', 'moral_disputes', 'electrical_engineering', 'astronomy', 'college_biology']\n",
    "\n",
    "DATASET_PATH = os.path.join(\"lukaemon/mmlu\")\n",
    "MMLU_DATAS = [load_dataset(DATASET_PATH, sub) for sub in SUBJECTS]\n",
    "INDEX = [i for i in range(len(SUBJECTS))]\n",
    "NAMES_WITH_DATAS = zip(INDEX, SUBJECTS, MMLU_DATAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_school_european_history\n",
      "Dataset({\n",
      "    features: ['input', 'A', 'B', 'C', 'D', 'target'],\n",
      "    num_rows: 164\n",
      "})\n",
      "This question refers to the following information.\n",
      "Read the the following quotation to answer questions.\n",
      "The various modes of worship which prevailed in the Roman world were all considered by the people as equally true; by the philosopher as equally false; and by the magistrate as equally useful.\n",
      "Edward Gibbon, The Decline and Fall of the Roman Empire, 1776â€“1788\n",
      "Gibbon's interpretation of the state of religious worship in ancient Rome could be summarized as\n",
      "Gibbon's interpretation of\n"
     ]
    }
   ],
   "source": [
    "MAX_COMPLETION_LENGTH = 8\n",
    "NUM_BEAMS = 20\n",
    "for index,name,data in NAMES_WITH_DATAS:\n",
    "    print(name)\n",
    "    print((data['test']))\n",
    "\n",
    "    input_string = data['test'][0]['input']\n",
    "    print(input_string)\n",
    "\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(inputs,\n",
    "                            max_length=MAX_COMPLETION_LENGTH, \n",
    "                            num_beams=NUM_BEAMS, \n",
    "                            num_return_sequences=NUM_BEAMS, \n",
    "                            output_scores=True,\n",
    "                            eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                            return_dict_in_generate=True)\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0][0], skip_special_tokens=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_id_0 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")])\n",
    "extra_id_1 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_1>\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Question prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "UL2_MODE = \"[NLG]\"\n",
    "\n",
    "def data_prompting(docs, tokenizer) -> Tuple:\n",
    "    '''\n",
    "        docs: DATA_SET[SUBJECTS_NAME], ex:MMLU[high_school_european_history]\n",
    "        return: Tuple(input_ids, labels)\n",
    "\n",
    "        input[example]: Question:<prompt> \n",
    "        label[example]: A. <choice1> B. <choice2> C. <choice3> D. <choice4>\n",
    "\n",
    "        Todo: few-shot data prompting\n",
    "    '''\n",
    "\n",
    "    keys = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for doc in docs:\n",
    "        input_ = UL2_MODE + \" \" + doc['input'] + \" \" + \"<extra_id_0>\"\n",
    "        completions = [f\"<extra_id_0> {doc[key]} <extra_id_1>\" for key in keys]\n",
    "        label = f\"{doc[doc['target']]}\"\n",
    "        \n",
    "        input_id = tokenizer(input_, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        # label_id = tokenizer(label, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        completions_ids = [tokenizer(completion, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\\\n",
    "                                                                for completion in completions]\n",
    "\n",
    "        # Assuming `max_length` is the maximum length you want to pad sequences to\n",
    "        max_length = max(seq.size(1) for seq in completions_ids)\n",
    "\n",
    "        # Pad sequences to the common length\n",
    "        padded_sequences = [F.pad(seq, (0, max_length - seq.size(1)), value=tokenizer.pad_token_id) for seq in completions_ids]\n",
    "\n",
    "        # Use pad_sequence\n",
    "        completions_ids_padded = torch.nn.utils.rnn.pad_sequence(padded_sequences, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "        completions_ids_padded = torch.squeeze(completions_ids_padded, dim = 1)\n",
    "        yield input_id, completions_ids_padded, label\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Development Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEVELOPMENT = False\n",
    "set_partition = 'validation' if IS_DEVELOPMENT else 'test' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDING_PUNCTUATIONS = '<' # If the model generates one, it is considered that the sentence is complete and we can parse for the last word\n",
    "\n",
    "def get_word_from_completion(completion: str):\n",
    "    '''Get the last word from the given completion, if there is a valid one. Return the word.'''\n",
    "    found = False\n",
    "    word = None\n",
    "    # if a punctuation can be found in the completion, get the string before the punctuation\n",
    "    for i in range(len(completion)):\n",
    "        if completion[i] in ENDING_PUNCTUATIONS:\n",
    "            word = completion[:i]\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        return None\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_completion(completion:torch.Tensor, label:str):\n",
    "    if not isinstance(completion, torch.Tensor):\n",
    "        return False\n",
    "    completion_string = tokenizer.decode(completion)\n",
    "\n",
    "    # print(f'completion_string:{completion_string}')\n",
    "    if not isinstance(completion_string, str):\n",
    "        return False\n",
    "    word = get_word_from_completion(completion_string)\n",
    "    # print(f'word:{word}')\n",
    "    if not isinstance(word, str):\n",
    "        return False\n",
    "    if word == label:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 1 # Obtain the avg_log_p_map_offset\n",
    "MAX_OFFSET = 1\n",
    "\n",
    "TOTAL_CASE = 0\n",
    "ACUURACTE_CASE = 0\n",
    "\n",
    "if RUN_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> input_ids, [completion_ids_0, completion_ids_1, completion_ids_2,...]\n",
    "    avg_log_p_map_offset = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    \n",
    "    for example_index in tqdm(range(len(INDEX))): \n",
    "    # for example_index in tqdm(range(1)): \n",
    "        data = MMLU_DATAS[example_index]\n",
    "        print(SUBJECTS[example_index])\n",
    "\n",
    "        for offset in range(MAX_OFFSET):\n",
    "            gen = data_prompting(data[set_partition], tokenizer)\n",
    "\n",
    "            for input_ids, completions_batch, label in gen:\n",
    "                avg_log_p_and_completion = []\n",
    "                outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "\n",
    "                for completion_index in range(len(completions_batch)):\n",
    "                    avg_log_p = -ce_loss(\n",
    "                        # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                        # the first one is <extra_id_0> and omitted\n",
    "                        outputs.logits[completion_index][1+offset:], \n",
    "                        completions_batch[completion_index][1+offset:]\n",
    "                    )\n",
    "                    avg_log_p_map_offset[(example_index, offset, completion_index)] = \\\n",
    "                        avg_log_p.detach().cpu().tolist()\n",
    "                    \n",
    "                    avg_log_p_and_completion.append([avg_log_p.detach().cpu().tolist(), completions_batch[completion_index]])\n",
    "\n",
    "                best_avg_log_p, best_completion = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "                # print(tokenizer.decode(best_completion))\n",
    "                # print(f'label:{label}')\n",
    "\n",
    "                if is_correct_completion(best_completion[1+offset:], label):\n",
    "                    ACUURACTE_CASE += 1\n",
    "                    print(f'count_correct +1 : {ACUURACTE_CASE}')\n",
    "                TOTAL_CASE +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4280"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACUURACTE_CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13985"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30604218805863426"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACUURACTE_CASE/TOTAL_CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 1 # Max reduction to emsemble conditionals for the same last word\n",
    "'''Max reduction to emsemble conditionals for the same last word, \n",
    "i.e., only the maximum avg_log_p is kept for each last word across different range_middle_span_length's and range_middle_to_end_gap's.\n",
    "Emsemble the baseline conditionals with the K-offset conditionals and middle-off conditionals.'''\n",
    "\n",
    "\n",
    "if RUN_CELL:\n",
    "    # Add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "    ADD_BASELINE = True\n",
    "    \n",
    "    for example_index in tqdm(range(len(INDEX))): \n",
    "        avg_log_p_and_completion = dict()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
