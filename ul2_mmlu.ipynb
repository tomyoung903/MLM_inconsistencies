{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/**\n",
    "#* @author chenyunan (chen.yunan_01@nus.edu.sg)\n",
    "#* @version 0.1\n",
    "#* @date 2023-12-04\n",
    "#* @copyright Copyright (c) 2023 \n",
    "#*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tom's update Jan 1, 2024\n",
    "\n",
    "\n",
    " accuracy currently at 0.336\n",
    "\n",
    "(1) removed is_correct_completion(); we now simply check the index\n",
    "\n",
    "(2) removed <extra_id_1> and <eos_token> from cross_entropy calculation\n",
    "\n",
    "(3) fixed typos\n",
    "\n",
    "(4) for Yunan -- how to further improve the code & performance:\n",
    "    \n",
    "    (a) On many of the samples, the constructed completions can contain <unk>'s, for example, when there is a { symbol in the completion. \n",
    "        {AND, OR}\n",
    "        get tokenized into\n",
    "        <unk> AND , ▁OR <unk> </s>\n",
    "\n",
    "        Having <unk>'s can hurt performance. One possible solution: remove the symbols can lead to <unk>'s \n",
    "\n",
    "    (b) Consider better prompt designs. For example, some questions end with a question mark, e.g., \n",
    "        Q: If the foot is abducted, it is moved in which direction?\t\n",
    "        A: 1. Inward\t2. Outward\t3. Upward\t4. Downward\n",
    "        To make it \"smoother\" for the LLM, we can modify the prompt to be,\n",
    "        Prompt: Question: If the foot is abducted, it is moved in which direction? Answer: \n",
    "        \n",
    "        Also, we can check how llm_eval_harness and instruct_eval did it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,4,5,6,7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import general_utils\n",
    "# clear GPU memory\n",
    "if True:   \n",
    "    general_utils.kill_gpu_process(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "import torch\n",
    "os.environ['PYDEVD_WARN_SLOW_RESOLVE_TIMEOUT'] = '5.0' # suppresses pydevd speed warnings\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import lambada_utils\n",
    "from lambada_utils import LambadaProcessor\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# We are using custom huggingface cache dirs in case the default one doesn't have the capacity, since the models can be quite large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='/data/personal/nus-ytj/MLM_inconsistencies/huggingface_cache' # relative to this notebook path\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\",\n",
    "                                        cache_dir = MY_HUGGINGFACE_CACHE_DIR+'/google-ul2')\n",
    "\n",
    "RUN_CELL = 1 # Load model 1\n",
    "# device_map=general_utils.get_ul2_device_map('2,3')\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\",\n",
    "                                                        cache_dir=MY_HUGGINGFACE_CACHE_DIR + '/google-ul2',\n",
    "                                                        low_cpu_mem_usage=True,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import MMLU datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SUBJECTS = ['high_school_european_history', 'business_ethics', 'clinical_knowledge', 'medical_genetics', \\\n",
    "            'high_school_us_history', 'high_school_physics', 'high_school_world_history', 'virology', \\\n",
    "            'high_school_microeconomics', 'econometrics', 'college_computer_science', 'high_school_biology', \\\n",
    "            'abstract_algebra', 'professional_accounting', 'philosophy', 'professional_medicine', 'nutrition', \\\n",
    "            'global_facts', 'machine_learning', 'security_studies', 'public_relations', 'professional_psychology', \\\n",
    "            'prehistory', 'anatomy', 'human_sexuality', 'college_medicine', 'high_school_government_and_politics', \\\n",
    "            'college_chemistry', 'logical_fallacies', 'high_school_geography', 'elementary_mathematics', 'human_aging', \\\n",
    "            'college_mathematics', 'high_school_psychology', 'formal_logic', 'high_school_statistics', 'international_law', \\\n",
    "            'high_school_mathematics', 'high_school_computer_science', 'conceptual_physics', 'miscellaneous', 'high_school_chemistry', \\\n",
    "            'marketing', 'professional_law', 'management', 'college_physics', 'jurisprudence', 'world_religions', 'sociology', 'us_foreign_policy', \\\n",
    "            'high_school_macroeconomics', 'computer_security', 'moral_scenarios', 'moral_disputes', 'electrical_engineering', 'astronomy', 'college_biology']\n",
    "\n",
    "# SUBJECTS = SUBJECTS[10:11] # tom is only using one subject for testing\n",
    "\n",
    "\n",
    "DATASET_PATH = os.path.join(\"lukaemon/mmlu\")\n",
    "MMLU_DATAS = [load_dataset(DATASET_PATH, sub) for sub in SUBJECTS]\n",
    "INDEX = [i for i in range(len(SUBJECTS))]\n",
    "NAMES_WITH_DATAS = zip(INDEX, SUBJECTS, MMLU_DATAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "<pad>█<extra_id_0>█▁The█▁set█▁of█▁all█▁common█▁di█visor█s█▁of█▁two█▁integer█s█▁is█▁the█▁set█▁of█▁all█▁integer█s█▁that█▁are█▁di█visor█s█▁of█▁both█▁integer█s█.█▁The█▁set█▁of█▁all█▁common█▁di█visor█s█▁of█▁two█▁integer█s█▁is█▁the█▁set█▁of█▁all█▁integer█s█▁that█▁are█▁di█visor█s█▁of█▁both█▁integer█s█.█▁The█▁set█▁of█▁all█▁common█▁di█visor█s█▁of█▁two█▁integer█s█▁is█▁the█▁set█▁of█▁all█▁integer█s█▁that█▁are█▁di█visor█s█▁of█▁both█▁integer█s█.█</s>\n",
      "<extra_id_0> {-6, -3, -2, -1, 1, 2, 3, 6}\n",
      "<extra_id_0>█▁█<unk>█-6█,█▁█-3█,█▁█-2█,█▁█-1█,█▁1,█▁2,█▁3,█▁6█<unk>█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁█a█▁factor█▁of█▁█ten█.█▁The█▁ratio█▁of█▁the█▁time█▁required█▁to█▁read█▁█a█▁large█▁file█▁under█▁version█▁2█▁to█▁the█▁time█▁required█▁to█▁read█▁the█▁same█▁large█▁file█▁under█▁version█▁3█▁is█▁approximately█▁█a█▁factor█▁of█▁two█.█▁The█▁ratio█▁of█▁the█▁time█▁required█▁to█▁read█▁█a█▁large█▁file█▁under█▁version█▁3█▁to█▁the█▁time█▁required█▁to█▁read█▁the█▁same█▁large█▁file█▁under█▁version█▁4█▁is█▁approximately█▁█a█▁factor█▁of█▁one█-█and█-█a█-█half█.█▁█.█▁█.█</s>\n",
      "<extra_id_0> 1:3.5\n",
      "<extra_id_0>█▁1█:█3.5█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█1█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█2█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█3█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█1█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█2█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█3█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█1█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█2█?█▁What█▁is█▁the█▁cluster█▁center█▁for█▁cluster█▁C█3█?█</s>\n",
      "<extra_id_0> C1: (3,3), C2: (4,4), C3: (6,6)\n",
      "<extra_id_0>█▁C█1█:█▁(3█,█3)█,█▁C█2█:█▁(4█,█4)█,█▁C█3█:█▁(6█,█6)█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█▁Which█▁of█▁the█▁following█▁is█▁NOT█▁complete█?█</s>\n",
      "<extra_id_0> {AND, OR}\n",
      "<extra_id_0>█▁█<unk>█AND█,█▁OR█<unk>█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁(█A█)█▁The█▁graph█▁has█▁█a█▁single█▁ver█tex█.█▁(█B█)█▁The█▁graph█▁has█▁no█▁edges█.█▁(█C█)█▁The█▁graph█▁has█▁no█▁self█l█oop█s█.█▁(█D█)█▁The█▁graph█▁has█▁no█▁edges█.█▁(█A█)█▁The█▁graph█▁has█▁no█▁edges█.█▁(█B█)█▁The█▁graph█▁has█▁no█▁self█l█oop█s█.█▁(█C█)█▁The█▁graph█▁has█▁no█▁edges█.█▁(█D█)█▁The█▁graph█▁has█▁no█▁self█l█oop█s█▁and█▁no█▁edges█.█</s>\n",
      "<extra_id_0> M = 7, m = 4\n",
      "<extra_id_0>█▁M█▁=█▁7,█▁█m█▁=█▁4█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁IV█.█▁It█▁is█▁█a█▁complete█▁proof█▁system█▁in█▁the█▁sense█▁that█▁there█▁is█▁█a█▁proof█▁of█▁un█sati█s█f█i█ability█▁for█▁every█▁uns█a█▁█t█is█f█i█able█▁formula█▁of█▁proposition█al█▁logic█.█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█?█</s>\n",
      "<extra_id_0> I and II only\n",
      "<extra_id_0>█▁I█▁and█▁II█▁only█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁IV█.█▁It█▁suffer█s█▁from█▁external█▁fragment█ation█.█▁Which█▁of█▁the█▁following█▁is█▁█a█▁characteristic█▁of█▁█a█▁█purely█▁segment█e█d█▁memory█▁system█?█▁I█.█▁It█▁divide█s█▁memory█▁into█▁units█▁of█▁equal█▁size█.█▁II█.█▁It█▁permits█▁implementation█▁of█▁virtual█▁memory█.█▁III█.█▁It█▁suffer█s█▁from█▁internal█▁fragment█ation█.█▁IV█.█▁It█▁suffer█s█▁from█▁external█▁fragment█ation█.█▁IV█.█▁It█▁suffer█s█▁from█▁external█▁fragment█ation█.█▁Correct█▁█:█▁It█▁suffer█s█▁from█▁external█▁fragment█ation█.█</s>\n",
      "<extra_id_0> II only\n",
      "<extra_id_0>█▁II█▁only█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁█Flo█ating█-█point█▁█a█rith█m█etic█▁is█▁█a█▁method█▁of█▁representing█▁real█▁numbers█▁in█▁█a█▁fi█nite█▁number█▁of█▁bits█.█▁█Flo█ating█-█point█▁█a█rith█m█etic█▁is█▁█a█▁method█▁of█▁representing█▁real█▁numbers█▁in█▁█a█▁fi█nite█▁number█▁of█▁bits█.█▁This█▁page█▁was█▁last█▁edited█▁on█▁26█▁May█▁2017,█▁at█▁15█:█59█▁(█UT█C█).█</s>\n",
      "<extra_id_0> Associativity can be achieved with appropriate roundoff conventions.\n",
      "<extra_id_0>█▁As█soci█at█ivity█▁can█▁be█▁achieved█▁with█▁appropriate█▁round█off█▁convention█s█.█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█▁to█▁sharing█?█▁What█▁is█▁█a█▁primary█▁draw█back█▁to█▁this█▁approach█?█</s>\n",
      "<extra_id_0> If the owner is allowed to delete a file, dangling links may result.\n",
      "<extra_id_0>█▁If█▁the█▁owner█▁is█▁allowed█▁to█▁delete█▁█a█▁file█,█▁█d█ang█ling█▁links█▁may█▁result█.█</s>\n",
      "-------\n",
      "<pad>█<extra_id_0>█▁of█▁integer█s█▁IV█.█▁A█▁doubl█y█▁linked█▁list█▁of█▁integer█s█.█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█▁█<unk>█?█</s>\n",
      "<extra_id_0> I, II, and III\n",
      "<extra_id_0>█▁I█,█▁II█,█▁and█▁III█</s>\n"
     ]
    }
   ],
   "source": [
    "''''Test generated completion against constructed completion'''\n",
    "RUN_CELL = 1\n",
    "example_id = 3\n",
    "NAMES_WITH_DATAS = list(NAMES_WITH_DATAS)\n",
    "data = NAMES_WITH_DATAS[0][2]\n",
    "if RUN_CELL:\n",
    "    for example_id in range(10):\n",
    "        MAX_COMPLETION_LENGTH = 100\n",
    "        NUM_BEAMS = 1\n",
    "        example = data['test'][example_id]\n",
    "        # print(data['test'])\n",
    "        example_input = example['input']\n",
    "        input_string = '[NLG] ' + example_input + ' <extra_id_0>'\n",
    "        # print('\\ninput_string:', input_string)\n",
    "\n",
    "        inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs,\n",
    "                                max_length=MAX_COMPLETION_LENGTH, \n",
    "                                num_beams=NUM_BEAMS, \n",
    "                                num_return_sequences=NUM_BEAMS, \n",
    "                                output_scores=True,\n",
    "                                # eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                                return_dict_in_generate=True)\n",
    "\n",
    "        # print('\\ncompletion generated:')\n",
    "        # print(tokenizer.decode(outputs[0][0]))\n",
    "        # print(outputs[0][0])\n",
    "        # print all tokens put in a long string\n",
    "        tokens = [tokenizer.convert_ids_to_tokens([id_])[0] for id_ in outputs[0][0]]\n",
    "        print('-------')\n",
    "        print('\\u2588'.join(tokens))\n",
    "\n",
    "        key = example['target']\n",
    "        # print('\\ncompletion constructed:')\n",
    "        completion_constructed = f\"<extra_id_0> {example[key]}\"\n",
    "        print(completion_constructed)\n",
    "        # print(tokenizer(completion_constructed, return_tensors=\"pt\").input_ids)\n",
    "        tokens = [tokenizer.convert_ids_to_tokens([id_])[0] for id_ in tokenizer(completion_constructed, return_tensors=\"pt\").input_ids[0]]\n",
    "        print('\\u2588'.join(tokens))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_id_0 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")])\n",
    "extra_id_1 = torch.tensor([tokenizer.convert_tokens_to_ids(\"<extra_id_1>\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Question prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "UL2_MODE = \"[NLG]\"\n",
    "\n",
    "def data_prompting(docs, tokenizer) -> Tuple:\n",
    "    '''\n",
    "        docs: DATA_SET[SUBJECTS_NAME], ex:MMLU[high_school_european_history]\n",
    "        return: Tuple(input_ids, labels)\n",
    "\n",
    "        input[example]: Question:<prompt> \n",
    "        label[example]: A. <choice1> B. <choice2> C. <choice3> D. <choice4>\n",
    "\n",
    "        Todo: few-shot data prompting\n",
    "    '''\n",
    "    keys = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    key_to_index = {\"A\":0, \"B\":1, \"C\":2, \"D\":3}\n",
    "    for doc in docs:\n",
    "        input_ = UL2_MODE + \" \" + doc['input'] + \" \" + \"<extra_id_0>\"\n",
    "        # print(input_)\n",
    "        # completions = [f\"<extra_id_0> {doc[key]} <extra_id_1>\" for key in keys]\n",
    "        completions = [f\"<extra_id_0> {doc[key]}\" for key in keys]\n",
    "        # print(completions)\n",
    "        label = key_to_index[doc['target']]\n",
    "        \n",
    "        input_ids = tokenizer(input_, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        # label_id = tokenizer(label, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\n",
    "        # completions_ids = [tokenizer(completion, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach().requires_grad_(False)\\\n",
    "                                                                # for completion in completions]\n",
    "        completions_ids = [tokenizer(completion, return_tensors=\"pt\").input_ids.to(\"cuda\").clone().detach()[:,:-1]\\\n",
    "                                                                for completion in completions] # remove <eos> token with [:,:-1]\n",
    "        # print(completions_ids)\n",
    "        # Assuming `max_length` is the maximum length you want to pad sequences to\n",
    "        max_length = max(seq.size(1) for seq in completions_ids)\n",
    "\n",
    "        # Note to Yunan: Please compress the following 2 code lines to remove one \"pad\" function call; Consult chatgpt or official doc for guidance on how to pad simply and effectively\n",
    "        # Pad sequences to the common length\n",
    "        padded_sequences = [F.pad(seq, (0, max_length - seq.size(1)), value=tokenizer.pad_token_id) for seq in completions_ids]\n",
    "\n",
    "        # Use pad_sequence\n",
    "        completions_ids_padded = torch.nn.utils.rnn.pad_sequence(padded_sequences, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "        completions_ids_padded = torch.squeeze(completions_ids_padded, dim = 1)\n",
    "        yield input_ids, completions_ids_padded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_DEVELOPMENT = False\n",
    "set_partition = 'validation' if IS_DEVELOPMENT else 'test' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# (1)   0%|          | 0/57 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MMLU_DATAS[0]\n",
    "gen = data_prompting(data[set_partition], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, completions_batch, label = gen.__next__()\n",
    "# print(f'input_ids:{input_ids}')\n",
    "print(f'completions_batch:{completions_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [23:42<00:00, 24.96s/it]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = 1 # Obtain the avg_log_p_map_offset\n",
    "TOTAL_CASE = 0\n",
    "ACCURATE_CASE = 0\n",
    "\n",
    "if RUN_CELL:\n",
    "# id_and_offset_to_input_and_completions:\n",
    "# (id, offset) -> input_ids, [completion_ids_0, completion_ids_1, completion_ids_2,...]\n",
    "    avg_log_p_map_offset = dict() # (id, offset, completion_index) -> avg_log_p of the tokens constituting the last word (might be punctuated)\n",
    "    \n",
    "    for example_index in tqdm(range(len(INDEX))): \n",
    "    # for example_index in tqdm(range(2)):\n",
    "        data = MMLU_DATAS[example_index]\n",
    "        # print(SUBJECTS[example_index])\n",
    "\n",
    "        gen = data_prompting(data[set_partition], tokenizer)\n",
    "\n",
    "        for input_ids, completions_batch, label in gen:\n",
    "            avg_log_p_and_completion = []\n",
    "            outputs = lambada_utils.multi_labels_forward(model, input_ids, completions_batch)\n",
    "            # print('new completion batch')\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                \n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][1:], \n",
    "                    completions_batch[completion_index][1:]\n",
    "                )\n",
    "                \n",
    "                avg_log_p_map_offset[(example_index, 0, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()\n",
    "                \n",
    "                avg_log_p_and_completion.append([avg_log_p.detach().cpu().tolist(), completion_index])\n",
    "                \n",
    "            best_avg_log_p, best_completion_index = max(avg_log_p_and_completion, key=lambda x: x[0])\n",
    "\n",
    "            if best_completion_index == label:\n",
    "                ACCURATE_CASE += 1\n",
    "            TOTAL_CASE += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3368609224168752"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACCURATE_CASE / TOTAL_CASE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
