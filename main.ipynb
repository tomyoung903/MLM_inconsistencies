{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.2.0\n",
      "tqdm==4.66.2\n",
      "transformers==4.38.1\n",
      "ipykernel==6.29.2\n",
      "SentencePiece==0.2.0\n",
      "accelerate==0.27.2\n",
      "matplotlib==3.8.3\n",
      "GitPython==3.1.42\n",
      "psutil==5.9.8\n",
      "datasets==2.17.1\n",
      "bitsandbytes==0.42.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# List of packages to check versions for\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"tqdm\",\n",
    "    \"transformers\",\n",
    "    \"ipykernel\",\n",
    "    \"SentencePiece\",\n",
    "    \"accelerate\",\n",
    "    \"matplotlib\",\n",
    "    \"GitPython\",\n",
    "    \"psutil\",\n",
    "    \"datasets\",\n",
    "    \"bitsandbytes\",\n",
    "]\n",
    "\n",
    "def get_latest_version(package_name):\n",
    "    url = f\"https://pypi.org/pypi/{package_name}/json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the response was an error\n",
    "        data = response.json()\n",
    "        return data['info']['version']\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "# Fetch and format the latest version for each package for requirements.txt\n",
    "requirements_lines = []\n",
    "for package in packages:\n",
    "    latest_version = get_latest_version(package)\n",
    "    if latest_version:\n",
    "        requirements_lines.append(f\"{package}=={latest_version}\")\n",
    "    else:\n",
    "        requirements_lines.append(f\"# Could not fetch latest version for {package}\")\n",
    "\n",
    "# Join the lines to format as expected in requirements.txt\n",
    "requirements_text = \"\\n\".join(requirements_lines)\n",
    "print(requirements_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_visible_devices = '1'\n",
    "# model_identifier = \"t5-11b\" \n",
    "model_identifier = \"google-ul2\"\n",
    "# Use custom huggingface cache dirs in case the default one has low capacity, since the models are large.\n",
    "MY_HUGGINGFACE_CACHE_DIR ='huggingface_cache'\n",
    "dataset_name = \"MMLU\" \n",
    "# dataset_name = \"BigBench\"\n",
    "# K-offset conditionals\n",
    "ALL_OFFSETS = [1, 2, 3,]\n",
    "# Multispan (Multimask) conditionals\n",
    "ALL_LENGTH_GAP_NUM_TUPLES = [\n",
    "    (3, 5, 1),\n",
    "    (3, 5, 2),\n",
    "    (3, 3, 1),\n",
    "    (3, 3, 2),\n",
    "    (3, 4, 1),\n",
    "    (3, 4, 2),\n",
    "]\n",
    "# filtering samples for specific lens for best sensitivity\n",
    "INPUT_LEN_MIN = 20 # the length of the input should be at least 20\n",
    "COMPLETION_LEN_MAX = 5 # the length of the completion should be at most 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and global utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nus-ytj/miniconda3/envs/my_inconsistencies/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_visible_devices\n",
    "from itertools import combinations\n",
    "import random\n",
    "import pickle\n",
    "from utils import general_utils, eoc\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from typing import Tuple, List\n",
    "import torch.nn.functional as F\n",
    "import eoc_datasets\n",
    "from model_configs import model_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model and load tokenizer\n",
    "config = model_configs[model_identifier]\n",
    "\n",
    "model_name, model_dir, mode, no_extra_tokens, model_kwargs = \\\n",
    "    config['model_name'], config['model_dir'], config['mode'], config['no_extra_tokens'], config['model_kwargs']\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=os.path.join(MY_HUGGINGFACE_CACHE_DIR, model_dir)\n",
    ")\n",
    "\n",
    "# define loss and get extra ids\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) #reduction='avg'\n",
    "ce_loss_sum = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum') #reduction='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/nus-ytj/miniconda3/envs/my_inconsistencies/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.79s/it]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True  # Load model\n",
    "if RUN_CELL:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=os.path.join(MY_HUGGINGFACE_CACHE_DIR, model_dir),\n",
    "        **model_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"MMLU\":\n",
    "    dataset_processor = eoc_datasets.MMLUProcessor(subjects=config['mmlu_subjects'])\n",
    "    data = dataset_processor.get_dataset(\n",
    "        set_partition='test', \n",
    "    )\n",
    "elif dataset_name == \"BigBench\":\n",
    "    dataset_processor = eoc_datasets.BigBenchProcessor(subjects=config['bigbench_subjects'])\n",
    "    data = dataset_processor.get_dataset(\n",
    "        set_partition='train', \n",
    "    )\n",
    "example_generator = dataset_processor.example_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "225it [00:01, 147.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input len max: 453, min: 21, avg: 46.306666666666665\n",
      "completion len max: 4, min: 2, avg: 3.542222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True   # set tensors_filtering_criterion by lengths\n",
    "if RUN_CELL:\n",
    "    def tensors_filtering_criterion(input_ids, completions_batch):\n",
    "        return len(input_ids[0]) > INPUT_LEN_MIN \\\n",
    "            and all([len(general_utils.remove_trailing_zeros_from_1d_tensor(completion)) < COMPLETION_LEN_MAX for completion in completions_batch])\n",
    "    gen = example_generator(data, tokenizer, mode=mode, tensors_filtering_criterion=tensors_filtering_criterion)\n",
    "    input_lens = []\n",
    "    completion_lens = []\n",
    "    for example_id, input_ids, completions_batch, label in tqdm(gen):\n",
    "        input_lens.append(len(input_ids[0]))\n",
    "        completion_lens.append(len(completions_batch[0])) # with padding, this is the max len of the completions\n",
    "    # print(f\"input len > 20 and completion len < 10  and len > 6: {sum([i > 20 and j < 6 for i, j in zip(input_lens, completion_lens)])}\")\n",
    "    # print(f\"completion len < 6: {sum([j < 6 for j in completion_lens])}\")\n",
    "    print(f\"input len max: {max(input_lens)}, min: {min(input_lens)}, avg: {sum(input_lens)/len(input_lens)}\")\n",
    "    print(f\"completion len max: {max(completion_lens)}, min: {min(completion_lens)}, avg: {sum(completion_lens)/len(completion_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:40,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True  # generate baseline info and conditionals\n",
    "if RUN_CELL:\n",
    "    baseline = dict() \n",
    "    # save the label and the number of completions\n",
    "    gen = example_generator(data, tokenizer, mode, tensors_filtering_criterion=tensors_filtering_criterion)\n",
    "    for example_id, input_ids, completions_batch, label in tqdm(gen):\n",
    "        baseline[example_id] = dict()\n",
    "        baseline[example_id]['label'] = label\n",
    "        baseline[example_id]['no_completions'] = len(completions_batch)\n",
    "        baseline[example_id]['p_map'] = []\n",
    "        p_and_completion = []\n",
    "        outputs = eoc.multi_labels_forward(model, input_ids.cuda(), completions_batch.cuda())\n",
    "\n",
    "        for completion_index in range(len(completions_batch)):\n",
    "            p = -ce_loss(\n",
    "                # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                # if the first one is <extra_id_0> and it is omitted\n",
    "                outputs.logits[completion_index][no_extra_tokens:].cuda(), \n",
    "                completions_batch[completion_index][no_extra_tokens:].cuda()\n",
    "            )\n",
    "\n",
    "            baseline[example_id]['p_map'] += [p.detach().cpu().tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-offset Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:25,  8.66it/s]\n",
      "225it [00:25,  8.73it/s]\n",
      "225it [00:22,  9.96it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True \n",
    "if RUN_CELL:\n",
    "    p_map_offset = dict() # maps (example_id, offset, completion_index) -> avg_p\n",
    "    for offset in ALL_OFFSETS:\n",
    "        gen = example_generator(data, tokenizer, mode, tensors_filtering_criterion=tensors_filtering_criterion)\n",
    "        for example_id, input_ids, completions_batch, label in tqdm(gen):\n",
    "            input_ids_offset, labels_offset = eoc.create_offset_sample_from_batch(\n",
    "                tokenizer,\n",
    "                input_ids,\n",
    "                completions_batch,\n",
    "                offset\n",
    "            )\n",
    "            outputs = eoc.multi_labels_forward(model, input_ids_offset.cuda(), labels_offset.cuda())\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the original completion and omit offset tokens \n",
    "                    # if the first one is <extra_id_0> and it is omitted\n",
    "                    outputs.logits[completion_index][no_extra_tokens+offset:].cuda(), \n",
    "                    labels_offset[completion_index][no_extra_tokens+offset:].cuda()\n",
    "                )\n",
    "                p_map_offset[(example_id, offset, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multispan Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [00:25,  8.90it/s]\n",
      "225it [00:21, 10.33it/s]\n",
      "225it [00:21, 10.38it/s]\n",
      "225it [00:21, 10.26it/s]\n",
      "225it [00:21, 10.39it/s]\n",
      "225it [00:21, 10.38it/s]\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True  # generate multispan conditionals\n",
    "if RUN_CELL:\n",
    "    p_map_multispan = dict()\n",
    "    for length_gap_num_tuple in ALL_LENGTH_GAP_NUM_TUPLES:\n",
    "        span_length, gap_between_spans, num_spans = length_gap_num_tuple    \n",
    "        gen = example_generator(data, tokenizer, mode, tensors_filtering_criterion=tensors_filtering_criterion)\n",
    "\n",
    "        for example_id, input_ids, completions_batch, label in tqdm(gen):\n",
    "            # print(input_ids.shape)\n",
    "            # continue\n",
    "            inputs_ids_multispan, labels_multispan = eoc.create_multiple_span_sample_from_batch(\n",
    "                tokenizer,\n",
    "                input_ids[0], # squeeze 1st dim\n",
    "                completions_batch,\n",
    "                span_length,\n",
    "                gap_between_spans,\n",
    "                num_spans,\n",
    "            )\n",
    "            outputs = eoc.multi_labels_forward(model, inputs_ids_multispan.cuda(), labels_multispan.cuda())\n",
    "\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                # assert multispan samples are correct \n",
    "                assert completions_batch[completion_index].nonzero().shape[0] == \\\n",
    "                    labels_multispan[completion_index][num_spans * (span_length + 1) :].nonzero().shape[0]\n",
    "\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the completion (see assert below)); \n",
    "                    # so the first <extra_id_0> is omitted, and for each span, the span + <extra_id_k> is omitted;\n",
    "                    # totally 1 + num_spans * (span_length + 1) tokens are omitted;\n",
    "                    # labels_multispan contains paddings.\n",
    "                    outputs.logits[completion_index][1 + num_spans * (span_length + 1) :].cuda(), \n",
    "                    labels_multispan[completion_index][1 + num_spans * (span_length + 1) :].cuda()\n",
    "                )\n",
    "                p_map_multispan[(example_id, span_length, gap_between_spans, num_spans, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disagreement and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_disagreement(p_and_completion_individually):\n",
    "    best_completion_indices = []\n",
    "    for p_and_completion_individual in p_and_completion_individually:\n",
    "        _, best_completion_index = max(p_and_completion_individual, key=lambda x: x[0])\n",
    "        best_completion_indices.append(best_completion_index)\n",
    "    return len(set(best_completion_indices)) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the EOC function'''\n",
    "# Max reduction to emsemble conditionals for the same last word\n",
    "'''Max reduction to emsemble conditionals for the same last word, \n",
    "i.e., only the maximum avg_log_p is kept for each last word across different range_middle_span_length's and range_middle_to_end_gap's.\n",
    "Emsemble the baseline conditionals with the K-offset conditionals and middle-off conditionals.'''\n",
    "\n",
    "def run_eoc(offsets, length_gap_num_tuples):\n",
    "    add_baseline = True\n",
    "    add_k_offset = offsets != []\n",
    "    add_multispan = length_gap_num_tuples != []\n",
    "\n",
    "    count_correct = 0\n",
    "    count_disagreement = 0\n",
    "    for example_index in range(len(baseline)):\n",
    "        no_completions = baseline[example_index]['no_completions']\n",
    "        # Create a list of tuples (avg_log_p, completion) for each completion\n",
    "        p_and_completion = []\n",
    "        p_and_completion_individually = []\n",
    "        # add the baseline (offset = 0 from K-offset ensemble) to the list\n",
    "        if add_baseline:\n",
    "            p_and_completion_individual = [\n",
    "                (baseline[example_index]['p_map'][completion_index], completion_index)\n",
    "                for completion_index in range(no_completions)\n",
    "            ]\n",
    "            p_and_completion += p_and_completion_individual\n",
    "            p_and_completion_individually.append(p_and_completion_individual)\n",
    "            \n",
    "        # add the whole K-offset ensemble to the list\n",
    "        if add_k_offset:\n",
    "            for offset in offsets:\n",
    "                p_and_completion_individual = [\n",
    "                    (p_map_offset[(example_index, offset, completion_index)], completion_index)\n",
    "                    for completion_index in range(no_completions)\n",
    "                ]\n",
    "                p_and_completion += p_and_completion_individual\n",
    "                p_and_completion_individually.append(p_and_completion_individual)\n",
    "                \n",
    "        if add_multispan:\n",
    "            for length_gap_num in length_gap_num_tuples:\n",
    "                p_and_completion_individual = [\n",
    "                    (p_map_multispan[(example_index, *length_gap_num, completion_index)], completion_index)\n",
    "                    for completion_index in range(no_completions)\n",
    "                ]\n",
    "                p_and_completion += p_and_completion_individual\n",
    "                p_and_completion_individually.append(p_and_completion_individual)\n",
    "\n",
    "        # Find the tuple with the maximum avg_log_p; this is essentially max reduction\n",
    "        _, best_completion_index = max(p_and_completion, key=lambda x: x[0])\n",
    "        label = baseline[example_index]['label']\n",
    "        if (isinstance(label, int) and best_completion_index == label) or \\\n",
    "        (isinstance(label, list) and best_completion_index in label) :# TruthfulQA has multiple correct answers\n",
    "            count_correct += 1\n",
    "        count_disagreement += calc_disagreement(p_and_completion_individually)\n",
    "    # print(\"accuracy:\", count_correct / len(baseline))\n",
    "    return count_correct / len(baseline), count_disagreement / len(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_DISTS: 0, avg_acc: 0.4577777777777778 avg_disagreement: 0.0\n",
      "NO_DISTS: 1, avg_acc: 0.48246913580246914 avg_disagreement: 0.14962962962962964\n",
      "NO_DISTS: 2, avg_acc: 0.4885185185185185 avg_disagreement: 0.21432098765432098\n",
      "NO_DISTS: 3, avg_acc: 0.492116402116402 avg_disagreement: 0.25624338624338616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_DISTS: 4, avg_acc: 0.4947442680776012 avg_disagreement: 0.28649029982363294\n",
      "NO_DISTS: 5, avg_acc: 0.4970723104056434 avg_disagreement: 0.3095238095238095\n",
      "NO_DISTS: 6, avg_acc: 0.4993650793650795 avg_disagreement: 0.32772486772486786\n",
      "NO_DISTS: 7, avg_acc: 0.5017283950617283 avg_disagreement: 0.34246913580246907\n",
      "NO_DISTS: 8, avg_acc: 0.5041975308641975 avg_disagreement: 0.3545679012345679\n",
      "NO_DISTS: 9, avg_acc: 0.5066666666666667 avg_disagreement: 0.36444444444444446\n"
     ]
    }
   ],
   "source": [
    "RUN_CELL = True  # Run EOC\n",
    "if RUN_CELL:\n",
    "    NO_OFFSETS = len(ALL_OFFSETS)\n",
    "    NO_MULTISPAN = len(ALL_LENGTH_GAP_NUM_TUPLES)\n",
    "    NO_DISTS_RANGE = list(range(NO_OFFSETS + NO_MULTISPAN + 1))\n",
    "    avg_accs = []\n",
    "    avg_disagreements = []\n",
    "    for NO_DISTS in NO_DISTS_RANGE: # no of distributions to ensemble\n",
    "        all_dist_ids = list(combinations(range(NO_MULTISPAN + NO_OFFSETS), NO_DISTS))\n",
    "        # shuffle and take the first 100\n",
    "        random.shuffle(all_dist_ids)\n",
    "        all_dist_ids = all_dist_ids[:500]\n",
    "        all_accs = []\n",
    "        all_disagreements = []\n",
    "        for dist_ids in all_dist_ids:\n",
    "            offsets = []\n",
    "            length_gap_num_tuples = []\n",
    "            for dist_id in dist_ids:\n",
    "                if dist_id < NO_OFFSETS:\n",
    "                    offsets.append(ALL_OFFSETS[dist_id])\n",
    "                else:\n",
    "                    length_gap_num_tuples.append(ALL_LENGTH_GAP_NUM_TUPLES[dist_id - NO_OFFSETS])            \n",
    "            acc, disagreement = run_eoc(\n",
    "                offsets,\n",
    "                length_gap_num_tuples,\n",
    "            )\n",
    "            # print offsets and length_gap_num_tuples and acc\n",
    "            # print(offsets, length_gap_num_tuples, acc)\n",
    "            all_accs.append(acc)\n",
    "            all_disagreements.append(disagreement)\n",
    "        avg_acc = sum(all_accs) / len(all_accs)\n",
    "        avg_disagreement = sum(all_disagreements) / len(all_disagreements)\n",
    "        avg_accs.append(avg_acc)\n",
    "        avg_disagreements.append(avg_disagreement)\n",
    "        # print number of dists and avg_acc\n",
    "        print(f\"NO_DISTS: {NO_DISTS}, avg_acc: {avg_acc}\", f\"avg_disagreement: {avg_disagreement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.4577777777777778,\n",
       "  0.48246913580246914,\n",
       "  0.4885185185185185,\n",
       "  0.492116402116402,\n",
       "  0.4947442680776012,\n",
       "  0.4970723104056434,\n",
       "  0.4993650793650795,\n",
       "  0.5017283950617283,\n",
       "  0.5041975308641975,\n",
       "  0.5066666666666667],\n",
       " [0.0,\n",
       "  0.14962962962962964,\n",
       "  0.21432098765432098,\n",
       "  0.25624338624338616,\n",
       "  0.28649029982363294,\n",
       "  0.3095238095238095,\n",
       "  0.32772486772486786,\n",
       "  0.34246913580246907,\n",
       "  0.3545679012345679,\n",
       "  0.36444444444444446])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_accs, avg_disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.4577777777777778,\n",
       "  0.48246913580246914,\n",
       "  0.4885185185185185,\n",
       "  0.492116402116402,\n",
       "  0.4947442680776012,\n",
       "  0.4970723104056434,\n",
       "  0.4993650793650795,\n",
       "  0.5017283950617283,\n",
       "  0.5041975308641975,\n",
       "  0.5066666666666667],\n",
       " [0.0,\n",
       "  0.14962962962962964,\n",
       "  0.21432098765432098,\n",
       "  0.25624338624338616,\n",
       "  0.28649029982363294,\n",
       "  0.3095238095238095,\n",
       "  0.32772486772486786,\n",
       "  0.34246913580246907,\n",
       "  0.3545679012345679,\n",
       "  0.36444444444444446])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_accs, avg_disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([0.4147727272727273,\n",
    "#   0.42550505050505055,\n",
    "#   0.43118686868686856,\n",
    "#   0.4345914502164504,\n",
    "#   0.4370490620490621,\n",
    "#   0.43903318903318905,\n",
    "#   0.4406114718614716,\n",
    "#   0.4417613636363637,\n",
    "#   0.4425505050505051,\n",
    "#   0.4431818181818182],\n",
    "#  [0.0,\n",
    "#   0.31313131313131315,\n",
    "#   0.43229166666666674,\n",
    "#   0.49952651515151525,\n",
    "#   0.5447781385281388,\n",
    "#   0.5767947330447328,\n",
    "#   0.6001758658008658,\n",
    "#   0.6177398989898988,\n",
    "#   0.6313131313131314,\n",
    "#   0.6420454545454546])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
