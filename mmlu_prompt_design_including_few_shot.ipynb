{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "We should use subject hint for MMLU! [p0]\n",
    "We should try out few shot prompting for MMLU! [p0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 57 tasks under MMLU. Each task has a unique subject name `SUBJECT_NAME`, such as high_school_european_history -> high school european history (lose the _ when used in the prompt).\n",
    "\n",
    "Each test sample is (question, four_answers). four_answers = true_answer + three_wrong_answers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm_eval_harness uses a nice add to the prompt to increase accuracy as below, which we should use.\n",
    "\n",
    "`The following is about SUBJECT_NAME.`\n",
    "\n",
    "We call it `subject_hint`.\n",
    "\n",
    "So in 0-shot, we feed the model `subject_hint + test_question + test_answer_choice` \n",
    "to get the probability.\n",
    "In 5-shot, we feed the model\n",
    "\n",
    "\n",
    "`The following are about SUBJECT_NAME.`\n",
    "\n",
    "`question1 + true_answer1`\n",
    "\n",
    "`question2 + true_answer2`\n",
    "\n",
    "`question3 + true_answer3`\n",
    "\n",
    "`question4 + true_answer4`\n",
    "\n",
    "`question5 + true_answer5`\n",
    "\n",
    "`test_question + test_answer_choice`\n",
    "\n",
    "to get the probability.\n",
    "\n",
    "`prompt1` ~ `prompt5` are prompts from the validation split (the training split is too small). `true_answer1` ~ `true_answer5` are the correct answers to the prompts. \n",
    "`test_answer_choice` is from `[test_answer_choice_A, test_answer_choice_B, test_answer_choice_C, test_answer_choice_D]`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
