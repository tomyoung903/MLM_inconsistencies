{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Unit test for create_multiple_span_sample()'''\n",
    "input_ids = \\\n",
    "processor.create_multiple_span_sample(\n",
    "    lambada[0]['inputs_pretokenized'],\n",
    "    id_to_completions_ids[0][0], \n",
    "    span_length=SPAN_LENGTH, \n",
    "    gap_between_spans=GAP_BETWEEN_SPANS, \n",
    "    num_spans=NUM_SPANS,\n",
    "    return_tensor='inputs'\n",
    ")\n",
    "\n",
    "labels = \\\n",
    "processor.create_multiple_span_sample(\n",
    "    lambada[0]['targets_pretokenized'],\n",
    "    id_to_completions_ids[0][0], \n",
    "    span_length=SPAN_LENGTH, \n",
    "    gap_between_spans=GAP_BETWEEN_SPANS, \n",
    "    num_spans=NUM_SPANS,\n",
    "    return_tensor='labels'\n",
    ")\n",
    "\n",
    "print('inputs_original_string:', lambada[0]['inputs_pretokenized'])\n",
    "print('labels_original_string:', tokenizer.decode(id_to_completions_ids[0][0]))\n",
    "print('inputs_original_ids:', torch.tensor(processor.tokenizer(lambada[0]['inputs_pretokenized'])['input_ids']))\n",
    "print('labels_original_ids:', id_to_completions_ids[0][0])\n",
    "\n",
    "\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)\n",
    "print('inputs decoded:', tokenizer.decode(input_ids))\n",
    "print('labels decoded:', tokenizer.decode(labels))\n",
    "\n",
    "# decode inputs and labels (one by one) in id_span_gap_num_to_input_and_completions\n",
    "for id_span_gap_num in id_span_gap_num_to_input_and_completions:\n",
    "    print(\"\\nid_span_gap_num:\\n\", id_span_gap_num)\n",
    "    print('inputs_original_string:', tokenizer.decode(tokenizer(lambada[id_span_gap_num[0]]['inputs_pretokenized']).input_ids))\n",
    "    print(\"\\ninputs\", tokenizer.decode(id_span_gap_num_to_input_and_completions[id_span_gap_num]['inputs']))\n",
    "\n",
    "    for i in range(len(id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels'])):\n",
    "        print(\"\\ncompletion:\", tokenizer.decode(id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels'][i]))\n",
    "        print('labels_original_string:', tokenizer.decode(id_to_completions_ids[id_span_gap_num[0]][i]))\n",
    "    print(\"\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenizer vocab, decode, encode'''\n",
    "vocab = tokenizer.get_vocab() \n",
    "# sort by key\n",
    "vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1])}\n",
    "print(vocab)\n",
    "\n",
    "tokenizer('``') == <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''align get_multiple_span_samples() and get_middle_off_samples()'''\n",
    "\n",
    "assert torch.all(id_span_gap_num_to_input_and_completions[(0,3,5,1)]['inputs'] == id_middlespan_gap_to_input_and_completions[(0,3,5)]['inputs']).tolist()\n",
    "assert torch.all(id_span_gap_num_to_input_and_completions[(0,3,5,1)]['labels'][0] == id_middlespan_gap_to_input_and_completions[(0,3,5)]['labels'][0]).tolist()\n",
    "\n",
    "for key in id_middlespan_gap_to_input_and_completions:\n",
    "    new_key = (key[0], key[1], key[2], 1)\n",
    "    # assert inputs\n",
    "    assert torch.all(id_span_gap_num_to_input_and_completions[new_key]['inputs'] == id_middlespan_gap_to_input_and_completions[key]['inputs']).tolist()\n",
    "    # assert labels\n",
    "    for i in range(len(id_span_gap_num_to_input_and_completions[new_key]['labels'])):\n",
    "        assert torch.all(id_span_gap_num_to_input_and_completions[new_key]['labels'][i] == id_middlespan_gap_to_input_and_completions[key]['labels'][i]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Obtain the avg_log_p_map for middle-off samples via data parallelism'''\n",
    "RUN_CELL = 0\n",
    "if RUN_CELL:\n",
    "    from multiprocessing import Process\n",
    "    import multiprocessing\n",
    "    avg_log_p_map_middle_off = dict()\n",
    "    # define the processing for each id_middlespan_gap example as a function and use threading to use 3 models in parallel\n",
    "    def process(list_id_middlespan_gap, model_, device='cuda:0'):\n",
    "        for id_middlespan_gap in tqdm(list_id_middlespan_gap):\n",
    "            input_ids = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['inputs'].unsqueeze(0).to(device)\n",
    "            completions_batch = id_middlespan_gap_to_input_and_completions[id_middlespan_gap]['labels'].to(device)\n",
    "            outputs = lambada_utils.multi_labels_forward(model_, input_ids, completions_batch)\n",
    "\n",
    "            middlespan_length = id_middlespan_gap[1]\n",
    "\n",
    "            for completion_index in range(len(completions_batch)):\n",
    "                avg_log_p = -ce_loss(\n",
    "                    # Only care about the tokens corresponding to the last word and omit offset tokens \n",
    "                    # the first one is <extra_id_0> and omitted\n",
    "                    outputs.logits[completion_index][2+middlespan_length:], \n",
    "                    completions_batch[completion_index][2+middlespan_length:]\n",
    "                )\n",
    "                avg_log_p_map_middle_off[(*id_middlespan_gap, completion_index)] = \\\n",
    "                    avg_log_p.detach().cpu().tolist()\n",
    "            \n",
    "    # run the above function in parallel\n",
    "    import threading\n",
    "    multiprocessing.set_start_method('spawn')\n",
    "\n",
    "    all_id_middlespan_gaps = list(id_middlespan_gap_to_input_and_completions.keys())\n",
    "    all_id_middlespan_gaps_0 = all_id_middlespan_gaps[:len(all_id_middlespan_gaps)//2]\n",
    "    all_id_middlespan_gaps_1 = all_id_middlespan_gaps[len(all_id_middlespan_gaps)//2:]\n",
    "    # all_id_middlespan_gaps_2 = all_id_middlespan_gaps[2*len(all_id_middlespan_gaps)//3:]\n",
    "\n",
    "    t0 = Process(target=process, args=(all_id_middlespan_gaps_0, model, 'cuda:0'))\n",
    "    t1 = Process(target=process, args=(all_id_middlespan_gaps_1, model1, 'cuda:2'))\n",
    "    # t2 = threading.Thread(target=process, args=(all_id_middlespan_gaps_2, model2, 'cuda:4'))\n",
    "\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    # t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # '''Generate the top completions (through beam search) for each example, and get the word from each completion.'''\n",
    "if RUN_CELL:\n",
    "    # generate for all examples, and then get the words from the completions, and compare the first one with the target\n",
    "    count_correct = 0 # No. correct last word predictions if only the top completion is considered\n",
    "    count_correct_top_num_beams = 0 # ... if the top num_beams completions are considered\n",
    "    count_no_words_found = 0  # No. examples where no valid last word is found\n",
    "\n",
    "    # punctuated_word: the last word and the punctuation that follows it\n",
    "    id_to_punctuated_words = {} # maps example index to a list of word and punc pairs; every punc is kept for each word\n",
    "    id_to_punctuated_words_unique = {} # ...; every punc is kept for each word  \n",
    "    id_to_completions_ids = {}\n",
    "\n",
    "    MAX_COMPLETION_LENGTH = 8 # for last word prediction, 8 is sufficient\n",
    "    NUM_BEAMS = 20 # 20 is sufficient; more doesn't help\n",
    "\n",
    "    # for example_index in tqdm(range(10)): # len(lambada)\n",
    "    for example_index in tqdm(range(len(lambada))): # len(lambada)\n",
    "        input_string = lambada[example_index]['inputs_pretokenized']\n",
    "        inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        outputs = model.generate(inputs,\n",
    "                                max_length=MAX_COMPLETION_LENGTH, \n",
    "                                num_beams=NUM_BEAMS, \n",
    "                                num_return_sequences=NUM_BEAMS, \n",
    "                                output_scores=True,\n",
    "                                eos_token_id=tokenizer.convert_tokens_to_ids('<extra_id_1>'), \n",
    "                                return_dict_in_generate=True)\n",
    "        \n",
    "        completions = [tokenizer.decode(outputs['sequences'][i]) for i in range(NUM_BEAMS)]\n",
    "        completions_ids_with_valid_lastword = [\n",
    "            outputs['sequences'][i].cpu()\n",
    "            for i in range(NUM_BEAMS)\n",
    "            if processor.get_word_from_completion(completions[i]) is not None # if the completion has a valid last word\n",
    "        ]\n",
    "\n",
    "        words = processor.get_words_from_completions(completions)\n",
    "        words_unique = list(set(words))\n",
    "\n",
    "        # TODO: combine them and move to lambada_utils.py\n",
    "        completions_without_pad = processor.remove_pad_id(completions_ids_with_valid_lastword)\n",
    "        completions_without_pad_before_punctuation = processor.before_first_punc_including(completions_without_pad) # including the first punc\n",
    "        \n",
    "        if words:\n",
    "            if words[0] == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct += 1\n",
    "        else:\n",
    "            count_no_words_found += 1\n",
    "            # print(\"no words found\")\n",
    "        punctuated_words = processor.get_punctuated_words(completions)\n",
    "        id_to_punctuated_words[example_index] = punctuated_words\n",
    "        \n",
    "        completions_without_pad_before_punctuation_unique = list(set([tuple(x.numpy()) for x in completions_without_pad_before_punctuation]))\n",
    "        id_to_completions_ids[example_index] = [torch.tensor(x) for x in completions_without_pad_before_punctuation_unique]\n",
    "\n",
    "        # find the best punctuatuation for each unique word (Maximum Probability Strategy, \n",
    "        # completions are naturally ordered by probs by generate()) TODO: move this for loop to utils.py\n",
    "        id_to_punctuated_words_unique[example_index] = []\n",
    "        for word in words_unique:\n",
    "            found = 0\n",
    "            # iterate through the word and punc pairs, and find the one that matches the word\n",
    "            for punctuated_word in punctuated_words:\n",
    "                # it is a match if pair = word + punc\n",
    "                ENDING_PUNCTUATIONS = ',!.:;?'\n",
    "                for punc in ENDING_PUNCTUATIONS:\n",
    "                    if punctuated_word == word + punc:\n",
    "                        id_to_punctuated_words_unique[example_index].append(punctuated_word)\n",
    "                        found = 1\n",
    "                        break\n",
    "                if found == 1:\n",
    "                    break\n",
    "        \n",
    "        # calculate the number of correct top num_beams: if the correct word is in the top num_beams, then it is correct\n",
    "        for word in words_unique:\n",
    "            if word == lambada[example_index]['targets_pretokenized'][0]:\n",
    "                count_correct_top_num_beams += 1\n",
    "                break\n",
    "    print(\"count_correct\", count_correct)\n",
    "    # count_correct, NLU: 0.7595\n",
    "    # count_correct, NLG: 0.7680 (test)\n",
    "    # count_correct, NLG: 0.7689 (test, spaced punc fixed)\n",
    "    # count_correct, S2S: 0.3743 (could be because how the mode handles extra_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CELL = 0 # TODO: a lot of duplicates in id_to_completions_ids, remove them\n",
    "if RUN_CELL:\n",
    "    id_to_completions_ids_unique = {}\n",
    "    for key in id_to_completions_ids:\n",
    "        completions_ids_unique = list(set([tuple(x.numpy()) for x in id_to_completions_ids[key]]))\n",
    "        id_to_completions_ids_unique[key] = [torch.tensor(x) for x in completions_ids_unique]\n",
    "    ul2_lambada_vanilla_beam_search_results['id_to_completions_ids'] = id_to_completions_ids_unique\n",
    "    # save it back to the pickle file\n",
    "    with open(timed_pickle_filename, 'wb') as fp:\n",
    "        pickle.dump(ul2_lambada_vanilla_beam_search_results, fp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
