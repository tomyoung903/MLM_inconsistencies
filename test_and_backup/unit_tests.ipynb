{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Unit test for create_multiple_span_sample()'''\n",
    "input_ids = \\\n",
    "processor.create_multiple_span_sample(\n",
    "    lambada[0]['inputs_pretokenized'],\n",
    "    id_to_completions_ids[0][0], \n",
    "    span_length=SPAN_LENGTH, \n",
    "    gap_between_spans=GAP_BETWEEN_SPANS, \n",
    "    num_spans=NUM_SPANS,\n",
    "    return_tensor='inputs'\n",
    ")\n",
    "\n",
    "labels = \\\n",
    "processor.create_multiple_span_sample(\n",
    "    lambada[0]['targets_pretokenized'],\n",
    "    id_to_completions_ids[0][0], \n",
    "    span_length=SPAN_LENGTH, \n",
    "    gap_between_spans=GAP_BETWEEN_SPANS, \n",
    "    num_spans=NUM_SPANS,\n",
    "    return_tensor='labels'\n",
    ")\n",
    "\n",
    "print('inputs_original_string:', lambada[0]['inputs_pretokenized'])\n",
    "print('labels_original_string:', tokenizer.decode(id_to_completions_ids[0][0]))\n",
    "print('inputs_original_ids:', torch.tensor(processor.tokenizer(lambada[0]['inputs_pretokenized'])['input_ids']))\n",
    "print('labels_original_ids:', id_to_completions_ids[0][0])\n",
    "\n",
    "\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)\n",
    "print('inputs decoded:', tokenizer.decode(input_ids))\n",
    "print('labels decoded:', tokenizer.decode(labels))\n",
    "\n",
    "# decode inputs and labels (one by one) in id_span_gap_num_to_input_and_completions\n",
    "for id_span_gap_num in id_span_gap_num_to_input_and_completions:\n",
    "    print(\"\\nid_span_gap_num:\\n\", id_span_gap_num)\n",
    "    print('inputs_original_string:', tokenizer.decode(tokenizer(lambada[id_span_gap_num[0]]['inputs_pretokenized']).input_ids))\n",
    "    print(\"\\ninputs\", tokenizer.decode(id_span_gap_num_to_input_and_completions[id_span_gap_num]['inputs']))\n",
    "\n",
    "    for i in range(len(id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels'])):\n",
    "        print(\"\\ncompletion:\", tokenizer.decode(id_span_gap_num_to_input_and_completions[id_span_gap_num]['labels'][i]))\n",
    "        print('labels_original_string:', tokenizer.decode(id_to_completions_ids[id_span_gap_num[0]][i]))\n",
    "    print(\"\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tokenizer vocab, decode, encode'''\n",
    "vocab = tokenizer.get_vocab() \n",
    "# sort by key\n",
    "vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1])}\n",
    "print(vocab)\n",
    "\n",
    "tokenizer('``') == <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''align get_multiple_span_samples() and get_middle_off_samples()'''\n",
    "\n",
    "assert torch.all(id_span_gap_num_to_input_and_completions[(0,3,5,1)]['inputs'] == id_middlespan_gap_to_input_and_completions[(0,3,5)]['inputs']).tolist()\n",
    "assert torch.all(id_span_gap_num_to_input_and_completions[(0,3,5,1)]['labels'][0] == id_middlespan_gap_to_input_and_completions[(0,3,5)]['labels'][0]).tolist()\n",
    "\n",
    "for key in id_middlespan_gap_to_input_and_completions:\n",
    "    new_key = (key[0], key[1], key[2], 1)\n",
    "    # assert inputs\n",
    "    assert torch.all(id_span_gap_num_to_input_and_completions[new_key]['inputs'] == id_middlespan_gap_to_input_and_completions[key]['inputs']).tolist()\n",
    "    # assert labels\n",
    "    for i in range(len(id_span_gap_num_to_input_and_completions[new_key]['labels'])):\n",
    "        assert torch.all(id_span_gap_num_to_input_and_completions[new_key]['labels'][i] == id_middlespan_gap_to_input_and_completions[key]['labels'][i]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
