{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-base\")\n",
    "# model.parallelize()\n",
    "model_cache_dir = '/work/09127/tomyoung/ls6/inconsistencies_project/t5-base-cache'\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", cache_dir=model_cache_dir).to(\"cuda\")\n",
    "model.parallelize()                                                                                                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n",
    "\n",
    "# t5 tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t5_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"[NLG] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man wiht a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. <extra_id_0>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "inputs = tokenizer(\"Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man wiht a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. <extra_id_0>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True)\n",
    "# input_string = \"\"                                           \n",
    "outputs_beam = model.generate(inputs, num_beams=2, num_return_sequences=2, output_scores=True, return_dict_in_generate=True)\n",
    "labels = tokenizer(\"<extra_id_0> Dudley was a very good-looking child, with a bald head\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs_forward = model(input_ids=inputs, labels=labels)\n",
    "# outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True)\n",
    "# outputs = model.generate(inputs, num_beams=2, max_length=3, num_return_sequences=2, output_scores=True, return_dict_in_generate=True)\n",
    "loss_forward = outputs_forward.loss\n",
    "logits_forward = outputs_forward.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"[NLG] I will be having a <extra_id_0>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, output_scores=True, return_dict_in_generate=True, max_length=8)\n",
    "outputs_beam = model.generate(inputs, num_beams=2, max_length=8, num_return_sequences=2, output_scores=True, return_dict_in_generate=True)\n",
    "labels = tokenizer(\"<extra_id_0> sabbatical for\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = labels[:, :-1]\n",
    "outputs_forward = model(input_ids=inputs, labels=labels)\n",
    "loss_forward = outputs_forward.loss\n",
    "logits_forward = outputs_forward.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    batch_size = logits.size(0)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        loss -= log_probs[i, labels[i]]\n",
    "        #.item()\n",
    "    loss /= batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_forward[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy_loss(logits_forward[0], labels[0])\n",
    "print(loss)\n",
    "print(loss_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = F.log_softmax(logits_forward[0], dim=1)\n",
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = logits_forward[0].size(0)\n",
    "log_probs = F.log_softmax(logits_forward[0], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1.3116 * 8 / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['sequences_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['scores']\n",
    "# get the probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that that the first one in each index in outputs_beam['scores'] is the same as in outputs['scores']\n",
    "from termcolor import cprint\n",
    "\n",
    "for i in range(len(outputs_beam['scores'])):\n",
    "    # print(outputs_beam['scores'][i][0] == outputs['scores'][i])\n",
    "    probs_outputs_beam_scores = torch.nn.functional.softmax(outputs_beam['scores'][i], dim=-1)\n",
    "    print(probs_outputs_beam_scores[0])\n",
    "    # argmax \n",
    "    print(torch.argmax(probs_outputs_beam_scores[0]))\n",
    "    # decode it\n",
    "    print(tokenizer.decode(torch.argmax(probs_outputs_beam_scores[0])))\n",
    "    # the value\n",
    "    print(probs_outputs_beam_scores[0][torch.argmax(probs_outputs_beam_scores[0])])\n",
    "    probs_outputs_scores = torch.nn.functional.softmax(outputs['scores'][i], dim=-1)\n",
    "    print(probs_outputs_scores[0])\n",
    "    # argmax\n",
    "    print(torch.argmax(probs_outputs_scores[0]))\\\n",
    "    # decode it\n",
    "    print(tokenizer.decode(torch.argmax(probs_outputs_scores[0])))\n",
    "    # the value\n",
    "    print(probs_outputs_scores[0][torch.argmax(probs_outputs_scores[0])])\n",
    "    print('-------------')\n",
    "    # the relative difference\n",
    "    print('relative difference:')\n",
    "    print((probs_outputs_beam_scores[0][torch.argmax(probs_outputs_beam_scores[0])] - probs_outputs_scores[0][torch.argmax(probs_outputs_scores[0])]) / probs_outputs_scores[0][torch.argmax(probs_outputs_scores[0])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = probs_outputs_scores[0][0].item()\n",
    "# check its memory size\n",
    "import sys\n",
    "sys.getsizeof(value)\n",
    "# check its binary representation\n",
    "import struct\n",
    "struct.pack('f', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['scores']\n",
    "# tokenizer.batch_decode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_index = 1\n",
    "outputs_beam[\"sequences\"][sequence_index]\n",
    "print(len(outputs_beam[\"sequences\"][sequence_index]))\n",
    "tokenizer.batch_decode(outputs_beam[\"sequences\"][sequence_index])\n",
    "tokenizer.decode(outputs_beam[\"sequences\"][sequence_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"sequences\"][1]\n",
    "print(len(outputs[\"sequences\"][1]))\n",
    "tokenizer.batch_decode(outputs[\"sequences\"][1])\n",
    "tokenizer.decode(outputs[\"sequences\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"sequences\"][0]\n",
    "# tokenizer.batch_decode(outputs[\"sequences\"][0])\n",
    "# tokenizer.decode(outputs[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['sequences_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['beam_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_beam['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['sequences']\n",
    "tokenizer.batch_decode(outputs_beam['sequences'][0])\n",
    "tokenizer.decode(outputs['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs['scores']))\n",
    "for k in range(len(outputs['scores'])):\n",
    "    outputs['scores'][k]\n",
    "    probs = torch.softmax(outputs['scores'][k], dim=-1)\n",
    "    best_id = torch.argmax(probs)\n",
    "    print(best_id)\n",
    "    print(tokenizer.decode(best_id))\n",
    "    print(probs[0][best_id])\n",
    "    # print(probs_forward[0][k][best_id])\n",
    "    # print relative difference\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the beam scores for the first sequence\n",
    "for k in range(len(outputs['scores'])):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_forward\n",
    "probs_forward = torch.softmax(logits_forward, dim=-1)\n",
    "probs\n",
    "# probs[0].shape\n",
    "\n",
    "for k in range(logits_forward.shape[1]):\n",
    "    print(labels[0][k])\n",
    "    print(tokenizer.decode(labels[0][k]))\n",
    "    print(probs_forward[0][k][labels[0][k]])\n",
    "    print('------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluate_conditional_probablity(model, tokenizer, input_string, target_string):\n",
    "    input_ids = tokenizer(input_string, return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer(target_string, return_tensors=\"pt\").input_ids\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    likelihood = math.exp(-loss)\n",
    "    return likelihood, loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# training\n",
    "input_ids = tokenizer(\"A man is having a <extra_id_0> for breakfast.\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> small pizza <extra_id_1>\", return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "# recover likelihood from loss\n",
    "likelihood = math.exp(-loss)\n",
    "likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5Model.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"100 + 100 equals <extra_id_0>. \", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# change the default huggingface caching directory\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir='./t5-large-cache')\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in the park\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "sequence_ids = model.generate(input_ids)\n",
    "sequences = tokenizer.batch_decode(sequence_ids)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "acceptable_alternatives_bigrams = '/work/09127/tomyoung/ls6/data/pkls/acceptable_alternatives_1000_ignore_cws_nos_50.pkl'\n",
    "\n",
    "with open(acceptable_alternatives_bigrams, 'rb') as f:\n",
    "    acceptable_alternatives_bigrams = pickle.load(f)\n",
    "acceptable_alternatives_bigrams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(acceptable_alternatives_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_alternatives_bigrams[(25, 3, 21)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_alternatives_bigrams[(25, 3, 22)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
