{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_log_p_of_option(inputs_pretokenized, word_and_punc_pair):\n",
    "    input_ids = tokenizer(inputs_pretokenized, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # print(data_appended[0]['inputs_pretokenized'])\n",
    "    # labels = tokenizer(\"<extra_id_0> \" + word_and_punc_pair + \" <extra_id_1>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    labels = tokenizer(\"<extra_id_0> \" + word_and_punc_pair, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    # HIGHLIGHT\n",
    "    labels = labels[:, :-1]\n",
    "    # print(\"<extra_id_0> \" + id_to_word_and_punc_pairs_processed[0][2] + \" <extra_id_1>\")\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    # logits = outputs.logits\n",
    "    # loss = loss_fn(logits.view(-1, logits.shape[-1])[1:labels.shape[1]-2], labels.view(-1)[1:labels.shape[1]-2])\n",
    "    return -outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of correct predictions again using get_avg_log_p_of_options and ids_to_word_and_punc_pairs_processed\n",
    "count_correct_avg_log_p_reranking = 0\n",
    "for example_index in tqdm(range(1)): # len(data_appended)\n",
    "    input_string = data_appended[example_index]['inputs_pretokenized']\n",
    "    word_and_punc_pair_avg_log_p_max = -10000000\n",
    "    best_word_and_punc_pair =  \"\"\n",
    "    for word_and_punc_pair in id_to_word_and_punc_pairs_processed[example_index]:\n",
    "        avg_log_p = get_avg_log_p_of_options(input_string, word_and_punc_pair)\n",
    "        # print(avg_log_p)\n",
    "        if avg_log_p > word_and_punc_pair_avg_log_p_max:\n",
    "            word_and_punc_pair_avg_log_p_max = avg_log_p\n",
    "            best_word_and_punc_pair = word_and_punc_pair\n",
    "    best_word = best_word_and_punc_pair[:-1]\n",
    "    print(best_word)\n",
    "    print(best_word_and_punc_pair)\n",
    "    if best_word == data_appended[example_index]['targets_pretokenized'][0]:\n",
    "        count_correct_avg_log_p_reranking += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/work/09127/tomyoung/ls6/LLM_cache/models--google--ul2\"\n",
    "\n",
    "# cache_dir='./models--google--ul2',\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", cache_dir='/work/09127/tomyoung/ls6/LLM_cache/google-ul2/', low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model.parallelize()                                                                                                  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")\n",
    "\n",
    "input_string = \"[NLU] Mr. Dursley was the director of a firm called <extra_id_0>, \\\n",
    "    which made <extra_id_1>. He was a big, solid man with a bald head. \\\n",
    "        Mrs. Dursley was thin and <extra_id_2> of neck, which came in very useful as she spent \\\n",
    "            so much of her time <extra_id_3>. The Dursleys had a small son \\\n",
    "                called Dudley and <extra_id_4>\"                                           \n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "# 19,459,613,696\n",
    "# inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(inputs, max_length=200)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "# -> \"<pad><extra_id_0> Burrows<extra_id_1> a lot of money from the manufacture of a product called '' Burrows'''s ''<extra_id_2> had a lot<extra_id_3> looking down people's throats<extra_id_4> a daughter called Petunia. Dudley was a very stupid boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat, ugly boy who was always getting into trouble. He was a big, fat,\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /work/09127/tomyoung/ls6/model_inputs_1_31am.pt\n",
    "model_inputs_1_31_am = torch.load('/work/09127/tomyoung/ls6/model_inputs_1_31am.pt')\n",
    "encoder_outputs_5_13am = torch.load('/work/09127/tomyoung/ls6/encoder_outputs_5_13am.pt')\n",
    "# torch.save(model.state_dict(), '/work/09127/tomyoung/ls6/model_state_dict_1_31am.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inputs_1_31_am: encoder_outputs, from generate, from model_kwargs, by passing prompt to the encoder\n",
    "outputs = model(encoder_outputs=model_inputs_1_31_am['encoder_outputs'], decoder_input_ids=model_inputs_1_31_am['decoder_input_ids'])\n",
    "# model_inputs_1_31_am\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Toy example'''\n",
    "input_string = \"[NLU] Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, solid man with a bald head. Mrs. Dursley was thin and blonde and more than the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere <extra_id_0>\"                                               \n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_length=200, num_beams=10, num_return_sequences=10)\n",
    "for i in range(1):\n",
    "    print(tokenizer.decode(outputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for 0th example\n",
    "input_string = data_appended[0]['inputs_pretokenized']\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_length=100, num_beams=100, num_return_sequences=100)\n",
    "options = [tokenizer.decode(outputs[i]) for i in range(100)]\n",
    "for i in range(100):\n",
    "    print(tokenizer.decode(outputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0\n",
    "print(len(outputs['scores']))\n",
    "for k in range(len(outputs['scores'])):\n",
    "    outputs['scores'][k]\n",
    "    probs = torch.softmax(outputs['scores'][k], dim=-1)\n",
    "    best_id = torch.argmax(probs)\n",
    "    print(best_id)\n",
    "    print(tokenizer.decode(best_id))\n",
    "    print(probs[0][best_id])\n",
    "\n",
    "# probs[0][3074]\n",
    "# [1]\n",
    "# torch.exp(outputs['scores'][1])\n",
    "# torch.argmax(torch.exp(outputs['scores'][0]))\n",
    "# # top 5\n",
    "# torch.topk(torch.exp(outputs['scores'][0]), 5)\n",
    "\n",
    "# torch.sum(torch.exp(outputs['scores'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# training\n",
    "input_ids = tokenizer(\"[NLG] A man is having a bun for <extra_id_0>\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "labels = tokenizer(\"<extra_id_0> lunch. A man is having a bun for lunch. A man is having a\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "# recover likelihood from loss\n",
    "likelihood = math.exp(-loss)\n",
    "likelihood"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
